---
title: "Obstacle Avoidance"
textbook: "#objectobstacle-avoidance"
---

# obstacle avoidance

## **Design of Autonomous Systems**
### csci 6907/4907-Section 86
### Prof. **Sibin Mohan**

---

so far...

---

methods to **detect** and **identify** objects

<img src="img/object/camera_bounding_boxes.gif" width="1400">

---

main goal &rarr; **do not collide** with objects in our path

<img src="img/object/avoidance/final_destination_logs.gif" width="1400">

---

two ways to achieve this goal

---

two ways to achieve this goal

<div class="multicolumn">

<div>

<br>

**stopping**

<img src="img/object/avoidance/stopping-flintstone.gif" height="500">


</div>

<div>

<br>

**alternate paths**

<img src="img/object/avoidance/racecar_overtake.gif" height="500">

</div>


</div>

---

### obstacle avoidance

---

### obstacle avoidance

> capability of robot or autonomous system to **detect and circumvent obstacles** in its path to reach a **predefined destination**


---

simplest way...

---

simplest way...

### use the **sensors**

---

### use the **sensors** 

- actively react to obstacles 

---

### use the **sensors** 

- actively react to obstacles 
- recalculate new paths

---

### use the **sensors** 

<br>

<div class="multicolumn">

<div>

<br>

- actively react to obstacles 
- recalculate new paths

</div>

<div>

<img src="img/object/avoidance/wiki_robot.gif" width="700" border="1">

</div>

</div>


---

**three simple steps**: 

---

**three simple steps**: 

- sense

---

**three simple steps**: 

- sense
- "think"

---

**three simple steps**: 

- sense
- "think"
- act

Note:
- inputs &rarr; distances of objects and provide the robot with data about its surroundings enabling it to detect obstacles and calculate their distances. The robot then adjusts its trajectory to navigate around obstacles while trying to reach its destination. This can be carried out in real-time 


---

**three simple steps**: 

- sense
- "think"
- act

works in "real-time" 

---

difficult in complex situations...

---

difficult in complex situations...

_e.g.,_ autonomous car in urban environmen

---

**contemporary** obstacle detection methods

---

**contemporary** obstacle detection methods

- reactive strategies
- global planners
- machine-learning based methods

---

obstacle avoidance &rarr; overlaps with the path planning 

---

obstacle avoidance &rarr; overlaps with the path planning 

- artificial potential field (APF)
- A* and D* searches
- RRT 

---

obstacle avoidance &rarr; overlaps with the path planning 

- artificial potential field (APF)
- A* and D* searches
- RRT 

their main goal &rarr; find paths through obstacle fields

---

<!-- .slide: data-background="white" -->

consider weighted A* example:

<img src="img/object/avoidance/Weighted_A_star_with_eps_5.gif" height="500">

---

let's look at some **obstacle avoidance algorithms**

---


### Classical/Geometric Methods

---

### Classical/Geometric Methods

**track the geometry** using "physics-based" concepts (_e.g.,_  APF)

---

### Classical/Geometric Methods

**track the geometry** using "physics-based" concepts (_e.g.,_  APF)

<br>

### [Vector-Field Histogram (VFH)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137)

---

### [Vector-Field Histogram (VFH)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137)

- build a **histogram of obstacle densities** 

---

### Vector-Field Histogram (VFH)

- build a **histogram of obstacle densities** 
- choose **low-density paths**

---

### Vector-Field Histogram (VFH)

1. identify obstacles &rarr; based on range sensor readings

<img src="img/object/avoidance/vfh.range_readings.png" width="1100">

---

### Vector-Field Histogram (VFH)

2. compute **polar density histograms** 

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="700">

---

### Vector-Field Histogram (VFH)

2. compute **polar density histograms** 

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="700">

identify obstacle location/proximity

---

### Vector-Field Histogram (VFH)

3. convert to **binary histograms**

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="500">
<img src="img/object/avoidance/vhf.histograms.png" height="500">

---

### Vector-Field Histogram (VFH)

3. convert to **binary histograms**

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="500">
<img src="img/object/avoidance/vhf.histograms.png" height="500">

indicates **valid steering directions** for robot

---

read the [original paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137) and [more details](https://web.eecs.utk.edu/~leparker/Courses/CS594-fall08/Lectures/Oct-21-Obstacle-Avoidance-I.pdf)

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 
- selects **safe** trajectories 
    - based on dynamic constraints

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 
- selects **safe** trajectories 
    - based on dynamic constraints

[read the textbook](https://autonomy-course.github.io/textbook/autonomy-textbook.html#classicalgeometric-methods) for more details and references

---

### Model Predictive Control (MPC)

---

### Model Predictive Control (MPC)

- advanced **process control** 

---

### Model Predictive Control (MPC)

- advanced **process control** 
- while **satisfying a set of constraints**

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 
- only implementing the current timeslot 

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 
- only implementing the current timeslot 
- optimizing **repeatedly**

---

### Model Predictive Control (MPC)

<br>

<img src="img/object/avoidance/MPC_scheme_basic.svg" width="1200">

---

### Model Predictive Control (MPC)

<br>

<img src="img/object/avoidance/MPC_scheme_basic.svg" width="1200">


can **predict future events** &rarr; and react accordingly

---

at time, $t$, 

---

at time, $t$, 

- current plant state is sampled

---

at time, $t$, 

- current plant state is sampled
- cost minimizing control strategy is computed*

(* numerical minimization algorithm) 

---

at time, $t$, 

- current plant state is sampled
- cost minimizing control strategy is computed
- for a relatively short **future** time horizon, $[t+T]$

---

explore state trajectories &rarr; emanating from current state


---

explore state trajectories &rarr; emanating from current state

find **cost-minimizing control strategy*** &rarr; until time $[t+T]$

---

explore state trajectories &rarr; emanating from current state

find **cost-minimizing control strategy*** &rarr; until time $[t+T]$

<br>

(* using [Euler–Lagrange equations](https://en.wikipedia.org/wiki/Euler–Lagrange_equation))

---

once control strategy is found...

---

once control strategy is found...

- only **first step is implemented**

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 
- calculations **repeated** &rarr; starting from new current state

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 
- calculations **repeated** &rarr; starting from new current state
- yields a **new control** and **new predicted state path** 

---

prediction horizon &rarr; keeps being shifted forward 

"**receding horizon control**"

---

MPC is **not** optimal

shows very good results in practice

---

### MPC for obstacle avoidance

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process
- once model is established &rarr; set up **control loop**

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process
- once model is established &rarr; set up **control loop**

<img src="img/object/avoidance/mpc.controller.png" width="1200">

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$

</div>

</div>

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 

</div>

</div>

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 
- to close the loop with vehicle dynamics

</div>

</div>


---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 
- to close the loop with vehicle dynamics

</div>

</div>

<br>

this computes &rarr; **predicted state trajectory**, $\mathbf{x}(t)$

---

<img src="img/object/avoidance/mpc.model.png" height="800">

---

<img src="img/object/avoidance/mpc.model.png" height="800">

feasibility checked against **vehicle and environmental constraints**

---

<img src="img/object/avoidance/mpc.model.png" height="800">

feasibility checked against **vehicle and environmental constraints**

_e.g.,_  **rollover** and **obstacle avoidance** constraints

---

MPC often works with path planning algorithms (_e.g.,_  RRT) 

---

MPC often works with path planning algorithms (_e.g.,_  RRT) 

### [Closed-Loop RRT (CL-RRT)](https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&isAllowed=y) 

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT)

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**
- at end of tree growing phase &rarr; **best trajectory** is chosen

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**
- at end of tree growing phase &rarr; **best trajectory** is chosen
- cycle **repeats**

---

**quality** of results &rarr; depends on **sampling strategies**

---

**quality** of results &rarr; depends on **sampling strategies**

purely random sampling &rarr; large numbers of wasted samples

---

CL-RRT examples


---

CL-RRT examples

|situation| details | |
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | |

<img src="img/object/avoidance/clrrt.left_turn.png" width="1000">

---

CL-RRT examples

|situation| details | image|
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | <img src="img/object/avoidance/clrrt.left_turn.png" width="200">|
| parking lot | goal is center right edge | |

<img src="img/object/avoidance/clrrt.parking_lot.png" width="700">

---

CL-RRT examples

|situation| details | |
|:--------|:--------|:----:|
| u-turn| facing (red) road blockage <br> white &rarr; blue samples are forward/back maneouvers| |
||

<img src="img/object/avoidance/clrrt.uturn.png" width="800">

---

CL-RRT examples

|situation| details | image|
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | <img src="img/object/avoidance/clrrt.left_turn.png" width="200">|
| parking lot | goal is center right edge | <img src="img/object/avoidance/clrrt.parking_lot.png" width="200">|
| u-turn| facing (red) road blockage <br> white and blues samples are forward/back maneouvers| <img src="img/object/avoidance/clrrt.uturn.png" width="200">|
||

---

read [original CL-RRT paper](https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&isAllowed=y) for more details/references

---

### Learning-Based Methods

---

### Learning-Based Methods

- autonomous vehicle can trace paths &rarr; **massive amounts of data** 

---

### Learning-Based Methods

- autonomous vehicle can trace paths &rarr; **massive amounts of data** 
- **adapt quickly** &rarr; changing scenarios/environments

---

multiple testing stages on **large data sets** 

of obstacles/environmental conditions 

---

ML-based solutions can even be **[mapless](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8202134)** 

---

traditional motion planners depend on,

- highly precise laser sensor and 
- obstacle map of navigation environment 

---

use **asynchronous deep reinforcement learning** 

---

use **asynchronous deep reinforcement learning** 

- mapless motion planner &rarr; trained end-to-end

---

use **asynchronous deep reinforcement learning** 

- mapless motion planner &rarr; trained end-to-end
- **without** manually designed features or prior demonstrations!

---

what is "**reinforcement learning**"?

---

### [Reinforcement Learning](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)


---

### Reinforcement Learning

- computational approach &rarr; **learning from interaction**

---

### Reinforcement Learning

- computational approach &rarr; **learning from interaction**
- focused on **goal-directed learning** from interaction

---

### Reinforcement Learning

- learning how to **map situations to actions** 

---

### Reinforcement Learning

- learning how to **map situations to actions** 
- maximize a **numerical reward signal**

---

they are **closed-loop** problems

---

they are **closed-loop** problems

learning system’s actions influence later inputs

---

learner is **not told** which actions to take 

---

learner is **not told** which actions to take 

must **discover** which actions yield **most reward** &rarr; by trying them out

---

actions may affect **future** situations and **all subsequent rewards**

---

three most important aspects of RL

1. **closed-loop** 
2. **no direct instructions** &rarr; what actions to take
3. consequences of actions+reward signals &rarr; **extended time periods**

Note:

One of the challenges &rarr; the **trade-off between exploration and exploitation**. To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has **tried in the past and found to be effective** in producing reward. But to discover such actions, it has to **try actions that it has not selected before**. The agent has to exploit what it already knows in order to obtain reward, but it also has to explore in order to make better action selections in the future. 

---

RL explicitly considers the **whole problem** of,

---

RL explicitly considers the **whole problem** of,

- a goal-directed agent 

---

RL explicitly considers the **whole problem** of,

- a goal-directed agent 
- interacting with an uncertain environment


---

RL explicitly considers the **whole problem** of,

- a goal-directed agent 
- interacting with an uncertain environment

<br>
<br>

**perfect mapping** to path finding/obstacle detection

---

RL &rarr; interdisciplinary area of machine learning and optimal control 

Note:
Consider this [example](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf):

> a mobile robot decides whether it should enter a new room in search of more trash to collect or start trying to find its way back to its battery recharging station. It makes its decision based on the current charge level of its battery and how quickly and easily it has been able to find the recharger in the past.

---

typical framing of RL problem

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="1100">

---

typical framing of RL problem

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="800">

</div>

<div>

<br>
<br>

- an agent takes actions in an environment

</div>

</div>


---

typical framing of RL problem

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="800">

</div>

<div>

<br>
<br>

- an agent takes actions in an environment
- interpreted into **reward** and **state representation** 

</div>

</div>


---

typical framing of RL problem

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="800">

</div>

<div>

<br>
<br>

- an agent takes actions in an environment
- interpreted into **reward** and **state representation** 
- fed back to agent

</div>

</div>

---

simplest model for RL &rarr; [Markov Decision Process (MDP)](https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf) 

---

simplest model for RL &rarr; [Markov Decision Process (MDP)](https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf) 

optimization models &rarr; decision-making where outcomes are random

---

simplest model for RL 

||||
|----|:-----|:----|
| $S$ | state space | set of **environment** and **agent states** |

---

simplest model for RL 

||||
|----|:-----|:----|
| $S$ | state space | set of **environment** and **agent states** |
| $A$ | action space | set of **actions** of agent |

---

simplest model for RL 

||||
|----|:-----|:----|
| $S$ | state space | set of **environment** and **agent states** |
| $A$ | action space | set of **actions** of agent |
| $P_a\left(s, s^{\prime}\right)$| transition probability (at time $t$) | from state $s$ to $s^{\prime}$ under action $a$|

<br>

$P_a\left(s, s^{\prime}\right)=\operatorname{Pr}\left(S_{t+1}=s^{\prime} \mid S_t=s, A_t=a\right)$ 

---

simplest model for RL 

||||
|----|:-----|:----|
| $S$ | state space | set of **environment** and **agent states** |
| $A$ | action space | set of **actions** of agent |
| $P_a\left(s, s^{\prime}\right)$| transition probability (at time $t$) | from state $s$ to $s^{\prime}$ under action $a$|
| $R_a\left(s, s^{\prime}\right)$ | immediate reward | after transition from $s$ to $s^{\prime}$ under action $a$ |
||

---

purpose of RL agent 


---

purpose of RL agent 

- learn an optimal (or near-optimal) policy 

---

purpose of RL agent 

- learn an optimal (or near-optimal) policy 
- **maximizes reward function** or other reinforcement signal

---

purpose of RL agent 

- learn an optimal (or near-optimal) policy 
- **maximizes reward function** or other reinforcement signal
- accumulates from immediate rewards

---

RL agent interacts with environment &rarr; **discrete time steps**

---

each time step $t$, 


---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$

---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$
- chooses an action $A_t$ &rarr; from set of available actions

---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$
- chooses an action $A_t$ &rarr; from set of available actions
    - subsequently sent to environment

---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$
- chooses an action $A_t$ &rarr; from set of available actions
    - subsequently sent to environment
- environment &rarr; moves to new state $S_{t+1}$

---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$
- chooses an action $A_t$ &rarr; from set of available actions
    - subsequently sent to environment
- environment &rarr; moves to new state $S_{t+1}$
- reward $R_{t+1}$ associated with transition $\left(S_t, A_t, S_{t+1}\right)$ is determined

---

main **goal** of RL &rarr; **learn a "policy"**, 

---

main **goal** of RL &rarr; **learn a "policy"**, 

$$
\pi: \mathcal{S} \times \mathcal{A} \rightarrow[0,1], \pi(s, a)=\operatorname{Pr}\left(A_t=a \mid S_t=s\right)
$$

---

main **goal** of RL &rarr; **learn a "policy"**, 

$$
\pi: \mathcal{S} \times \mathcal{A} \rightarrow[0,1], \pi(s, a)=\operatorname{Pr}\left(A_t=a \mid S_t=s\right)
$$

that **maximizes the expected cumulative reward**

---

[textbook](https://autonomy-course.github.io/textbook/autonomy-textbook.html#learning-based-methods) has links to book on RL and MDP 

---

### reinforcement learning+Obstacle Detection

---

example | (asynchronous) RL 

---

example | (asynchronous) RL 

robot &rarr;  find way through **unfamiliar** terrain

---

example | (asynchronous) RL 

robot &rarr;  find way through **unfamiliar** terrain

<img src="img/object/avoidance/rl.example.png" width="1200">

---

### reinforcement learning+Obstacle Detection

comes down to &rarr; defining the **right reward function**

---

|reward functions||
|:-----|-----|

---

|reward functions||
|:-----|-----|
|distance **from** obstacle||

---


|reward functions||
|:-----|-----|
|distance **from** obstacle||
|speed/angular velocity ||

Note:
-Positive rewards can be given for maintaining a reasonable speed, and negative rewards for excessive angular velocity changes (which might lead to the agent getting stuck or making sharp turns). 


---


|reward functions||
|:-----|-----|
|distance **from** obstacle||
|speed/angular velocity ||
|collision penalties||

---

|reward functions||
|:-----|-----|
|distance **from** obstacle||
|speed/angular velocity ||
|collision penalties||
|goal reward ||

---

|reward functions||
|:-----|-----|
|distance **from** obstacle||
|speed/angular velocity ||
|collision penalties||
|goal reward ||
|time penalty||
||

---

|reward functions|reward type|
|:-----|-----|
|distance **from** obstacle||
|speed/angular velocity ||
|collision penalties||
|goal reward ||
|time penalty||
||

---

|reward functions|reward type|
|:-----|-----|
|distance **from** obstacle| **positive** |
|speed/angular velocity ||
|collision penalties||
|goal reward ||
|time penalty||
||


---

|reward functions|reward type|
|:-----|-----|
|distance **from** obstacle| **positive** |
|speed/angular velocity | **positive**/**negative**|
|collision penalties||
|goal reward ||
|time penalty||
||


---

|reward functions|reward type|
|:-----|-----|
|distance **from** obstacle| **positive** |
|speed/angular velocity | **positive**/**negative**|
|collision penalties| **negative**|
|goal reward ||
|time penalty||
||


---

|reward functions|reward type|
|:-----|-----|
|distance **from** obstacle| **positive** |
|speed/angular velocity | **positive**/**negative**|
|collision penalties| **negative**|
|goal reward | **positive**|
|time penalty||
||


---

|reward functions|reward type|
|:-----|-----|
|distance **from** obstacle| **positive** |
|speed/angular velocity | **positive**/**negative**|
|collision penalties| **negative**|
|goal reward | **positive**|
|time penalty| **negative**|
||

---

read the paper, ["Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8202134) by Tai et al. for earlier example

---

### [imitation Learning](https://arxiv.org/pdf/1604.07316) 

---

### [imitation Learning](https://arxiv.org/pdf/1604.07316) 

learn policies **from human demonstrations**

---

### imitation Learning

**automatically learns internal representations** 

---

### imitation Learning

**automatically learns internal representations** 

of necessary processing steps

---

### imitation Learning | example 

---

### imitation Learning | example 

- detecting useful road features

---

### imitation Learning | example 

- detecting useful road features 
- using **only human steering angles** as training signal!

---

high-level process

<img src="img/object/avoidance/imitation_learning.example.png" width="1200">

---

high-level process

<div class="multicolumn">

<div>

<br>

<img src="img/object/avoidance/imitation_learning.example.png" width="1200">

</div>

<div>

- images are fed into a CNN 

</div>

</div>
---

high-level process

<div class="multicolumn">

<div>

<br>

<img src="img/object/avoidance/imitation_learning.example.png" width="1200">

</div>

<div>

- images are fed into a CNN 
- compute steering command

</div>

</div>
---

high-level process

<div class="multicolumn">

<div>

<br>

<img src="img/object/avoidance/imitation_learning.example.png" width="1200">

</div>

<div>

- images are fed into a CNN 
- compute steering command
- compare desired command

</div>

</div>
---

high-level process

<div class="multicolumn">

<div>

<br>

<img src="img/object/avoidance/imitation_learning.example.png" width="1200">

</div>

<div>

- images are fed into a CNN 
- compute steering command
- compare desired command
- adjust CNN weights 

</div>

</div>
---

high-level process

<div class="multicolumn">

<div>

<br>

<img src="img/object/avoidance/imitation_learning.example.png" width="1200">

</div>

<div>

- images are fed into a CNN 
- compute steering command
- compare desired command
- adjust CNN weights 
- bring CNN output close to desired output

</div>

</div>
---

high-level process

<div class="multicolumn">

<div>

<br>

<img src="img/object/avoidance/imitation_learning.example.png" width="1200">

</div>

<div>

- images are fed into a CNN 
- compute steering command
- compare desired command
- adjust CNN weights 
- bring CNN output close to desired output
- back propagation &rarr; weight adjustment

</div>

</div>

---

once trained &rarr; **network generates steering** from video...

---

once trained &rarr; **network generates steering** from video...

...of a **single center camera**!

---

<img src="img/object/avoidance/imitation_learning.png" width="1200">

Note:
- Screen shot of the simulator in interactive mode. See Section 7.1 for explanation of the performance metrics. The green area on the left is unknown because of the viewpoint transformation. The highlighted wide rectangle below the horizon is the area which is sent to the CNN.

---

read the paper, ["end to end learning for self-driving cars"](https://arxiv.org/pdf/1604.07316) for more details

---


### Trajectory Calculations

Let's look at one [example](https://ieeexplore.ieee.org/document/8519525) where mathematical ("sigmoid functions") are used to estimate a smooth trajectory for avoiding an obstacle &rarr; in this case a moving car that's ahead of you in the same lane.

<details>
<summary>Sigmoid functions</summary>

A [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) is an **S-shaped mathematical function** that **maps any input** value to an output between `0` and `1`. 

<img src="img/object/avoidance/sigmoid.svg" width="300">

<br>

The most common sigmoid function is the logistic function:

$$
\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1 + e^x} = 1 - \sigma(-x)
$$

Key properties of sigmoid functions:

- smooth, continuous curve
- utput range limited to $(0,1)$
- approaches `0` as $x \rightarrow - \infty$ 
- approaches `1` as $x \rightarrow + \infty$
- has a derivative that is **always positive**
- steepest slope occurs at $x = 0$

</details>

<br>

This method avoids vehicles/obstacles by proposing a **smooth local modified trajectory** of a global path. They use a combination of,

- a parametrized sigmoid function and 
- a rolling horizon (a time-dependant model is solved repeatedly).

The main idea is to react to the obstacles but also to ensure a **smooth** response/trajectory. This is a **local** method that can work in conjunction with a global path planning/obstacle avoidane method. The reference trajectory is calculated simultaneously when the displacement is started.

One of the main considerations &rarr; **execution time** since it is imperative that the solution be calculated and implemented in a reall short amount of time.

Consider the following Sigmoid function:

$$
y(x) = \frac{1}{1 + e^{(-a(x-c))}}
$$

where,

|term|definition|
|-----|:----|
| $y(x)$| lateral offset of the vehicle |
| $x$ | position in longitudinal direction |
| $B# | the "way position", $P3$ <br> to generate obstacle avoidance manoeuvre |
| $c$ | modifies the shape of the function |
| $a$ | slope of the sigmoid |
||

<br>

The following figure shows the shape of the sigmoid ad curvature for various values of $a$,

<img src="img/object/avoidance/sigmoid.1.png" width="500">

<br>

The idea us to get from $P1 \rightarrow P3$, **via** $P2$.

- inputs &rarr; obstacle position and position of vehicle
- lateral offset &rarr; calculated based on these parameters

Hence, at a high level, the process looks like:

<img src="img/object/avoidance/sigmoid.2.png" width="400">

<br>

1. find a **circular area** around he obstacle &rarr; so that we can compute a **safe** region to avoid and
2. use the sigmoid functions to compute a **smooth trajectory** based on the circular region &rarr; make the transitions smoother and safer.

<br>

|||
|----|:----|
| $S$ | desired **lateral** safety distance |
| $S_m$ | **longitudinal** safety distance |
||

<br>

A "**horizon planning approach**" is used to compute the path by,

- dividing the drivable space into convex regions
- trajectory of each region is computed as the vehicle moves forward.

<br>

The various, incremental steps for the process,

1. approaching another car/obstacle &rarr; too far away to be a problem

<img src="img/object/avoidance/sigmoid.3.png" width="400">

<br>

2. object detected
    - safety circle calculated
    - smooth trajectory calculated
    our car (red) moves to new trajectory

<img src="img/object/avoidance/sigmoid.4.png" width="400">

<br>

**Note:**  the obstacle (blue car) has moved forward. So our calculations should account for this.

3. move past obstacle
    - we can start to move back to original path/lane
    - complete the trajectory

<img src="img/object/avoidance/sigmoid.5.png" width="400">

<br>

Read the full paper [Smooth Obstacle Avoidance Path Planning for Autonomous Vehicles](https://ieeexplore.ieee.org/document/8519525) by Ben-Messaoud et al for all the details.
