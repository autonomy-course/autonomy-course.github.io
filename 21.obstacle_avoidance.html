<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

        
            <link rel="icon" href="assets/sibin.integral.favicon.ico">
        

        <link rel="stylesheet" href="assets/reveal-js/dist/reveal.css" />

        
            <link rel="stylesheet" href="assets/custom.sibin.css" />
        
        
            <link rel="stylesheet" href="assets/monokai.css" />
        

        
            
        
            
        
    </head>
    <body>
        <div class="reveal">
            <div class="slides">
                <section data-markdown
                
                    data-separator="^\s*---\s*$"
                
                    data-separator-vertical="^\s*-v-\s*$"
                
                    data-separator-notes="^Notes?:"
                
                    data-charset="utf-8"
                
                    data-auto-animate="True"
                
                >
                    <textarea data-template>
                        # obstacle avoidance

## **Design of Autonomous Systems**
### csci 6907/4907-Section 86
### Prof. **Sibin Mohan**

---

so far...

---

methods to **detect** and **identify** objects

<img src="img/object/camera_bounding_boxes.gif" width="1400">

---

main goal &rarr; **do not collide** with objects in our path

<img src="img/object/avoidance/final_destination_logs.gif" width="1400">

---

two ways to achieve this goal

---

two ways to achieve this goal

<div class="multicolumn">

<div>

<br>

**stopping**

<img src="img/object/avoidance/stopping-flintstone.gif" height="500">


</div>

<div>

<br>

**alternate paths**

<img src="img/object/avoidance/racecar_overtake.gif" height="500">

</div>


</div>

---

### obstacle avoidance

---

### obstacle avoidance

> capability of robot or autonomous system to **detect and circumvent obstacles** in its path to reach a **predefined destination**


---

simplest way...

---

simplest way...

### use the **sensors**

---

### use the **sensors** 

- actively react to obstacles 

---

### use the **sensors** 

- actively react to obstacles 
- recalculate new paths

---

### use the **sensors** 

<br>

<div class="multicolumn">

<div>

<br>

- actively react to obstacles 
- recalculate new paths

</div>

<div>

<img src="img/object/avoidance/wiki_robot.gif" width="700" border="1">

</div>

</div>


---

**three simple steps**: 

---

**three simple steps**: 

- sense

---

**three simple steps**: 

- sense
- "think"

---

**three simple steps**: 

- sense
- "think"
- act

Note:
- inputs &rarr; distances of objects and provide the robot with data about its surroundings enabling it to detect obstacles and calculate their distances. The robot then adjusts its trajectory to navigate around obstacles while trying to reach its destination. This can be carried out in real-time 


---

**three simple steps**: 

- sense
- "think"
- act

works in "real-time" 

---

difficult in complex situations...

---

difficult in complex situations...

_e.g.,_ autonomous car in urban environmen

---

**contemporary** obstacle detection methods

---

**contemporary** obstacle detection methods

- reactive strategies
- global planners
- machine-learning based methods

---

obstacle avoidance &rarr; overlaps with the path planning 

---

obstacle avoidance &rarr; overlaps with the path planning 

- artificial potential field (APF)
- A* and D* searches
- RRT 

---

obstacle avoidance &rarr; overlaps with the path planning 

- artificial potential field (APF)
- A* and D* searches
- RRT 

their main goal &rarr; find paths through obstacle fields

---

<!-- .slide: data-background="white" -->

consider weighted A* example:

<img src="img/object/avoidance/Weighted_A_star_with_eps_5.gif" height="500">

---

let's look at some **obstacle avoidance algorithms**

---


### Classical/Geometric Methods

---

### Classical/Geometric Methods

**track the geometry** using "physics-based" concepts (_e.g.,_  APF)

---

### Classical/Geometric Methods

**track the geometry** using "physics-based" concepts (_e.g.,_  APF)

<br>

### [Vector-Field Histogram (VFH)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137)

---

### [Vector-Field Histogram (VFH)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137)

- build a **histogram of obstacle densities** 

---

### Vector-Field Histogram (VFH)

- build a **histogram of obstacle densities** 
- choose **low-density paths**

---

### Vector-Field Histogram (VFH)

1. identify obstacles &rarr; based on range sensor readings

<img src="img/object/avoidance/vfh.range_readings.png" width="1100">

---

### Vector-Field Histogram (VFH)

2. compute **polar density histograms** 

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="700">

---

### Vector-Field Histogram (VFH)

2. compute **polar density histograms** 

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="700">

identify obstacle location/proximity

---

### Vector-Field Histogram (VFH)

3. convert to **binary histograms**

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="500">
<img src="img/object/avoidance/vhf.histograms.png" height="500">

---

### Vector-Field Histogram (VFH)

3. convert to **binary histograms**

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="500">
<img src="img/object/avoidance/vhf.histograms.png" height="500">

indicates **valid steering directions** for robot

---

read the [original paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137) and [more details](https://web.eecs.utk.edu/~leparker/Courses/CS594-fall08/Lectures/Oct-21-Obstacle-Avoidance-I.pdf)

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 
- selects **safe** trajectories 
    - based on dynamic constraints

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 
- selects **safe** trajectories 
    - based on dynamic constraints

[read the textbook](https://autonomy-course.github.io/textbook/autonomy-textbook.html#classicalgeometric-methods) for more details and references

---

### Model Predictive Control (MPC)

---

### Model Predictive Control (MPC)

- advanced **process control** 

---

### Model Predictive Control (MPC)

- advanced **process control** 
- while **satisfying a set of constraints**

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 
- only implementing the current timeslot 

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 
- only implementing the current timeslot 
- optimizing **repeatedly**

---

### Model Predictive Control (MPC)

<br>

<img src="img/object/avoidance/MPC_scheme_basic.svg" width="1200">

---

### Model Predictive Control (MPC)

<br>

<img src="img/object/avoidance/MPC_scheme_basic.svg" width="1200">


can **predict future events** &rarr; and react accordingly

---

at time, $t$, 

---

at time, $t$, 

- current plant state is sampled

---

at time, $t$, 

- current plant state is sampled
- cost minimizing control strategy is computed*

(* numerical minimization algorithm) 

---

at time, $t$, 

- current plant state is sampled
- cost minimizing control strategy is computed
- for a relatively short **future** time horizon, $[t+T]$

---

explore state trajectories &rarr; emanating from current state


---

explore state trajectories &rarr; emanating from current state

find **cost-minimizing control strategy*** &rarr; until time $[t+T]$

---

explore state trajectories &rarr; emanating from current state

find **cost-minimizing control strategy*** &rarr; until time $[t+T]$

<br>

(* using [Euler–Lagrange equations](https://en.wikipedia.org/wiki/Euler–Lagrange_equation))

---

once control strategy is found...

---

once control strategy is found...

- only **first step is implemented**

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 
- calculations **repeated** &rarr; starting from new current state

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 
- calculations **repeated** &rarr; starting from new current state
- yields a **new control** and **new predicted state path** 

---

prediction horizon &rarr; keeps being shifted forward 

"**receding horizon control**"

---

MPC is **not** optimal

shows very good results in practice

---

### MPC for obstacle avoidance

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process
- once model is established &rarr; set up **control loop**

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process
- once model is established &rarr; set up **control loop**

<img src="img/object/avoidance/mpc.controller.png" width="1200">

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$

</div>

</div>

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 

</div>

</div>

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 
- to close the loop with vehicle dynamics

</div>

</div>


---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 
- to close the loop with vehicle dynamics

</div>

</div>

<br>

this computes &rarr; **predicted state trajectory**, $\mathbf{x}(t)$

---

<img src="img/object/avoidance/mpc.model.png" height="800">

---

<img src="img/object/avoidance/mpc.model.png" height="800">

feasibility checked against **vehicle and environmental constraints**

---

<img src="img/object/avoidance/mpc.model.png" height="800">

feasibility checked against **vehicle and environmental constraints**

\eg **rollover** and **obstacle avoidance** constraints

---

MPC often works with path planning algorithms (\eg RRT) 

---

MPC often works with path planning algorithms (\eg RRT) 

### [Closed-Loop RRT (CL-RRT)](https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&isAllowed=y) 

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT)

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**
- at end of tree growing phase &rarr; **best trajectory** is chosen

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**
- at end of tree growing phase &rarr; **best trajectory** is chosen
- cycle **repeats**

---

**quality** of results &rarr; depends on **sampling strategies**

---

**quality** of results &rarr; depends on **sampling strategies**

purely random sampling &rarr; large numbers of wasted samples

---

CL-RRT examples


---

CL-RRT examples

|situation| details | |
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | |

<img src="img/object/avoidance/clrrt.left_turn.png" width="1000">

---

CL-RRT examples

|situation| details | image|
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | <img src="img/object/avoidance/clrrt.left_turn.png" width="200">|
| parking lot | goal is center right edge | |

<img src="img/object/avoidance/clrrt.parking_lot.png" width="700">

---

CL-RRT examples

|situation| details | |
|:--------|:--------|:----:|
| u-turn| facing (red) road blockage <br> white &rarr; blue samples are forward/back maneouvers| |
||

<img src="img/object/avoidance/clrrt.uturn.png" width="800">

---

CL-RRT examples

|situation| details | image|
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | <img src="img/object/avoidance/clrrt.left_turn.png" width="200">|
| parking lot | goal is center right edge | <img src="img/object/avoidance/clrrt.parking_lot.png" width="200">|
| u-turn| facing (red) road blockage --- white and blues samples are forward/back maneouvers| <img src="img/object/avoidance/clrrt.uturn.png" width="200">|
||

---

read [original CL-RRT paper](https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&isAllowed=y) for more details/references

---

### Learning-Based Methods

---

### Learning-Based Methods

- autonomous vehicle can trace paths &rarr; **massive amounts of data** 

---

### Learning-Based Methods

- autonomous vehicle can trace paths &rarr; **massive amounts of data** 
- **adapt quickly** &rarr; changing scenarios/environments

---

multiple testing stages on **large data sets** 

of obstacles/environmental conditions 

---

ML-based solutions can even be **[mapless](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8202134)** 

---

traditional motion planners depend on,

- highly precise laser sensor and 
- obstacle map of navigation environment 

---

use **asynchronous deep reinforcement learning** 

---

use **asynchronous deep reinforcement learning** 

- mapless motion planner &rarr; trained end-to-end

---

use **asynchronous deep reinforcement learning** 

- mapless motion planner &rarr; trained end-to-end
- **without** manually designed features or prior demonstrations!

---

what is "**reinforcement learning**"?

---

### [Reinforcement Learning](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)


---

### Reinforcement Learning

- computational approach &rarr; **learning from interaction**

---

### Reinforcement Learning

- computational approach &rarr; **learning from interaction**
- focused on **goal-directed learning** from interaction

---

### Reinforcement Learning

- learning how to **map situations to actions** 

---

### Reinforcement Learning

- learning how to **map situations to actions** 
- maximize a **numerical reward signal**

---

they are **closed-loop** problems

---

they are **closed-loop** problems

learning system’s actions influence later inputs

---

learner is **not told** which actions to take 

---

learner is **not told** which actions to take 

must **discover** which actions yield **most reward** &rarr; by trying them out

---

actions may affect **future** situations and **all subsequent rewards**

---

three most important aspects of RL

1. **closed-loop** 
2. **no direct instructions** &rarr; what actions to take
3. consequences of actions+reward signals &rarr; **extended time periods**

Note:

One of the challenges &rarr; the **trade-off between exploration and exploitation**. To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has **tried in the past and found to be effective** in producing reward. But to discover such actions, it has to **try actions that it has not selected before**. The agent has to exploit what it already knows in order to obtain reward, but it also has to explore in order to make better action selections in the future. 

---

RL explicitly considers the **whole problem** of,

---

RL explicitly considers the **whole problem** of,

- a goal-directed agent 

---

RL explicitly considers the **whole problem** of,

- a goal-directed agent 
- interacting with an uncertain environment


---

RL explicitly considers the **whole problem** of,

- a goal-directed agent 
- interacting with an uncertain environment

<br>
<br>

**perfect mapping** to path finding/obstacle detection

---

RL &rarr; interdisciplinary area of machine learning and optimal control 

Note:
Consider this [example](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf):

> a mobile robot decides whether it should enter a new room in search of more trash to collect or start trying to find its way back to its battery recharging station. It makes its decision based on the current charge level of its battery and how quickly and easily it has been able to find the recharger in the past.

---

typical framing of RL problem

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="1100">

---

typical framing of RL problem

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="800">

</div>

<div>

<br>
<br>

- an agent takes actions in an environment

</div>

</div>


---

typical framing of RL problem

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="800">

</div>

<div>

<br>
<br>

- an agent takes actions in an environment
- interpreted into **reward** and **state representation** 

</div>

</div>


---

typical framing of RL problem

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="800">

</div>

<div>

<br>
<br>

- an agent takes actions in an environment
- interpreted into **reward** and **state representation** 
- fed back to agent

</div>

</div>

---

simplest model for RL &rarr; [Markov Decision Process (MDP)](https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf) 

---

simplest model for RL &rarr; [Markov Decision Process (MDP)](https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf) 

optimization models &rarr; decision-making where outcomes are random

---

simplest model for RL 

||||
|----|:-----|:----|
| $S$ | state space | set of **environment** and **agent states** |

---

simplest model for RL 

||||
|----|:-----|:----|
| $S$ | state space | set of **environment** and **agent states** |
| $A$ | action space | set of **actions** of agent |

---

simplest model for RL 

||||
|----|:-----|:----|
| $S$ | state space | set of **environment** and **agent states** |
| $A$ | action space | set of **actions** of agent |
| $P_a\left(s, s^{\prime}\right)$| transition probability (at time $t$) | from state $s$ to $s^{\prime}$ under action $a$|

<br>

$P_a\left(s, s^{\prime}\right)=\operatorname{Pr}\left(S_{t+1}=s^{\prime} \mid S_t=s, A_t=a\right)$ 

---

simplest model for RL 

||||
|----|:-----|:----|
| $S$ | state space | set of **environment** and **agent states** |
| $A$ | action space | set of **actions** of agent |
| $P_a\left(s, s^{\prime}\right)$| transition probability (at time $t$) | from state $s$ to $s^{\prime}$ under action $a$|
| $R_a\left(s, s^{\prime}\right)$ | immediate reward | after transition from $s$ to $s^{\prime}$ under action $a$ |
||

---

purpose of RL agent 


---

purpose of RL agent 

- learn an optimal (or near-optimal) policy 

---

purpose of RL agent 

- learn an optimal (or near-optimal) policy 
- **maximizes reward function** or other reinforcement signal

---

purpose of RL agent 

- learn an optimal (or near-optimal) policy 
- **maximizes reward function** or other reinforcement signal
- accumulates from immediate rewards

---

RL agent interacts with environment &rarr; **discrete time steps**

---

each time step $t$, 


---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$

---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$
- chooses an action $A_t$ &rarr; from set of available actions

---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$
- chooses an action $A_t$ &rarr; from set of available actions
    - subsequently sent to environment

---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$
- chooses an action $A_t$ &rarr; from set of available actions
    - subsequently sent to environment
- environment &rarr; moves to new state $S_{t+1}$

---

each time step $t$, 

- agent receives &rarr; current state $S_t$ and reward $R_t$
- chooses an action $A_t$ &rarr; from set of available actions
    - subsequently sent to environment
- environment &rarr; moves to new state $S_{t+1}$
- reward $R_{t+1}$ associated with transition $\left(S_t, A_t, S_{t+1}\right)$ is determined

---

main **goal** of RL &rarr; **learn a "policy"**, 

---

main **goal** of RL &rarr; **learn a "policy"**, 

$$
\pi: \mathcal{S} \times \mathcal{A} \rightarrow[0,1], \pi(s, a)=\operatorname{Pr}\left(A_t=a \mid S_t=s\right)
$$

---

main **goal** of RL &rarr; **learn a "policy"**, 

$$
\pi: \mathcal{S} \times \mathcal{A} \rightarrow[0,1], \pi(s, a)=\operatorname{Pr}\left(A_t=a \mid S_t=s\right)
$$

that **maximizes the expected cumulative reward**

---

[textbook](https://autonomy-course.github.io/textbook/autonomy-textbook.html#learning-based-methods) has links to book on RL and MDP 

---

### reinforcement learning+Obstacle Detection

---

### reinforcement learning+Obstacle Detection

comes down to &rarr; defining the **right reward function**

---
                    </textarea>
                </section>
            </div>
        </div>
        <script src="assets/reveal-js/dist/reveal.js"></script>
        <script src="assets/reveal-js/plugin/markdown/markdown.js"></script>
        <script src="assets/reveal-js/plugin/highlight/highlight.js"></script>
        <script src="assets/reveal-js/plugin/zoom/zoom.js"></script>
        <script src="assets/reveal-js/plugin/notes/notes.js"></script>
        <script src="assets/reveal-js/plugin/math/math.js"></script>

        
            
                
                    
                        <script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin/plugin/mermaid/mermaid.min.js"></script>
                    
                
            
                
                    
                        <script src="https://cdn.jsdelivr.net/npm/reveal-plantuml/dist/reveal-plantuml.min.js"></script>
                    
                
            
        

        <script>
            Reveal.initialize({
                
                    
                        history: true,
                    
                        slideNumber: "c/t",
                    
                        height: 1080,
                    
                        width: 1920,
                    
                        transition: "fade",
                    
                        backgroundTransition: "slide",
                    
                        RevealMermaid: {"htmlLabels": true, "useMaxWidth": true},
                    
                
                plugins: [
                    RevealMarkdown,
                    RevealHighlight,
                    RevealZoom,
                    RevealNotes,
                    RevealMath,

                    
                        
                            
                                RevealMermaid,
                            
                        
                            
                        
                    
                ],
            });
        </script>
    </body>
</html>