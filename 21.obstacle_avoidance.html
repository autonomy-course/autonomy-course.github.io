<!DOCTYPE html>
<html>
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no" />

        
            <link rel="icon" href="assets/sibin.integral.favicon.ico">
        

        <link rel="stylesheet" href="assets/reveal-js/dist/reveal.css" />

        
            <link rel="stylesheet" href="assets/custom.sibin.css" />
        
        
            <link rel="stylesheet" href="assets/monokai.css" />
        

        
            
        
            
        
    </head>
    <body>
        <div class="reveal">
            <div class="slides">
                <section data-markdown
                
                    data-separator="^\s*---\s*$"
                
                    data-separator-vertical="^\s*-v-\s*$"
                
                    data-separator-notes="^Notes?:"
                
                    data-charset="utf-8"
                
                    data-auto-animate="True"
                
                >
                    <textarea data-template>
                        # obstacle avoidance

## **Design of Autonomous Systems**
### csci 6907/4907-Section 86
### Prof. **Sibin Mohan**

---

so far...

---

methods to **detect** and **identify** objects

<img src="img/object/camera_bounding_boxes.gif" width="1400">

---

main goal &rarr; **do not collide** with objects in our path

<img src="img/object/avoidance/final_destination_logs.gif" width="1400">

---

two ways to achieve this goal

---

two ways to achieve this goal

<div class="multicolumn">

<div>

<br>

**stopping**

<img src="img/object/avoidance/stopping-flintstone.gif" height="500">


</div>

<div>

<br>

**alternate paths**

<img src="img/object/avoidance/racecar_overtake.gif" height="500">

</div>


</div>

---

### obstacle avoidance

---

### obstacle avoidance

> capability of robot or autonomous system to **detect and circumvent obstacles** in its path to reach a **predefined destination**


---

simplest way...

---

simplest way...

### use the **sensors**

---

### use the **sensors** 

- actively react to obstacles 

---

### use the **sensors** 

- actively react to obstacles 
- recalculate new paths

---

### use the **sensors** 

<br>

<div class="multicolumn">

<div>

<br>

- actively react to obstacles 
- recalculate new paths

</div>

<div>

<img src="img/object/avoidance/wiki_robot.gif" width="700" border="1">

</div>

</div>


---

**three simple steps**: 

---

**three simple steps**: 

- sense

---

**three simple steps**: 

- sense
- "think"

---

**three simple steps**: 

- sense
- "think"
- act

Note:
- inputs &rarr; distances of objects and provide the robot with data about its surroundings enabling it to detect obstacles and calculate their distances. The robot then adjusts its trajectory to navigate around obstacles while trying to reach its destination. This can be carried out in real-time 


---

**three simple steps**: 

- sense
- "think"
- act

works in "real-time" 

---

difficult in complex situations...

---

difficult in complex situations...

_e.g.,_ autonomous car in urban environmen

---

**contemporary** obstacle detection methods

---

**contemporary** obstacle detection methods

- reactive strategies
- global planners
- machine-learning based methods

---

obstacle avoidance &rarr; overlaps with the path planning 

---

obstacle avoidance &rarr; overlaps with the path planning 

- artificial potential field (APF)
- A* and D* searches
- RRT 

---

obstacle avoidance &rarr; overlaps with the path planning 

- artificial potential field (APF)
- A* and D* searches
- RRT 

their main goal &rarr; find paths through obstacle fields

---

<!-- .slide: data-background="white" -->

consider weighted A* example:

<img src="img/object/avoidance/Weighted_A_star_with_eps_5.gif" height="500">

---

let's look at some **obstacle avoidance algorithms**

---


### Classical/Geometric Methods

---

### Classical/Geometric Methods

**track the geometry** using "physics-based" concepts (_e.g.,_  APF)

---

### Classical/Geometric Methods

**track the geometry** using "physics-based" concepts (_e.g.,_  APF)

<br>

### [Vector-Field Histogram (VFH)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137)

---

### [Vector-Field Histogram (VFH)](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137)

- build a **histogram of obstacle densities** 

---

### Vector-Field Histogram (VFH)

- build a **histogram of obstacle densities** 
- choose **low-density paths**

---

### Vector-Field Histogram (VFH)

1. identify obstacles &rarr; based on range sensor readings

<img src="img/object/avoidance/vfh.range_readings.png" width="1100">

---

### Vector-Field Histogram (VFH)

2. compute **polar density histograms** 

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="700">

---

### Vector-Field Histogram (VFH)

2. compute **polar density histograms** 

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="700">

identify obstacle location/proximity

---

### Vector-Field Histogram (VFH)

3. convert to **binary histograms**

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="500">
<img src="img/object/avoidance/vhf.histograms.png" height="500">

---

### Vector-Field Histogram (VFH)

3. convert to **binary histograms**

<br>
<br>

<img src="img/object/avoidance/vfh.histogram_density.png" height="500">
<img src="img/object/avoidance/vhf.histograms.png" height="500">

indicates **valid steering directions** for robot

---

read the [original paper](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=88137) and [more details](https://web.eecs.utk.edu/~leparker/Courses/CS594-fall08/Lectures/Oct-21-Obstacle-Avoidance-I.pdf)

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 
- selects **safe** trajectories 
    - based on dynamic constraints

---

other **geometric approaches** &rarr; [dynamic window approach](https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf) 

- samples **velocity space** 
- selects **safe** trajectories 
    - based on dynamic constraints

[read the textbook](https://autonomy-course.github.io/textbook/autonomy-textbook.html#classicalgeometric-methods) for more details and references

---

### Model Predictive Control (MPC)

---

### Model Predictive Control (MPC)

- advanced **process control** 

---

### Model Predictive Control (MPC)

- advanced **process control** 
- while **satisfying a set of constraints**

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 
- only implementing the current timeslot 

---

### Model Predictive Control (MPC)

optimize **current timeslot** &rarr; accounting for **future timeslots**

- optimizing **finite time-horizon** 
- only implementing the current timeslot 
- optimizing **repeatedly**

---

### Model Predictive Control (MPC)

<br>

<img src="img/object/avoidance/MPC_scheme_basic.svg" width="1200">

---

### Model Predictive Control (MPC)

<br>

<img src="img/object/avoidance/MPC_scheme_basic.svg" width="1200">


can **predict future events** &rarr; and react accordingly

---

at time, $t$, 

---

at time, $t$, 

- current plant state is sampled

---

at time, $t$, 

- current plant state is sampled
- cost minimizing control strategy is computed*

(* numerical minimization algorithm) 

---

at time, $t$, 

- current plant state is sampled
- cost minimizing control strategy is computed
- for a relatively short **future** time horizon, $[t+T]$

---

explore state trajectories &rarr; emanating from current state


---

explore state trajectories &rarr; emanating from current state

find **cost-minimizing control strategy*** &rarr; until time $[t+T]$

---

explore state trajectories &rarr; emanating from current state

find **cost-minimizing control strategy*** &rarr; until time $[t+T]$

<br>

(* using [Euler–Lagrange equations](https://en.wikipedia.org/wiki/Euler–Lagrange_equation))

---

once control strategy is found...

---

once control strategy is found...

- only **first step is implemented**

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 
- calculations **repeated** &rarr; starting from new current state

---

once control strategy is found...

- only **first step is implemented**
- plant state &rarr; **sampled again** 
- calculations **repeated** &rarr; starting from new current state
- yields a **new control** and **new predicted state path** 

---

prediction horizon &rarr; keeps being shifted forward 

"**receding horizon control**"

---

MPC is **not** optimal

shows very good results in practice

---

### MPC for obstacle avoidance

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process
- once model is established &rarr; set up **control loop**

---

### MPC for obstacle avoidance

- MPC &rarr; relies on **dynamic models** of process
- once model is established &rarr; set up **control loop**

<img src="img/object/avoidance/mpc.controller.png" width="1200">

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$

</div>

</div>

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 

</div>

</div>

---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 
- to close the loop with vehicle dynamics

</div>

</div>


---

### MPC for obstacle avoidance

<br>

<div class="multicolumn">

<div>

<img src="img/object/avoidance/mpc.controller.png" width="1200">

</div>

<div>

- given a reference command, $\mathbf{r}$
- controller generates high rate **vehicle commands**, $\mathbf{u}$ 
- to close the loop with vehicle dynamics

</div>

</div>

<br>

this computes &rarr; **predicted state trajectory**, $\mathbf{x}(t)$

---

<img src="img/object/avoidance/mpc.model.png" height="800">

---

<img src="img/object/avoidance/mpc.model.png" height="800">

feasibility checked against **vehicle and environmental constraints**

---

<img src="img/object/avoidance/mpc.model.png" height="800">

feasibility checked against **vehicle and environmental constraints**

\eg **rollover** and **obstacle avoidance** constraints

---

MPC often works with path planning algorithms (\eg RRT) 

---

MPC often works with path planning algorithms (\eg RRT) 

### [Closed-Loop RRT (CL-RRT)](https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&isAllowed=y) 

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT)

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**
- at end of tree growing phase &rarr; **best trajectory** is chosen

---

### Closed-Loop RRT (CL-RRT)

- grows a **tree of feasible trajectories** (using RRT) 
    - originating from current vehicle state
- attempts to reach a **specified goal set**
- at end of tree growing phase &rarr; **best trajectory** is chosen
- cycle **repeats**

---

**quality** of results &rarr; depends on **sampling strategies**

---

**quality** of results &rarr; depends on **sampling strategies**

purely random sampling &rarr; large numbers of wasted samples

---

CL-RRT examples


---

CL-RRT examples

|situation| details | |
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | |

<img src="img/object/avoidance/clrrt.left_turn.png" width="1000">

---

CL-RRT examples

|situation| details | image|
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | <img src="img/object/avoidance/clrrt.left_turn.png" width="200">|
| parking lot | goal is center right edge | |

<img src="img/object/avoidance/clrrt.parking_lot.png" width="700">

---

CL-RRT examples

|situation| details | |
|:--------|:--------|:----:|
| u-turn| facing (red) road blockage <br> white &rarr; blue samples are forward/back maneouvers| |
||

<img src="img/object/avoidance/clrrt.uturn.png" width="800">

---

CL-RRT examples

|situation| details | image|
|:--------|:--------|:----:|
| intersection | vehicle trying to make a right turn | <img src="img/object/avoidance/clrrt.left_turn.png" width="200">|
| parking lot | goal is center right edge | <img src="img/object/avoidance/clrrt.parking_lot.png" width="200">|
| u-turn| facing (red) road blockage --- white and blues samples are forward/back maneouvers| <img src="img/object/avoidance/clrrt.uturn.png" width="200">|
||

---

read [original CL-RRT paper](https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&isAllowed=y) for more details/references

---


### Learning-Based Methods

With the advent of ML/AI techniques, an autonomous vehicle can trace a path to its destination using massive amounts of data. It can also **adapt quickly** to changing scenarios/environments. It can achieve this using many testing stages on large data sets of obstacles and environmental conditions. 

ML-based solutions can even be **[mapless](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8202134)**. Traditional motion planners for mobile ground robots with range sensors (_e.g.,_  LiDAR) mostly depend on the obstacle map of the navigation environment where both,

- the highly precise laser sensor and 
- the obstacle map building work of the environment 

are indispensable.

Using an asynchronous deep reinforcement learning method, a "mapless" motion planner can be trained end-to-end **without any manually designed features and prior demonstrations**!

1. **[Reinforcement Learning](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf)** is a **computational approach** to **learning from interaction**. These methods are  focused on **goal-directed learning from interaction**.

> Reinforcement learning problems involve learning what to do—how to map situations to actions -- so as to **maximize a numerical reward signal**. In an essential way they are **closed-loop** problems because the learning system’s actions influence its later inputs. Moreover, the **learner is not told which actions to take**, as in many forms of machine learning, but instead must **discover which actions yield the most reward by trying them out**. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, **all subsequent rewards**.

Three most important aspects of RL:

1. **closed-loop** in an essential way
2. **no direct instructions** as to what actions to take
3. consequences of actions, including reward signals, play out over **extended time periods**.

One of the challenges &rarr; the **trade-off between exploration and exploitation**. To obtain a lot of reward, a reinforcement learning agent must prefer actions that it has **tried in the past and found to be effective** in producing reward. But to discover such actions, it has to **try actions that it has not selected before**. The agent has to exploit what it already knows in order to obtain reward, but it also has to explore in order to make better action selections in the future. 


A key feature of RL is that it explicitly considers the **whole problem** of a goal-directed agent interacting with an uncertain environment &rarr; this is a perfect analogy for path finding/obstacle detection. 

RL is really an interdisciplinary area of machine learning and optimal control. 

Consider this [example](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf):

> a mobile robot decides whether it should enter a new room in search of more trash to collect or start trying to find its way back to its battery recharging station. It makes its decision based on the current charge level of its battery and how quickly and easily it has been able to find the recharger in the past.

Here is a [typical framing of the RL problem](https://en.wikipedia.org/wiki/Reinforcement_learning):

<img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="300">

---

In the above example, an agent takes actions in an environment, which is interpreted into a reward and a state representation, which are fed back to the agent.

The simplest model for RL uses a [Markov Decision Process (MDP)](https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf) &rarr; _i.e.,_  optimization models for modeling decision-making in situations where outcomes are random, _viz.,_ 

- a set of **environment** and **agent states** (the state space), $S$
- a set of **actions** (the action space), $A$, of the agent
- $P_a\left(s, s^{\prime}\right)=\operatorname{Pr}\left(S_{t+1}=s^{\prime} \mid S_t=s, A_t=a\right)$ &rarr; the transition probability (at time $t$) from state $s$ to $s^{\prime}$ under action $a$
- $R_a\left(s, s^{\prime}\right)$ &rarr; the immediate reward after transition from $s$ to $s^{\prime}$ under action $a$.

The purpose of RL &rarr; agent to learn an optimal (or near-optimal) policy that **maximizes reward function** or other (user-provided) reinforcement signal that accumulates from immediate rewards.

A basic reinforcement learning agent interacts with its environment in **discrete time steps**. At each time step $t$, 

- the agent receives the current state $S_t$ and reward $R_t$
- it chooses an action $A_t$ from the set of available actions
    - subsequently sent to the environment
- environment moves to a new state $S_{t+1}$
- the reward $R_{t+1}$ associated with transition $\left(S_t, A_t, S_{t+1}\right)$ is determined

---

Hence, the main **goal** of RL is to **learn a "policy"**, 

$$
\pi: \mathcal{S} \times \mathcal{A} \rightarrow[0,1], \pi(s, a)=\operatorname{Pr}\left(A_t=a \mid S_t=s\right)
$$

that **maximizes the expected cumulative reward**.

Read the book, ["Reinforcement Learning: An Introduction"](https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf) by Sutton and Barto for a more detailed introduction to RL. 

Here is a [primer on MDP](https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf) by Wang. 

---

**RL Applied to Obstacle Detection**

RL lends itself very nicely to the process of obstacle avoidance &rarr; _i.e.,_  finding a path through an area with multiple obstacles. It comes to defining the **right reward function**.

The following image shows a high-level flow of using (asynchronous) RL &rarr; for a robot to find its way through unfamiliar terrain.  

<img src="img/object/avoidance/rl.example.png" width="400">

---

Read the paper, ["Virtual-to-real Deep Reinforcement Learning: Continuous Control of Mobile Robots for Mapless Navigation](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=8202134) by Tai et al. for more details.

---

2. **[Imitation Learning](https://arxiv.org/pdf/1604.07316)** attempts to learn policies **from human demonstrations**.

For instance, a system **automatically learns internal representations** of the necessary processing steps (_e.g.,_  detecting useful road features) using **only human steering angles** as training signal!

Here is a high-level diagram of the process:

<img src="img/object/avoidance/imitation_learning.example.png" width="400">

---

The steps are:

- images are fed into a CNN 
- computes a proposed steering command
- proposed command is compared to the desired command for that image
- weights of the CNN are adjusted &rarr; to bring the CNN output closer to desired output
- weight adjustment is accomplished using back propagation.

Once trained, the network can generate steering from the video images of a **single center camera**!

Read the paper, [End to End Learning for Self-Driving Cars](https://arxiv.org/pdf/1604.07316) by Bojarski et al. for more details.


### Trajectory Calculations

Let's look at one [example](https://ieeexplore.ieee.org/document/8519525) where mathematical ("sigmoid functions") are used to estimate a smooth trajectory for avoiding an obstacle &rarr; in this case a moving car that's ahead of you in the same lane.

<details>
<summary>Sigmoid functions</summary>

A [Sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function) is an **S-shaped mathematical function** that **maps any input** value to an output between `0` and `1`. 

<img src="img/object/avoidance/sigmoid.svg" width="300">

---

The most common sigmoid function is the logistic function:

$$
\sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1 + e^x} = 1 - \sigma(-x)
$$

Key properties of sigmoid functions:

- smooth, continuous curve
- utput range limited to $(0,1)$
- approaches `0` as $x \rightarrow - \infty$ 
- approaches `1` as $x \rightarrow + \infty$
- has a derivative that is **always positive**
- steepest slope occurs at $x = 0$

</details>

---

This method avoids vehicles/obstacles by proposing a **smooth local modified trajectory** of a global path. They use a combination of,

- a parametrized sigmoid function and 
- a rolling horizon (a time-dependant model is solved repeatedly).

The main idea is to react to the obstacles but also to ensure a **smooth** response/trajectory. This is a **local** method that can work in conjunction with a global path planning/obstacle avoidane method. The reference trajectory is calculated simultaneously when the displacement is started.

One of the main considerations &rarr; **execution time** since it is imperative that the solution be calculated and implemented in a reall short amount of time.

Consider the following Sigmoid function:

$$
y(x) = \frac{1}{1 + e^{(-a(x-c))}}
$$

where,

|term|definition|
|-----|:----|
| $y(x)$| lateral offset of the vehicle |
| $x$ | position in longitudinal direction |
| $B# | the "way position", $P3$ --- to generate obstacle avoidance manoeuvre |
| $c$ | modifies the shape of the function |
| $a$ | slope of the sigmoid |
||

---

The following figure shows the shape of the sigmoid ad curvature for various values of $a$,

<img src="img/object/avoidance/sigmoid.1.png" width="500">

---

The idea us to get from $P1 \rightarrow P3$, **via** $P2$.

- inputs &rarr; obstacle position and position of vehicle
- lateral offset &rarr; calculated based on these parameters

Hence, at a high level, the process looks like:

<img src="img/object/avoidance/sigmoid.2.png" width="400">

---

1. find a **circular area** around he obstacle &rarr; so that we can compute a **safe** region to avoid and
2. use the sigmoid functions to compute a **smooth trajectory** based on the circular region &rarr; make the transitions smoother and safer.

---

|||
|----|:----|
| $S$ | desired **lateral** safety distance |
| $S_m$ | **longitudinal** safety distance |
||

---

A "**horizon planning approach**" is used to compute the path by,

- dividing the drivable space into convex regions
- trajectory of each region is computed as the vehicle moves forward.

---

The various, incremental steps for the process,

1. approaching another car/obstacle &rarr; too far away to be a problem

<img src="img/object/avoidance/sigmoid.3.png" width="400">

---

2. object detected
    - safety circle calculated
    - smooth trajectory calculated
    our car (red) moves to new trajectory

<img src="img/object/avoidance/sigmoid.4.png" width="400">

---

**Note:**  the obstacle (blue car) has moved forward. So our calculations should account for this.

3. move past obstacle
    - we can start to move back to original path/lane
    - complete the trajectory

<img src="img/object/avoidance/sigmoid.5.png" width="400">

---

Read the full paper [Smooth Obstacle Avoidance Path Planning for Autonomous Vehicles](https://ieeexplore.ieee.org/document/8519525) by Ben-Messaoud et al for all the details.
                    </textarea>
                </section>
            </div>
        </div>
        <script src="assets/reveal-js/dist/reveal.js"></script>
        <script src="assets/reveal-js/plugin/markdown/markdown.js"></script>
        <script src="assets/reveal-js/plugin/highlight/highlight.js"></script>
        <script src="assets/reveal-js/plugin/zoom/zoom.js"></script>
        <script src="assets/reveal-js/plugin/notes/notes.js"></script>
        <script src="assets/reveal-js/plugin/math/math.js"></script>

        
            
                
                    
                        <script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin/plugin/mermaid/mermaid.min.js"></script>
                    
                
            
                
                    
                        <script src="https://cdn.jsdelivr.net/npm/reveal-plantuml/dist/reveal-plantuml.min.js"></script>
                    
                
            
        

        <script>
            Reveal.initialize({
                
                    
                        history: true,
                    
                        slideNumber: "c/t",
                    
                        height: 1080,
                    
                        width: 1920,
                    
                        transition: "fade",
                    
                        backgroundTransition: "slide",
                    
                        RevealMermaid: {"htmlLabels": true, "useMaxWidth": true},
                    
                
                plugins: [
                    RevealMarkdown,
                    RevealHighlight,
                    RevealZoom,
                    RevealNotes,
                    RevealMath,

                    
                        
                            
                                RevealMermaid,
                            
                        
                            
                        
                    
                ],
            });
        </script>
    </body>
</html>