<!doctype html>
<html >
<head>

    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
        <!--[if lt IE 9]>
                <script src="http://css3-mediaqueries-js.googlecode.com/svn/trunk/css3-mediaqueries.js"></script>
        <![endif]-->
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <meta http-equiv="Content-Style-Type" content="text/css" />

    <!-- <link rel="stylesheet" type="text/css" href="template.css" /> -->
    <link rel="stylesheet" type="text/css" href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/template.css" />

    <link href="https://vjs.zencdn.net/5.4.4/video-js.css" rel="stylesheet" />

    <script src="https://code.jquery.com/jquery-2.2.1.min.js"></script>
    <!-- <script type='text/javascript' src='menu/js/jquery.cookie.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.hoverIntent.minified.js'></script> -->
    <!-- <script type='text/javascript' src='menu/js/jquery.dcjqaccordion.2.7.min.js'></script> -->

    <!-- <link href="menu/css/skins/blue.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/graphite.css" rel="stylesheet" type="text/css" /> -->
    <!-- <link href="menu/css/skins/grey.css" rel="stylesheet" type="text/css" /> -->

    <!-- <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script> -->


    <!-- <script src="script.js"></script> -->

    <!-- <script src="jquery.sticky-kit.js "></script> -->
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.cookie.js'></script>
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.hoverIntent.minified.js'></script>
    <script type='text/javascript' src='https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/js/jquery.dcjqaccordion.2.7.min.js'></script>

    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/blue.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/graphite.css" rel="stylesheet" type="text/css" />
    <link href="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/menu/css/skins/grey.css" rel="stylesheet" type="text/css" />
    <link href="./elegant_bootstrap.css" rel="stylesheet" type="text/css" />
    <!-- <link href="https://cdn.rawgit.com/ryangrose/easy-pandoc-templates/948e28e5/css/elegant_bootstrap.css" rel="stylesheet" type="text/css" /> -->

    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.4/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    <script src="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/script.js"></script>

    <script src="https://cdn.rawgit.com/diversen/pandoc-bootstrap-adaptive-template/959c3622/jquery.sticky-kit.js"></script>

    <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
    <meta name="generator" content="pandoc" />
  <meta name="author" content="Prof. Sibin Mohan, The George Washington University" />
  <meta name="date" content="2025-04-24" />
  <title>Design of Autonomous Systems</title>
  <style type="text/css">code{white-space: pre;}</style>
  <style type="text/css">
html { -webkit-text-size-adjust: 100%; }
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
  </style>
  <link rel="stylesheet" href="style.css" />
</head>
<body>


    <div class="navbar navbar-static-top">
    <div class="navbar-inner">
      <div class="container">
        <span class="doc-title">Design of Autonomous Systems</span>
        <ul class="nav pull-right doc-info">
                    <li><p class="navbar-text">Prof. Sibin Mohan, The
George Washington University</p></li>
                              <li><p class="navbar-text">April 24,
2025</p></li>
                  </ul>
      </div>
    </div>
  </div>
    <div class="container">
    <div class="row">
            <div id="TOC" class="span3">
        <div class="well toc">

        <ul>
        <li><a href="#introduction" id="toc-introduction"><span
        class="toc-section-number">1</span> introduction</a>
        <ul>
        <li><a href="#autonomy" id="toc-autonomy"><span
        class="toc-section-number">1.1</span> autonomy</a>
        <ul>
        <li><a href="#what-are-the-aspects-of-autonomy"
        id="toc-what-are-the-aspects-of-autonomy"><span
        class="toc-section-number">1.1.1</span> what are the
        <em>aspects</em> of autonomy?</a></li>
        </ul></li>
        <li><a href="#let-us-define-autonomy"
        id="toc-let-us-define-autonomy"><span
        class="toc-section-number">1.2</span> let us define
        <strong>autonomy</strong></a></li>
        <li><a href="#autonomous-systems"
        id="toc-autonomous-systems"><span
        class="toc-section-number">1.3</span> autonomous
        systems</a></li>
        <li><a href="#sensors-and-actuators"
        id="toc-sensors-and-actuators"><span
        class="toc-section-number">1.4</span> sensors and
        actuators…</a></li>
        <li><a href="#sensing-and-actuation-in-the-real-world"
        id="toc-sensing-and-actuation-in-the-real-world"><span
        class="toc-section-number">1.5</span> sensing and actuation in
        the real world</a>
        <ul>
        <li><a href="#come-back-to-sensing"
        id="toc-come-back-to-sensing"><span
        class="toc-section-number">1.5.1</span> Come back to
        <strong>sensing</strong></a></li>
        </ul></li>
        <li><a href="#overviewarchitecture-of-autonomous-systems"
        id="toc-overviewarchitecture-of-autonomous-systems"><span
        class="toc-section-number">1.6</span> Overview/Architecture of
        Autonomous Systems</a>
        <ul>
        <li><a href="#high-order-functions"
        id="toc-high-order-functions"><span
        class="toc-section-number">1.6.1</span> high-order
        functions</a></li>
        <li><a href="#slam" id="toc-slam"><span
        class="toc-section-number">1.6.2</span> slam</a></li>
        <li><a href="#waypoint-detection"
        id="toc-waypoint-detection"><span
        class="toc-section-number">1.6.3</span> waypoint
        detection</a></li>
        <li><a href="#yolo" id="toc-yolo"><span
        class="toc-section-number">1.6.4</span> yolo</a></li>
        <li><a href="#object-avoidance" id="toc-object-avoidance"><span
        class="toc-section-number">1.6.5</span> object
        avoidance</a></li>
        <li><a href="#path-planning" id="toc-path-planning"><span
        class="toc-section-number">1.6.6</span> path planning</a></li>
        <li><a href="#compute-platform" id="toc-compute-platform"><span
        class="toc-section-number">1.6.7</span> compute
        platform</a></li>
        <li><a href="#still-some-non-functional-requirements-remain"
        id="toc-still-some-non-functional-requirements-remain"><span
        class="toc-section-number">1.6.8</span> still some
        <strong>non-functional</strong> requirements remain</a></li>
        <li><a href="#safety" id="toc-safety"><span
        class="toc-section-number">1.6.9</span> safety!</a></li>
        <li><a href="#security" id="toc-security"><span
        class="toc-section-number">1.6.10</span> security</a></li>
        <li><a href="#course-structure" id="toc-course-structure"><span
        class="toc-section-number">1.6.11</span> Course
        Structure</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#embedded-architectures"
        id="toc-embedded-architectures"><span
        class="toc-section-number">2</span> Embedded Architectures</a>
        <ul>
        <li><a href="#the-wcet-problem" id="toc-the-wcet-problem"><span
        class="toc-section-number">2.1</span> The <strong>wcet</strong>
        problem</a></li>
        <li><a href="#embedded-processors"
        id="toc-embedded-processors"><span
        class="toc-section-number">2.2</span> Embedded Processors</a>
        <ul>
        <li><a href="#microcontrollers" id="toc-microcontrollers"><span
        class="toc-section-number">2.2.1</span>
        Microcontrollers</a></li>
        <li><a href="#digital-signal-processors-dsps"
        id="toc-digital-signal-processors-dsps"><span
        class="toc-section-number">2.2.2</span> Digital Signal
        Processors (DSPs)</a></li>
        <li><a href="#microprocessors" id="toc-microprocessors"><span
        class="toc-section-number">2.2.3</span> Microprocessors</a></li>
        <li><a href="#system-on-a-chip-soc"
        id="toc-system-on-a-chip-soc"><span
        class="toc-section-number">2.2.4</span> System-on-a-Chip
        (SoC)</a></li>
        <li><a href="#embedded-accelarators-e.g.-gpu-enabled-systems"
        id="toc-embedded-accelarators-e.g.-gpu-enabled-systems"><span
        class="toc-section-number">2.2.5</span> Embedded Accelarators
        (e.g. GPU-enabled systems)</a></li>
        <li><a href="#asics-and-fpgas" id="toc-asics-and-fpgas"><span
        class="toc-section-number">2.2.6</span> ASICs and FPGAs</a></li>
        </ul></li>
        <li><a href="#communication-and-io"
        id="toc-communication-and-io"><span
        class="toc-section-number">2.3</span> Communication and I/O</a>
        <ul>
        <li><a href="#uart-rs-232" id="toc-uart-rs-232"><span
        class="toc-section-number">2.3.1</span> UART | RS-232</a></li>
        <li><a href="#synchronous-i2c-and-spi"
        id="toc-synchronous-i2c-and-spi"><span
        class="toc-section-number">2.3.2</span> Synchronous |
        I<sup>2</sup>C and SPI</a></li>
        <li><a href="#general-purpose-io-gpio"
        id="toc-general-purpose-io-gpio"><span
        class="toc-section-number">2.3.3</span> General-Purpose I/O
        (GPIO)</a></li>
        <li><a href="#jtag-debugging-interface"
        id="toc-jtag-debugging-interface"><span
        class="toc-section-number">2.3.4</span> JTAG Debugging
        Interface</a></li>
        <li><a href="#controller-area-network-can"
        id="toc-controller-area-network-can"><span
        class="toc-section-number">2.3.5</span> Controller Area Network
        (CAN)</a></li>
        <li><a href="#other-broadly-used-protocols"
        id="toc-other-broadly-used-protocols"><span
        class="toc-section-number">2.3.6</span> Other Broadly Used
        Protocols</a></li>
        </ul></li>
        <li><a href="#raspberry-pi-and-navio2"
        id="toc-raspberry-pi-and-navio2"><span
        class="toc-section-number">2.4</span> Raspberry Pi and
        Navio2</a></li>
        <li><a href="#references" id="toc-references"><span
        class="toc-section-number">2.5</span> References</a></li>
        </ul></li>
        <li><a href="#sensors-and-sensing"
        id="toc-sensors-and-sensing"><span
        class="toc-section-number">3</span> Sensors and Sensing</a>
        <ul>
        <li><a href="#types-of-sensors" id="toc-types-of-sensors"><span
        class="toc-section-number">3.1</span> Types of Sensors</a>
        <ul>
        <li><a href="#inertial-measurement-units-imu"
        id="toc-inertial-measurement-units-imu"><span
        class="toc-section-number">3.1.1</span> Inertial Measurement
        Units (IMU)</a></li>
        <li><a
        href="#bouncing-of-electromagnetic-waves-lidar-and-mmwave"
        id="toc-bouncing-of-electromagnetic-waves-lidar-and-mmwave"><span
        class="toc-section-number">3.1.2</span> Bouncing of
        Electromagnetic Waves | LiDAR and mmWave</a></li>
        <li><a href="#ultrasonic" id="toc-ultrasonic"><span
        class="toc-section-number">3.1.3</span> Ultrasonic</a></li>
        </ul></li>
        <li><a href="#errors-in-sensing"
        id="toc-errors-in-sensing"><span
        class="toc-section-number">3.2</span> Errors in Sensing</a></li>
        <li><a href="#analog-to-digital-convertors-adcs"
        id="toc-analog-to-digital-convertors-adcs"><span
        class="toc-section-number">3.3</span> Analog to Digital
        Convertors (ADCs)</a>
        <ul>
        <li><a href="#adc-sampling-rate"
        id="toc-adc-sampling-rate"><span
        class="toc-section-number">3.3.1</span> ADC Sampling
        Rate</a></li>
        <li><a href="#adc-resolution" id="toc-adc-resolution"><span
        class="toc-section-number">3.3.2</span> ADC Resolution</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#real-time-operating-systems"
        id="toc-real-time-operating-systems"><span
        class="toc-section-number">4</span> Real-Time Operating
        Systems</a>
        <ul>
        <li><a href="#key-characteristics-for-rtos"
        id="toc-key-characteristics-for-rtos"><span
        class="toc-section-number">4.0.1</span> Key characteristics for
        RTOS</a></li>
        <li><a href="#kernels-in-rtos" id="toc-kernels-in-rtos"><span
        class="toc-section-number">4.1</span> Kernels in RTOS</a>
        <ul>
        <li><a href="#tasks-jobs-threads"
        id="toc-tasks-jobs-threads"><span
        class="toc-section-number">4.1.1</span> Tasks, Jobs,
        Threads</a></li>
        <li><a href="#inter-task-communication-and-synchronization"
        id="toc-inter-task-communication-and-synchronization"><span
        class="toc-section-number">4.1.2</span> (Inter-Task)
        Communication and Synchronization</a></li>
        <li><a href="#memory-management"
        id="toc-memory-management"><span
        class="toc-section-number">4.1.3</span> Memory
        Management</a></li>
        <li><a href="#timer-and-interrupt-management"
        id="toc-timer-and-interrupt-management"><span
        class="toc-section-number">4.1.4</span> Timer and Interrupt
        Management</a></li>
        <li><a href="#kernel-performance-metrics"
        id="toc-kernel-performance-metrics"><span
        class="toc-section-number">4.1.5</span> Kernel Performance
        Metrics</a></li>
        </ul></li>
        <li><a href="#examples-of-rtos" id="toc-examples-of-rtos"><span
        class="toc-section-number">4.2</span> Examples of RTOS</a>
        <ul>
        <li><a href="#freertos" id="toc-freertos"><span
        class="toc-section-number">4.2.1</span> FreeRTOS</a></li>
        <li><a href="#linuxreal-time" id="toc-linuxreal-time"><span
        class="toc-section-number">4.2.2</span> Linux+Real-Time</a></li>
        <li><a href="#raspberry-pi-osreal-time"
        id="toc-raspberry-pi-osreal-time"><span
        class="toc-section-number">4.2.3</span> Raspberry Pi
        OS+Real-Time</a></li>
        </ul></li>
        <li><a href="#robot-operating-system-ros"
        id="toc-robot-operating-system-ros"><span
        class="toc-section-number">4.3</span> Robot Operating System
        (ROS)</a>
        <ul>
        <li><a href="#ros-components" id="toc-ros-components"><span
        class="toc-section-number">4.3.1</span> ROS Components</a></li>
        <li><a href="#ros-and-real-time"
        id="toc-ros-and-real-time"><span
        class="toc-section-number">4.3.2</span> ROS and
        Real-time?</a></li>
        <li><a href="#rosnavio2" id="toc-rosnavio2"><span
        class="toc-section-number">4.3.3</span> Ros+Navio2</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#scheduling-for-real-time-systems"
        id="toc-scheduling-for-real-time-systems"><span
        class="toc-section-number">5</span> Scheduling for Real-Time
        Systems</a>
        <ul>
        <li><a href="#real-time-models" id="toc-real-time-models"><span
        class="toc-section-number">5.1</span> Real-Time Models</a>
        <ul>
        <li><a href="#workload-model" id="toc-workload-model"><span
        class="toc-section-number">5.1.1</span> Workload Model</a></li>
        <li><a href="#utilization" id="toc-utilization"><span
        class="toc-section-number">5.1.2</span> Utilization</a></li>
        <li><a href="#resource-model" id="toc-resource-model"><span
        class="toc-section-number">5.1.3</span> Resource Model</a></li>
        <li><a href="#scheduling-and-algorithms"
        id="toc-scheduling-and-algorithms"><span
        class="toc-section-number">5.1.4</span> Scheduling and
        Algorithms</a></li>
        </ul></li>
        <li><a href="#schedulers" id="toc-schedulers"><span
        class="toc-section-number">5.2</span> Schedulers</a>
        <ul>
        <li><a href="#cyclic-executives"
        id="toc-cyclic-executives"><span
        class="toc-section-number">5.2.1</span> Cyclic
        Executives</a></li>
        <li><a href="#frames" id="toc-frames"><span
        class="toc-section-number">5.2.2</span> Frames</a></li>
        <li><a href="#priority-based-schedulers"
        id="toc-priority-based-schedulers"><span
        class="toc-section-number">5.2.3</span> Priority-Based
        Schedulers</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#control-theory" id="toc-control-theory"><span
        class="toc-section-number">6</span> Control Theory</a>
        <ul>
        <li><a href="#control-theory-introduction"
        id="toc-control-theory-introduction"><span
        class="toc-section-number">6.1</span> Control Theory |
        Introduction</a>
        <ul>
        <li><a href="#open-loop-vs-closed-loop-control"
        id="toc-open-loop-vs-closed-loop-control"><span
        class="toc-section-number">6.1.1</span> Open-Loop vs Closed-Loop
        Control</a></li>
        </ul></li>
        <li><a href="#feedback-control" id="toc-feedback-control"><span
        class="toc-section-number">6.2</span> Feedback Control</a></li>
        <li><a href="#feedback-control-applied-to-lane-following"
        id="toc-feedback-control-applied-to-lane-following"><span
        class="toc-section-number">6.3</span> Feedback Control Applied
        to Lane Following</a></li>
        <li><a href="#pid-control" id="toc-pid-control"><span
        class="toc-section-number">6.4</span> PID Control</a>
        <ul>
        <li><a href="#proportional-p-control"
        id="toc-proportional-p-control"><span
        class="toc-section-number">6.4.1</span> Proportional (P)
        Control</a></li>
        <li><a href="#derivative-d-control"
        id="toc-derivative-d-control"><span
        class="toc-section-number">6.4.2</span> Derivative (D)
        Control</a></li>
        <li><a href="#integral-i-control"
        id="toc-integral-i-control"><span
        class="toc-section-number">6.4.3</span> Integral (I)
        Control</a></li>
        <li><a href="#some-additional-feedback-control-applications"
        id="toc-some-additional-feedback-control-applications"><span
        class="toc-section-number">6.4.4</span> Some additional feedback
        control applications:</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#actuation" id="toc-actuation"><span
        class="toc-section-number">7</span> Actuation</a>
        <ul>
        <li><a href="#pulse-width-modulation"
        id="toc-pulse-width-modulation"><span
        class="toc-section-number">7.1</span> Pulse Width Modulation</a>
        <ul>
        <li><a href="#duty-cycle" id="toc-duty-cycle"><span
        class="toc-section-number">7.1.1</span> Duty Cycle</a></li>
        <li><a href="#pwm-period" id="toc-pwm-period"><span
        class="toc-section-number">7.1.2</span> PWM Period</a></li>
        <li><a href="#pwm-sampling-theorem"
        id="toc-pwm-sampling-theorem"><span
        class="toc-section-number">7.1.3</span> PWM Sampling
        Theorem</a></li>
        <li><a href="#example-servo-motor-control"
        id="toc-example-servo-motor-control"><span
        class="toc-section-number">7.1.4</span> Example | Servo Motor
        Control</a></li>
        <li><a href="#pwm-generation-in-microcontrollers"
        id="toc-pwm-generation-in-microcontrollers"><span
        class="toc-section-number">7.1.5</span> PWM Generation in
        Microcontrollers</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#state-estimation" id="toc-state-estimation"><span
        class="toc-section-number">8</span> State Estimation</a>
        <ul>
        <li><a href="#state-estimation-1"
        id="toc-state-estimation-1"><span
        class="toc-section-number">8.1</span> State Estimation</a>
        <ul>
        <li><a href="#how-to-estimate-state"
        id="toc-how-to-estimate-state"><span
        class="toc-section-number">8.1.1</span> How to Estimate
        State?</a></li>
        <li><a href="#exponential-moving-average-ema"
        id="toc-exponential-moving-average-ema"><span
        class="toc-section-number">8.1.2</span> Exponential Moving
        Average (EMA)</a></li>
        <li><a href="#state-space-representation"
        id="toc-state-space-representation"><span
        class="toc-section-number">8.1.3</span> State Space
        Representation</a></li>
        <li><a href="#probabilistic-state-estimation"
        id="toc-probabilistic-state-estimation"><span
        class="toc-section-number">8.1.4</span> Probabilistic State
        Estimation</a></li>
        </ul></li>
        <li><a href="#bayes-filter" id="toc-bayes-filter"><span
        class="toc-section-number">8.2</span> Bayes Filter</a>
        <ul>
        <li><a href="#bayes-filter-prediction"
        id="toc-bayes-filter-prediction"><span
        class="toc-section-number">8.2.1</span> Bayes filter |
        <strong>Prediction</strong></a></li>
        <li><a href="#bayes-filter-measurement-update"
        id="toc-bayes-filter-measurement-update"><span
        class="toc-section-number">8.2.2</span> Bayes filter |
        <strong>Measurement Update</strong></a></li>
        <li><a href="#bayes-filter-combining-prediction-and-measurement"
        id="toc-bayes-filter-combining-prediction-and-measurement"><span
        class="toc-section-number">8.2.3</span> Bayes filter |
        <strong>Combining</strong> Prediction and Measurement</a></li>
        </ul></li>
        <li><a href="#kalman-filter" id="toc-kalman-filter"><span
        class="toc-section-number">8.3</span> Kalman Filter</a>
        <ul>
        <li><a href="#gaussian-distribution"
        id="toc-gaussian-distribution"><span
        class="toc-section-number">8.3.1</span> Gaussian
        Distribution</a></li>
        <li><a href="#state-in-kalman-filters"
        id="toc-state-in-kalman-filters"><span
        class="toc-section-number">8.3.2</span> <strong>State</strong>
        in Kalman Filters</a></li>
        <li><a href="#kalman-filter-prediction"
        id="toc-kalman-filter-prediction"><span
        class="toc-section-number">8.3.3</span> Kalman Filter |
        <strong>Prediction</strong></a></li>
        <li><a href="#kalman-filter-update"
        id="toc-kalman-filter-update"><span
        class="toc-section-number">8.3.4</span> Kalman Filter |
        <strong>Update</strong></a></li>
        <li><a href="#kalman-filter-kalman-gain"
        id="toc-kalman-filter-kalman-gain"><span
        class="toc-section-number">8.3.5</span> Kalman Filter |
        <strong>Kalman Gain</strong></a></li>
        <li><a href="#kalman-filter-summary"
        id="toc-kalman-filter-summary"><span
        class="toc-section-number">8.3.6</span> Kalman Filter |
        <strong>Summary</strong></a></li>
        </ul></li>
        <li><a href="#multivariate-kalman-filter"
        id="toc-multivariate-kalman-filter"><span
        class="toc-section-number">8.4</span> Multivariate Kalman
        Filter</a>
        <ul>
        <li><a href="#process-model-and-noise"
        id="toc-process-model-and-noise"><span
        class="toc-section-number">8.4.1</span> Process Model and
        Noise</a></li>
        <li><a href="#control-input" id="toc-control-input"><span
        class="toc-section-number">8.4.2</span> Control Input</a></li>
        <li><a href="#summary-of-prediction-model"
        id="toc-summary-of-prediction-model"><span
        class="toc-section-number">8.4.3</span> Summary of Prediction
        Model</a></li>
        <li><a href="#measurement-update"
        id="toc-measurement-update"><span
        class="toc-section-number">8.4.4</span> Measurement
        Update</a></li>
        <li><a href="#state-and-uncertainty-update"
        id="toc-state-and-uncertainty-update"><span
        class="toc-section-number">8.4.5</span> State and Uncertainty
        Update</a></li>
        <li><a href="#estimating-hidden-variables"
        id="toc-estimating-hidden-variables"><span
        class="toc-section-number">8.4.6</span> Estimating Hidden
        Variables</a></li>
        <li><a href="#multivariate-kalman-filter-summary"
        id="toc-multivariate-kalman-filter-summary"><span
        class="toc-section-number">8.4.7</span> Multivariate Kalman
        Filter Summary</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#sensor-fusion" id="toc-sensor-fusion"><span
        class="toc-section-number">9</span> Sensor Fusion</a>
        <ul>
        <li><a href="#kalman-filter-for-sensor-fusion"
        id="toc-kalman-filter-for-sensor-fusion"><span
        class="toc-section-number">9.0.1</span> Kalman Filter for Sensor
        Fusion</a></li>
        <li><a href="#taylor-series" id="toc-taylor-series"><span
        class="toc-section-number">9.0.2</span> Taylor Series</a></li>
        <li><a href="#extended-kalman-filter-ekf"
        id="toc-extended-kalman-filter-ekf"><span
        class="toc-section-number">9.1</span> Extended Kalman Filter
        (EKF)</a>
        <ul>
        <li><a href="#ekf-algorithm" id="toc-ekf-algorithm"><span
        class="toc-section-number">9.1.1</span> EKF Algorithm</a></li>
        <li><a href="#ekf-and-sensor-fusion"
        id="toc-ekf-and-sensor-fusion"><span
        class="toc-section-number">9.1.2</span> EKF and Sensor
        Fusion</a></li>
        <li><a href="#example-imu-and-gps-fusion-using-ekf"
        id="toc-example-imu-and-gps-fusion-using-ekf"><span
        class="toc-section-number">9.1.3</span> Example: IMU and GPS
        Fusion using EKF</a></li>
        <li><a href="#miscellaneous-and-advanced-topics"
        id="toc-miscellaneous-and-advanced-topics"><span
        class="toc-section-number">9.1.4</span> Miscellaneous and
        Advanced Topics</a></li>
        <li><a href="#unscented-kalman-filter-comparison"
        id="toc-unscented-kalman-filter-comparison"><span
        class="toc-section-number">9.1.5</span> Unscented Kalman Filter
        Comparison</a></li>
        </ul></li>
        <li><a href="#high-order-sensor-fusions"
        id="toc-high-order-sensor-fusions"><span
        class="toc-section-number">9.2</span> High-Order Sensor
        Fusions</a>
        <ul>
        <li><a href="#sensor-fusion-classes"
        id="toc-sensor-fusion-classes"><span
        class="toc-section-number">9.2.1</span> Sensor Fusion |
        Classes</a></li>
        <li><a href="#what-is-iou" id="toc-what-is-iou"><span
        class="toc-section-number">9.2.2</span> What is “IOU”?</a></li>
        <li><a href="#sensor-fusion-example-camera-and-lidar"
        id="toc-sensor-fusion-example-camera-and-lidar"><span
        class="toc-section-number">9.2.3</span> Sensor Fusion Example |
        Camera and LiDAR</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#slam-1" id="toc-slam-1"><span
        class="toc-section-number">10</span> SLAM</a>
        <ul>
        <li><a href="#slam-problem-definition"
        id="toc-slam-problem-definition"><span
        class="toc-section-number">10.1</span> SLAM Problem
        Definition</a></li>
        <li><a href="#mathematical-formulations"
        id="toc-mathematical-formulations"><span
        class="toc-section-number">10.2</span> Mathematical
        Formulations</a>
        <ul>
        <li><a href="#interpretation" id="toc-interpretation"><span
        class="toc-section-number">10.2.1</span> interpretation</a></li>
        <li><a href="#key-differences-between-full-slam-and-online-slam"
        id="toc-key-differences-between-full-slam-and-online-slam"><span
        class="toc-section-number">10.2.2</span> key differences between
        full slam and online slam</a></li>
        </ul></li>
        <li><a href="#slam-hardware" id="toc-slam-hardware"><span
        class="toc-section-number">10.3</span> SLAM Hardware</a></li>
        <li><a href="#landmark-identification"
        id="toc-landmark-identification"><span
        class="toc-section-number">10.4</span> Landmark
        Identification</a>
        <ul>
        <li><a href="#landmarks-spikes" id="toc-landmarks-spikes"><span
        class="toc-section-number">10.4.1</span> Landmarks |
        Spikes</a></li>
        <li><a href="#landmarks-ransac-random-sample-consensus"
        id="toc-landmarks-ransac-random-sample-consensus"><span
        class="toc-section-number">10.4.2</span> Landmarks | RANSAC
        (Random Sample Consensus)</a></li>
        <li><a href="#landmarks-scan-matching"
        id="toc-landmarks-scan-matching"><span
        class="toc-section-number">10.4.3</span> Landmarks | Scan
        Matching</a></li>
        <li><a href="#comparison-of-spike-ransac-and-scan-matching"
        id="toc-comparison-of-spike-ransac-and-scan-matching"><span
        class="toc-section-number">10.4.4</span> Comparison of Spike,
        RANSAC and Scan Matching</a></li>
        </ul></li>
        <li><a href="#data-association" id="toc-data-association"><span
        class="toc-section-number">10.5</span> Data Association</a></li>
        <li><a href="#ekf-slam" id="toc-ekf-slam"><span
        class="toc-section-number">10.6</span> EKF-SLAM</a></li>
        <li><a href="#slam-implementations"
        id="toc-slam-implementations"><span
        class="toc-section-number">10.7</span> SLAM
        Implementations</a></li>
        </ul></li>
        <li><a href="#path-planning-1" id="toc-path-planning-1"><span
        class="toc-section-number">11</span> Path Planning</a>
        <ul>
        <li><a href="#path-planning-approaches"
        id="toc-path-planning-approaches"><span
        class="toc-section-number">11.1</span> Path Planning |
        Approaches</a></li>
        <li><a href="#path-planning-predictions-and-decision-making"
        id="toc-path-planning-predictions-and-decision-making"><span
        class="toc-section-number">11.2</span> Path Planning |
        Predictions and Decision Making</a>
        <ul>
        <li><a href="#decision-making" id="toc-decision-making"><span
        class="toc-section-number">11.2.1</span> Decision
        Making</a></li>
        </ul></li>
        <li><a href="#path-planning-setup"
        id="toc-path-planning-setup"><span
        class="toc-section-number">11.3</span> Path Planning | Setup</a>
        <ul>
        <li><a href="#graphs-used-for-path-planning"
        id="toc-graphs-used-for-path-planning"><span
        class="toc-section-number">11.3.1</span> Graphs Used for Path
        Planning</a></li>
        </ul></li>
        <li><a href="#path-planning-algorithms"
        id="toc-path-planning-algorithms"><span
        class="toc-section-number">11.4</span> Path Planning |
        Algorithms</a>
        <ul>
        <li><a href="#artificial-potential-field-apf"
        id="toc-artificial-potential-field-apf"><span
        class="toc-section-number">11.4.1</span> Artificial Potential
        Field (APF)</a></li>
        <li><a href="#a-and-d-search" id="toc-a-and-d-search"><span
        class="toc-section-number">11.4.2</span> A* and D*
        Search</a></li>
        <li><a href="#rapidly-exploring-random-tree-rrt-algorithm"
        id="toc-rapidly-exploring-random-tree-rrt-algorithm"><span
        class="toc-section-number">11.4.3</span> Rapidly-exploring
        Random Tree (RRT) Algorithm</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#object-detection-and-avoidance"
        id="toc-object-detection-and-avoidance"><span
        class="toc-section-number">12</span> Object Detection and
        Avoidance</a>
        <ul>
        <li><a href="#computer-vision-methods"
        id="toc-computer-vision-methods"><span
        class="toc-section-number">12.1</span> Computer Vision
        Methods</a></li>
        <li><a href="#deep-learning-methods"
        id="toc-deep-learning-methods"><span
        class="toc-section-number">12.2</span> Deep-Learning Methods</a>
        <ul>
        <li><a href="#convolutional-neural-networks-cnns"
        id="toc-convolutional-neural-networks-cnns"><span
        class="toc-section-number">12.2.1</span> Convolutional Neural
        Networks (CNNs)</a></li>
        <li><a href="#cnns-and-object-detection"
        id="toc-cnns-and-object-detection"><span
        class="toc-section-number">12.2.2</span> CNNs and Object
        Detection</a></li>
        </ul></li>
        <li><a href="#object-detection-parameters"
        id="toc-object-detection-parameters"><span
        class="toc-section-number">12.3</span> Object Detection |
        Parameters</a></li>
        <li><a href="#objectobstacle-avoidance"
        id="toc-objectobstacle-avoidance"><span
        class="toc-section-number">12.4</span> Object/obstacle
        Avoidance</a>
        <ul>
        <li><a href="#classicalgeometric-methods"
        id="toc-classicalgeometric-methods"><span
        class="toc-section-number">12.4.1</span> Classical/Geometric
        Methods</a></li>
        <li><a href="#model-predictive-control-mpc"
        id="toc-model-predictive-control-mpc"><span
        class="toc-section-number">12.4.2</span> Model Predictive
        Control (MPC)</a></li>
        <li><a href="#learning-based-methods"
        id="toc-learning-based-methods"><span
        class="toc-section-number">12.4.3</span> Learning-Based
        Methods</a></li>
        <li><a href="#trajectory-calculations"
        id="toc-trajectory-calculations"><span
        class="toc-section-number">12.4.4</span> Trajectory
        Calculations</a></li>
        </ul></li>
        </ul></li>
        <li><a href="#safety-and-standards"
        id="toc-safety-and-standards"><span
        class="toc-section-number">13</span> Safety and Standards</a>
        <ul>
        <li><a href="#levels-of-automation"
        id="toc-levels-of-automation"><span
        class="toc-section-number">13.1</span> Levels of
        Automation</a></li>
        <li><a href="#sae-j3016-standard-for-functional-safety"
        id="toc-sae-j3016-standard-for-functional-safety"><span
        class="toc-section-number">13.2</span> SAE J3016 Standard for
        Functional Safety</a></li>
        <li><a href="#ansiul-4600-standard"
        id="toc-ansiul-4600-standard"><span
        class="toc-section-number">13.3</span> ANSI/UL 4600
        Standard</a></li>
        <li><a href="#other-standardsefforts"
        id="toc-other-standardsefforts"><span
        class="toc-section-number">13.4</span> Other
        Standards/Efforts</a></li>
        <li><a href="#self-driving-car-liability"
        id="toc-self-driving-car-liability"><span
        class="toc-section-number">13.5</span> Self-driving Car
        Liability</a></li>
        </ul></li>
        <li><a href="#security-for-autonomous-systems"
        id="toc-security-for-autonomous-systems"><span
        class="toc-section-number">14</span> Security for Autonomous
        Systems</a>
        <ul>
        <li><a href="#attacks" id="toc-attacks"><span
        class="toc-section-number">14.1</span> Attacks</a>
        <ul>
        <li><a href="#sensor-based-attacks"
        id="toc-sensor-based-attacks"><span
        class="toc-section-number">14.1.1</span> Sensor-based
        Attacks</a></li>
        <li><a href="#actuation-based-attacks"
        id="toc-actuation-based-attacks"><span
        class="toc-section-number">14.1.2</span> Actuation-based
        Attacks</a></li>
        <li><a href="#computing-softwarehardware-attacks"
        id="toc-computing-softwarehardware-attacks"><span
        class="toc-section-number">14.1.3</span> Computing
        Software/Hardware Attacks</a></li>
        <li><a href="#miscellaneouscommunication-attacks"
        id="toc-miscellaneouscommunication-attacks"><span
        class="toc-section-number">14.1.4</span>
        Miscellaneous/Communication Attacks</a></li>
        </ul></li>
        <li><a href="#defenses" id="toc-defenses"><span
        class="toc-section-number">14.2</span> Defenses</a>
        <ul>
        <li><a href="#detectingreacting-to-possible-attacks"
        id="toc-detectingreacting-to-possible-attacks"><span
        class="toc-section-number">14.2.1</span> Detecting/Reacting to
        (possible) Attacks</a></li>
        <li><a href="#software-based-security-solutions"
        id="toc-software-based-security-solutions"><span
        class="toc-section-number">14.2.2</span> Software-Based Security
        Solutions</a></li>
        <li><a href="#miscellaneous-methods"
        id="toc-miscellaneous-methods"><span
        class="toc-section-number">14.2.3</span> Miscellaneous
        Methods</a></li>
        <li><a href="#holistic-view" id="toc-holistic-view"><span
        class="toc-section-number">14.2.4</span> Holistic View</a></li>
        </ul></li>
        </ul></li>
        </ul>

        </div>
      </div>
            <div class="span9">
            <!--link rel="stylesheet" href="./custom.sibin.css"-->
            <section id="introduction" class="level1" data-number="1">
            <h1 data-number="1"><span
            class="header-section-number">1</span> introduction</h1>
            <section id="autonomy" class="level2" data-number="1.1">
            <h2 data-number="1.1"><span
            class="header-section-number">1.1</span> autonomy</h2>
            <p>what is “<em>autonomy</em>”?</p>
            <p>we see various examples of it…</p>
            <p><img src="img/philippine_uav.png" height="100" width = "200" style="display: inline-block; margin-right: 10px;">
            <img src="img/white_tesla.png" height="100" width = "200"  style="display: inline-block;"></p>
            <section id="what-are-the-aspects-of-autonomy"
            class="level3" data-number="1.1.1">
            <h3 data-number="1.1.1"><span
            class="header-section-number">1.1.1</span> what are the
            <em>aspects</em> of autonomy?</h3>
            <table>
            <colgroup>
            <col style="width: 13%" />
            <col style="width: 86%" />
            </colgroup>
            <tbody>
            <tr>
            <td><strong>perception</strong></td>
            <td>how do you “<em>see</em>” the world around you?</td>
            </tr>
            <tr>
            <td><strong>sensing</strong></td>
            <td>various ways to perceive the world around you
            (<em>e.g</em>, camera, LiDar)</td>
            </tr>
            <tr>
            <td><strong>compute</strong></td>
            <td>what do you “<em>do</em>” with the information about the
            world?</td>
            </tr>
            <tr>
            <td><strong>motion</strong></td>
            <td>do your computations result in any “<em>physical</em>”
            changes?</td>
            </tr>
            <tr>
            <td><strong>actuation</strong></td>
            <td>what “<em>actions</em>”, if any, do you take for said
            physical changes?</td>
            </tr>
            <tr>
            <td><strong>planning</strong></td>
            <td>can you do some “<em>higher order</em>” thinking <br>
            (<em>i.e.,</em> not just your immediate next move)</td>
            </tr>
            </tbody>
            </table>
            </section>
            </section>
            <section id="let-us-define-autonomy" class="level2"
            data-number="1.2">
            <h2 data-number="1.2"><span
            class="header-section-number">1.2</span> let us define
            <strong>autonomy</strong></h2>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <tbody>
            <tr>
            <td>Autonomy is the ability to <br> <strong>perform given
            tasks</strong> <br> based on the systems perception <br>
            <scb>without</scb> human intervention</td>
            <td><img src="img/robot_profile_view.jpg" height="275"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            <section id="autonomous-systems" class="level2"
            data-number="1.3">
            <h2 data-number="1.3"><span
            class="header-section-number">1.3</span> autonomous
            systems</h2>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 25%" />
            <col style="width: 25%" />
            <col style="width: 25%" />
            </colgroup>
            <tbody>
            <tr>
            <td><strong>cyber</strong></td>
            <td><img src="img/cps_software.png" width="275"></td>
            <td><img src="img/cps_networking.png" height="275"></td>
            <td><img src="img/cps_ecus.png" height="275"></td>
            </tr>
            <tr>
            <td><strong>physical</strong></td>
            <td><img src="img/cps_sensors.png" height="275"></td>
            <td><img src="img/cps_actuators.png" height="275"></td>
            <td><img src="img/cps_plants.png" height="275"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>hence, they fall under the class of systems →
            <strong>cyber-physical</strong> systems</p>
            </section>
            <section id="sensors-and-actuators" class="level2"
            data-number="1.4">
            <h2 data-number="1.4"><span
            class="header-section-number">1.4</span> sensors and
            actuators…</h2>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="img/cps_sensors.png" width="150" style="border: 2px solid purple; display: inline-block; padding: 10px; background-color:rgb(236, 219, 250);"></td>
            <td><img src="img/cps_actuators.png" width="125" style="border: 2px solid purple; display: inline-block; padding: 10px; background-color:rgb(236, 219, 250);"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>…are <strong>everywhere</strong>!</p>
            <p>the <strong>embedded</strong> components → interactions
            with the real world</p>
            </section>
            <section id="sensing-and-actuation-in-the-real-world"
            class="level2" data-number="1.5">
            <h2 data-number="1.5"><span
            class="header-section-number">1.5</span> sensing and
            actuation in the real world</h2>
            <p>consider the following example of two cars… <img
            src="img/cars_sensing/cars_sensing_1.png"
            alt="Two cars, one behind the over, top view" /></p>
            <p>the second car is approaching the first <img
            src="img/cars_sensing/cars_sensing_2.png"
            alt="Two cars, one behind the over, top view, an arror to the left on top of the car on the right" /></p>
            <p><strong>sensors</strong> → constantly gathering
            data/sensing</p>
            <div class="multicolumn">
            <div>
            <ol>
            <li>
            periodic sensing
            </li>
            </ol>
            </div>
            <div>
            <p><img src="img/cars_sensing/cars_sensing_3.png" width="400"></p>
            </div>
            </div>
            <p>on detection (of other car) → quickly
            <strong>compute</strong> what to do</p>
            <div class="multicolumn">
            <div>
            <ol>
            <li>
            periodic sensing
            </li>
            <li>
            computation
            </li>
            </ol>
            </div>
            <div>
            <p><img src="img/cars_sensing/cars_sensing_4.png" width="400"></p>
            </div>
            </div>
            <p>take <strong>physical action</strong> (actuation) → say
            by braking <em>in time</em></p>
            <div class="multicolumn">
            <div>
            <ol>
            <li>
            periodic sensing
            </li>
            <li>
            computation
            </li>
            <li>
            actuation
            </li>
            </ol>
            </div>
            <div>
            <p><img src="img/cars_sensing/cars_sensing_5.png" width="400"></p>
            </div>
            </div>
            <div class="multicolumn">
            <div>
            <ol>
            <li>
            periodic sensing
            </li>
            <li>
            computation
            </li>
            <li>
            actuation
            </li>
            </ol>
            </div>
            <div>
            <p><img src="img/sense_planning_actuation.png" width="400"></p>
            </div>
            </div>
            <p>“<strong>control</strong>”</p>
            <p>Remember this → on detection (of other car) →
            <scb>quickly</scb> <strong>compute</strong> what to do</p>
            <p><img src="img/cars_sensing/cars_sensing_4.png" width="400"></p>
            <p>“quickly” compute → complete computation/actuation →
            before a <strong>deadline</strong></p>
            <p>This is a <strong>real-time system</strong>.</p>
            <section id="come-back-to-sensing" class="level3"
            data-number="1.5.1">
            <h3 data-number="1.5.1"><span
            class="header-section-number">1.5.1</span> Come back to
            <strong>sensing</strong></h3>
            <!--div class="multicolumn">
            <div>
            <br>
            <ul>
                <li>we see <i>one</i> sensor (maybe LiDAR)</li>
                <li>reality &rarr; <b>multiple</b> sensors</li>
                <li>cameras, radars, lidar, etc.</li>
            <ul>
            </div>
            <div>
            <img src="img/autonomous_cars_sensors.png">
            </div>
            </div-->
            <p>Multiple sensors in an autonomous vehicle → need to
            <em>combine</em> them somehow</p>
            <p><strong>sensor fusion</strong></p>
            <p>Once we have information from the sensors (fused or
            otherwise)…</p>
            <p><img src="img/kalman_statistical_view.png" width="400"></p>
            <p>We need <strong>state estimation</strong>
            (<strong>kalman</strong> filter, <strong>ekf</strong>).</p>
            </section>
            </section>
            <section id="overviewarchitecture-of-autonomous-systems"
            class="level2" data-number="1.6">
            <h2 data-number="1.6"><span
            class="header-section-number">1.6</span>
            Overview/Architecture of Autonomous Systems</h2>
            <p>So far, we have (briefly) talked about…</p>
            <p>Sensing:</p>
            <p><img src="img/stack_architecture/stack_overview.2.png" width="200"></p>
            <p>Actuation:</p>
            <p><img src="img/stack_architecture/stack_overview.3.png" width="200"></p>
            <p>But the system includes…an <strong>operating
            system</strong> (OS) in there</p>
            <p><img src="img/stack_architecture/stack_overview.4.png" width="300"></p>
            <p>and it includes <strong>real-time</strong>
            mechanisms.</p>
            <p>We have briefly discussed, <strong>EKF</strong>:</p>
            <p><img src="img/stack_architecture/stack_overview.5.png" width="300"></p>
            <p><strong>note</strong>: ekf is versatile; can be used for
            sensor fusion, slam, etc.</p>
            <p>All of it integrates with…<strong>control</strong>:</p>
            <p><img src="img/stack_architecture/stack_overview.6.png" width="300"></p>
            <p>There are some <strong>real-time</strong> functions in
            there…</p>
            <p><img src="img/stack_architecture/stack_overview.7.png" width="300"></p>
            <p>like <em>braking</em>, <em>engine control</em>.</p>
            <p>Question: if we design such a system…</p>
            <p><img src="img/stack_architecture/stack_overview.7.png" width="300"></p>
            <p>is it “<strong>autonomous</strong>”?</p>
            <p>We are missing some “higher order” functionss from the
            perspective of the autonomous system:</p>
            <ul>
            <li><em>where</em> am I?</li>
            <li><em>where</em> do I need to go?</li>
            <li><em>how</em> do I get there?</li>
            <li><em>what</em> obstacles may I face?</li>
            <li><em>how</em> do I avoid them?</li>
            </ul>
            <p>let us not forget the most important question of all…</p>
            <p><img src="img/drax_gamora.avif" width="400"></p>
            <p><strong>why</strong> is gamora?</p>
            <section id="high-order-functions" class="level3"
            data-number="1.6.1">
            <h3 data-number="1.6.1"><span
            class="header-section-number">1.6.1</span> high-order
            functions</h3>
            <p>In order to answer the following, we need
            <strong>additional functionality</strong>. Let us go through
            what that might be.</p>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <tbody>
            <tr>
            <td></td>
            <td><img src="img/stack_architecture/stack_overview.7.png" width="300"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            <section id="slam" class="level3" data-number="1.6.2">
            <h3 data-number="1.6.2"><span
            class="header-section-number">1.6.2</span> slam</h3>
            <p>Simultaneous localization and mapping → figure out
            <strong>where</strong> we are.</p>
            <p><img src="img/stack_architecture/stack_overview.8.png" width="300"></p>
            </section>
            <section id="waypoint-detection" class="level3"
            data-number="1.6.3">
            <h3 data-number="1.6.3"><span
            class="header-section-number">1.6.3</span> waypoint
            detection</h3>
            <p>Understand how to move in the <em>right</em> direction at
            the <strong>micro</strong> level, <em>i.e.,</em> find
            <strong>waypoints</strong>.</p>
            <p><img src="img/stack_architecture/stack_overview.9.png" width="300"></p>
            </section>
            <section id="yolo" class="level3" data-number="1.6.4">
            <h3 data-number="1.6.4"><span
            class="header-section-number">1.6.4</span> yolo</h3>
            <p>Is it “you only live once”? Actually this stands for:
            “you only <strong>look</strong> once”. It is an object
            <strong>detection</strong> model that uses convolutional
            neural networks (cnns)</p>
            <p><img src="img/stack_architecture/stack_overview.10.png" width="300"></p>
            </section>
            <section id="object-avoidance" class="level3"
            data-number="1.6.5">
            <h3 data-number="1.6.5"><span
            class="header-section-number">1.6.5</span> object
            avoidance</h3>
            <p>The objective is to avoid objects in the
            <strong>immediate path</strong>.</p>
            <p><img src="img/stack_architecture/stack_overview.11.png" width="300"></p>
            </section>
            <section id="path-planning" class="level3"
            data-number="1.6.6">
            <h3 data-number="1.6.6"><span
            class="header-section-number">1.6.6</span> path
            planning</h3>
            <p>i.e., how to get to <strong>destination</strong> at the
            <strong>macro</strong> level → uses waypoints.</p>
            <p><img src="img/stack_architecture/stack_overview.12.png" width="300"></p>
            </section>
            <section id="compute-platform" class="level3"
            data-number="1.6.7">
            <h3 data-number="1.6.7"><span
            class="header-section-number">1.6.7</span> compute
            platform</h3>
            <p>To run all of these functions, we need low power,
            embedded platforms.</p>
            <p><img src="img/stack_architecture/stack_overview.13.png" width="300"></p>
            </section>
            <section id="still-some-non-functional-requirements-remain"
            class="level3" data-number="1.6.8">
            <h3 data-number="1.6.8"><span
            class="header-section-number">1.6.8</span> still some
            <strong>non-functional</strong> requirements remain</h3>
            <p>any guesses what they could be?</p>
            </section>
            <section id="safety" class="level3" data-number="1.6.9">
            <h3 data-number="1.6.9"><span
            class="header-section-number">1.6.9</span> safety!</h3>
            <p>Essentially safety of → operator, other people, the
            vehicle, environment This is <strong>cross-cutting</strong>
            issue → affected <scb>by</scb> <strong>all</strong> parts of
            system.</p>
            <p><img src="img/stack_architecture/stack_overview.14.png" width="300"></p>
            </section>
            <section id="security" class="level3" data-number="1.6.10">
            <h3 data-number="1.6.10"><span
            class="header-section-number">1.6.10</span> security</h3>
            <p>Security is another cross-cutting issue → <scb>can
            affect</scb> <strong>all</strong> components.</p>
            <p><img src="img/stack_architecture/stack_overview.png" width="300"></p>
            </section>
            <section id="course-structure" class="level3"
            data-number="1.6.11">
            <h3 data-number="1.6.11"><span
            class="header-section-number">1.6.11</span> Course
            Structure</h3>
            <p>Hence this figure is a (loose) map of this course:</p>
            <p><img src="img/stack_architecture/stack_overview.png" width="300">
            <!--link rel="stylesheet" href="./custom.sibin.css"--></p>
            </section>
            </section>
            </section>
            <section id="embedded-architectures" class="level1"
            data-number="2">
            <h1 data-number="2"><span
            class="header-section-number">2</span> Embedded
            Architectures</h1>
            <p>Just like “autonomy” describing and “embedded system” is
            hard. What (typically) distinguishes it from other types of
            computer systems (e.g., laptops, servers or GPUs even) is
            that such systems are typically created for
            <em>specific</em> functionality and often remain fixed and
            operational for years, decades even.</p>
            <p>Embedded systems often trade off between performance and
            other considerations such as power (or battery life), less
            memory, fewer peripherals, limited applications, smaller
            operating system (OS) and so on. There are numerous reasons
            for this – chief among them is <em>predictability</em> –
            designers need to guarantee that the system works correctly,
            and remains safe, all the time. Hence, it must be easy to
            <em>certify</em> <a href="#fn1" class="footnote-ref"
            id="fnref1" role="doc-noteref"><sup>1</sup></a> the
            <em>entire</em> system. This process ensures that the system
            operates <strong>safely</strong>.</p>
            <section id="the-wcet-problem" class="level2"
            data-number="2.1">
            <h2 data-number="2.1"><span
            class="header-section-number">2.1</span> The
            <strong>wcet</strong> problem</h2>
            <p>One piece of information that is required to ensure
            predictability and guarentee safety is <strong><a
            href="https://www.cs.fsu.edu/~whalley/papers/tecs07.pdf">worst-case
            execution time</a></strong> (WCET). The WCET/BCET is the
            <strong>longest</strong>/shortest execution time possible
            for a program, <strong>on a specific hardware
            platform</strong> – and it has to consider <em>all possible
            inputs</em>. WCET is necessary to ensure the
            “schedulability”, resource requirements and performance
            limits of embedded and real-time programs. There are lots of
            approaches to computing the WCET, <em>e.g.,</em></p>
            <ul>
            <li><a
            href="https://www.cs.fsu.edu/~whalley/papers/tecs07.pdf">dynamic/empirical</a>
            analysis → run the program lots of times (thousands,
            millions?) on the platform and measure it</li>
            <li><a
            href="https://www.cs.fsu.edu/~whalley/papers/tecs07.pdf">static</a>
            analysis → analyze the program at <em>compile time</em> to
            compute the <em>worst-case paths</em> through the
            program</li>
            <li><a
            href="https://sibin.github.io/papers/2008_NCSU-Dissertation_CheckerMode_SibinMohan.pdf">hybrid</a>
            → a combination of the two</li>
            <li><a
            href="https://people.ac.upc.edu/fcazorla/articles/jabella_ecrts2014_2.pdf">probabilistic</a>
            → a combination of dynamic analysis+statistical methods</li>
            <li><a
            href="https://dl.acm.org/doi/10.1145/3570361.3615740">ML-based
            methods</a> → applying machine-learning to the problem</li>
            </ul>
            <p>At a high-level, the execution time distributions of
            applications look like:</p>
            <p><img src="./img/embedded_arch/wcet_wilhelm.png" width="400" style="display: inline-block;" title="https://www.inf.ed.ac.uk/teaching/courses/es/PDFs/lecture_11.pdf" /></p>
            <p>WCET analysis is a very active area of research and
            hundreds of papers have been written about it, since it
            directly affects the safety of many critical systems
            (aircraft, power systems, nuclear reactors, space vehicles
            and…autonomous systems).</p>
            <p>There are structural challenges (both in software and
            hardware) that prevent the computation of <em>proper</em>
            wcet for anything but trivial examples. For instance,
            consider,</p>
            <div class="sourceCode" id="cb1"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> main<span class="op">()</span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> max <span class="op">=</span> <span class="dv">10</span> <span class="op">;</span></span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> sum <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span><span class="op">(</span> <span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span> <span class="op">;</span> i <span class="op">&lt;</span> max <span class="op">;</span> <span class="op">++</span>i<span class="op">)</span></span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>        sum <span class="op">+=</span> i <span class="op">;</span></span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <p>How do you compute the WCET for this code? Say running on
            some known processor, P?</p>
            <p>Well, there’s some information we need,</p>
            <ul>
            <li>how long each instruction takes to execute on P</li>
            <li>how many loop iterations?</li>
            <li>what is the startup/cleanup times for the program on
            P?</li>
            </ul>
            <p>Let’s assume (from the manual for P), we get the
            following information,</p>
            <div class="sourceCode" id="cb2"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="dv">1</span>   <span class="dt">void</span> main<span class="op">()</span>         <span class="co">// startup cost = 100 cycles</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="dv">2</span>   <span class="op">{</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="dv">3</span>       <span class="dt">int</span> max <span class="op">=</span> <span class="dv">15</span> <span class="op">;</span>  <span class="co">// 10 cycles</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="dv">4</span>       <span class="dt">int</span> sum <span class="op">=</span> <span class="dv">0</span><span class="op">;</span>    <span class="co">// 10 cycles </span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="dv">5</span>       <span class="cf">for</span><span class="op">(</span> <span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span> <span class="op">;</span> i <span class="op">&lt;</span> max <span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="co">// 5 cycles, once</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="dv">6</span>            sum <span class="op">+=</span> i <span class="op">;</span> <span class="co">// 20 cycles each iteration</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a><span class="dv">7</span>   <span class="op">}</span>                   <span class="co">// cleanup cost = 120 cycles</span></span></code></pre></div>
            <p>So, based on this, we can calculate the total time to
            execute this program:</p>
            <p><span
            class="math display"><em>w</em><em>c</em><em>e</em><em>t</em> = <em>s</em><em>t</em><em>a</em><em>r</em><em>t</em><em>u</em><em>p</em>_<em>c</em><em>o</em><em>s</em><em>t</em> + <em>l</em><em>i</em><em>n</em><em>e</em>_3 + <em>l</em><em>i</em><em>n</em><em>e</em>_4 + <em>l</em><em>o</em><em>o</em><em>p</em>_<em>s</em><em>t</em><em>a</em><em>r</em><em>t</em><em>u</em><em>p</em>_<em>c</em><em>o</em><em>s</em><em>t</em> + (<em>l</em><em>i</em><em>n</em><em>e</em>_6 * <em>m</em><em>a</em><em>x</em>)  [1]</span></p>
            <p><span
            class="math display"><em>w</em><em>c</em><em>e</em><em>t</em> = 100 + 10 + 10 + 5 + (20 * 15)</span></p>
            <p><span
            class="math display"><em>w</em><em>c</em><em>e</em><em>t</em> = 425 <em>c</em><em>y</em><em>c</em><em>l</em><em>e</em><em>s</em></span></p>
            <p>Now consider this slight change to the above code:</p>
            <div class="sourceCode" id="cb3"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> main<span class="op">(</span> <span class="dt">int</span> argc<span class="op">,</span> <span class="dt">char</span><span class="op">*</span> argv<span class="op">[]</span> <span class="op">)</span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> max <span class="op">=</span> atoi<span class="op">(</span> argv<span class="op">[</span><span class="dv">1</span><span class="op">]</span> <span class="op">)</span> <span class="op">;</span>     <span class="co">// convert the command line arg to max</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> sum <span class="op">=</span> <span class="dv">0</span><span class="op">;</span> </span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span><span class="op">(</span> <span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span> <span class="op">;</span> i <span class="op">&lt;</span> max <span class="op">;</span> <span class="op">++</span>i<span class="op">)</span> <span class="co">// how many iterations?</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        sum <span class="op">+=</span> i <span class="op">;</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <p>The problem is that equation [1] above fails since we no
            longer know the value of <code>max</code>. Hence the
            <em>program can run for any arbitrary amount of time,
            depending on the given input!</em> Note that
            <strong>none</strong> of the aforemention wcet methods will
            help in this case since the input can be completely
            arbitrary. Hence, the structure of the software code can
            affect wcet calculations.</p>
            <p>Another problem is that of <strong>hardware</strong> (and
            interactions between hardware and software). Now consider if
            we modify the original code as,</p>
            <div class="sourceCode" id="cb4"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#define VERY_LARGE_ARRAY</span><span class="op">+</span><span class="pp">SIZE </span><span class="dv">1</span><span class="op">&gt;&gt;</span><span class="dv">18</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> main<span class="op">()</span></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="op">{</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> first_array<span class="op">[</span>VERY_LARGE_ARRAY_SIZE<span class="op">]</span> <span class="op">;</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> second_array<span class="op">[</span>VERY_LARGE_ARRAY_SIZE<span class="op">]</span> <span class="op">;</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> sum_first <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>    <span class="dt">int</span> sum_second <span class="op">=</span> <span class="dv">0</span><span class="op">;</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span><span class="op">(</span> <span class="dt">int</span> i <span class="op">=</span> <span class="dv">0</span> <span class="op">;</span> i <span class="op">&lt;</span> VERY_LARGE_ARRAY_SIZE <span class="op">*</span> <span class="dv">2</span> <span class="op">;</span> <span class="op">++</span>i<span class="op">)</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    <span class="op">{</span></span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span><span class="op">(</span> i<span class="op">%</span><span class="dv">2</span> <span class="op">)</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            first_sum <span class="op">+=</span> first_array<span class="op">[</span>i<span class="op">/</span><span class="dv">2</span><span class="op">]</span> <span class="op">;</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>            second_sum <span class="op">+=</span> second_array<span class="op">[(</span><span class="dt">int</span><span class="op">)((</span>i<span class="op">/</span><span class="dv">2</span><span class="op">)+</span><span class="dv">1</span><span class="op">)]</span> <span class="op">;</span></span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <p>Now, while we can compute, using equation [1] the wcet
            from the code perspective (since we know the loop runs for
            <code>VERY_LARGE_ARRAY_SIZE * 2</code> iterations), there
            will be significant non-obvious hardware issues, in the
            <strong>cache</strong>. Each iteration is accessing a
            <em>different</em> large array. Hence, it will load the
            cache with lines from that array and in the <em>very next
            iteration</em> the other array will be loaded, also missing
            in the cache. For instance,</p>
            <table>
            <colgroup>
            <col style="width: 14%" />
            <col style="width: 25%" />
            <col style="width: 17%" />
            <col style="width: 41%" />
            </colgroup>
            <thead>
            <tr>
            <th>iteration</th>
            <th>operation</th>
            <th>cache state</th>
            <th>reason</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1</td>
            <td><code>first_array</code> loaded</td>
            <td>miss</td>
            <td>evicts whatever was previously in cache</td>
            </tr>
            <tr>
            <td>2</td>
            <td><code>second_array</code> loaded</td>
            <td>miss</td>
            <td><strong>evicts <code>first_array</code></strong> due to
            lack of space</td>
            </tr>
            <tr>
            <td>3</td>
            <td><code>first_array</code> loaded again</td>
            <td>miss</td>
            <td><strong>evicts <code>second_array</code></strong> due to
            lack of space</td>
            </tr>
            <tr>
            <td>…</td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Hence, this program will <em>constantly</em> sufffer
            cache misses and since caches misses (and reloads) are
            expensive (in terms of time), the loop’s execution time will
            balloon out of control! Hence, even though we fixed the code
            issue (upper bound on number of iterations, hardware
            artifacts can change the wcet calculations). So now, we need
            to <em>model cache behavior</em> for each program and data
            variable! This is <a
            href="https://user.it.uu.se/~wangyi/pdf-files/2015/lgyrw-acm15.pdf">notoriously
            complicated</a> even for the simplest of programs.</p>
            <p>Other hardware designs further complicate matters,
            e.g.,</p>
            <ul>
            <li>processor pipelining</li>
            <li>prefetching</li>
            <li>branch prediction</li>
            <li>multithreading</li>
            <li>multicore systems</li>
            <li>memory buses</li>
            <li>networks-on-chip</li>
            <li>and too many others to recount here…</li>
            </ul>
            <p>Any contemporary processor design that improves
            performance, <em>turns out to be bad for wcet analysis</em>.
            So, the fewer (or simpler versions of) these features, the
            better it is for the (eventual) safety and certification of
            the system.</p>
            <p>This is one of the main reasons why embedded (and
            especially real-time) systems <strong>prefer simpler
            processors</strong> (simple pipelines, fewer complex
            features, simpler memory/cache architectures, if any) since
            they’re easier to analyze. In fact, many critical systems
            (e.g., aircraft, cars, etc.) <strong>use older
            processors</strong> (often designed in the 1980s and 1990s)
            – even the ones beind design today!</p>
            </section>
            <section id="embedded-processors" class="level2"
            data-number="2.2">
            <h2 data-number="2.2"><span
            class="header-section-number">2.2</span> Embedded
            Processors</h2>
            <p>Just as embedded systems are varied, embedded processors
            come in a myriad of shapes and sizes as well. From the very
            small and simple (e.g., DSPs) to the very large and complex
            (modern multicore chips, some with GPUs!). Here is a
            (non-exhaustive) list of the types of embedded
            processors/architectures in use today:</p>
            <ol type="1">
            <li><a href="#microcontrollers">Microcontrollers</a></li>
            <li><a href="#digital-signal-processors-dsps">Digital Signal
            Processors</a> (DSPs)</li>
            <li><a href="#microprocessors">Microprocessors</a> of
            various designs and architectures (e.g., ARM, x86)</li>
            <li><a href="#system-on-a-chip-soc">System-on-a-Chip</a>
            (SoC)</li>
            <li><a
            href="#embedded-accelarators-eg-gpu-enabled-systems">Embedded
            accelerators</a></li>
            <li><a href="#asics-and-fpgas">ASICs and FPGAs</a></li>
            </ol>
            <section id="microcontrollers" class="level3"
            data-number="2.2.1">
            <h3 data-number="2.2.1"><span
            class="header-section-number">2.2.1</span>
            Microcontrollers</h3>
            <p>According to <a
            href="https://en.wikipedia.org/wiki/Microcontroller">Wikipedia</a>,</p>
            <blockquote>
            <p>“A microcontroller (MC, UC, or μC) or microcontroller
            unit (MCU) is a small computer on a single integrated
            circuit.”</p>
            </blockquote>
            <p>These may be among the most common type of “processors”
            used in embedded systems. According to many studies,
            <strong><a
            href="https://www.embedded.com/the-two-percent-solution/">more
            than 55%</a></strong> of the world’s processors are
            microntrollers! Microcontrollers are typically used in
            small, yet critical, systems such as car engine control,
            implantable medical devices, thermal monitoring, <a
            href="https://sibin.github.io/papers/2021_BuildSys_PIRMedic_AshishKashinath.pdf">fault
            detection and classification</a> among millions of other
            applications.</p>
            <p>Microcontrollers hardware features typically include,</p>
            <table>
            <colgroup>
            <col style="width: 55%" />
            <col style="width: 45%" />
            </colgroup>
            <thead>
            <tr>
            <th>component</th>
            <th>details</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>one (sometimes more) CPU cores</td>
            <td>typically simple <code>4</code> or <code>8</code> bit
            chips</td>
            </tr>
            <tr>
            <td>small pipelined architectues</td>
            <td>sometimes <code>2</code> or <code>4</code> stage
            pipelines</td>
            </tr>
            <tr>
            <td>some limited memory</td>
            <td>typically a few hundred kilobytes, perhaps in the form
            of EEPROMs or FLASH</td>
            </tr>
            <tr>
            <td>some programmable I/O</td>
            <td>to interact with the real world</td>
            </tr>
            <tr>
            <td>low operating frequencies</td>
            <td>e.g., <code>4 KHz</code>; simpler/older processors, yet
            more predictable</td>
            </tr>
            <tr>
            <td>low power consumption</td>
            <td>in the <strong>milliwatts</strong> or
            <strong>microwatts</strong> ranges; might even be
            <strong>nanowatts</strong> when the system is
            <em>sleeping</em></td>
            </tr>
            <tr>
            <td>interrupts (some programmable)</td>
            <td>often <em>real-time</em> (ficed/low latency)</td>
            </tr>
            <tr>
            <td>several general-purpose I/O (GPIO) pins</td>
            <td>for I/O</td>
            </tr>
            <tr>
            <td>timers</td>
            <td>e.g., a programmable interval timer (PIT)</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>There are some <strong>additional features</strong> found
            on some microcontrollers, viz.,</p>
            <table>
            <colgroup>
            <col style="width: 55%" />
            <col style="width: 45%" />
            </colgroup>
            <thead>
            <tr>
            <th>component</th>
            <th>details</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>analog to digital (ADC) signal convertors</td>
            <td>to convert incoming (real-world, sensor) data to a
            digital form that the uC can operate on</td>
            </tr>
            <tr>
            <td>digital-to-analog (DAC) convertor</td>
            <td>to do the opposite, convert from digital to analog
            signals to send outputs in that form</td>
            </tr>
            <tr>
            <td>universal asynchronous transmitter/receiver (UART)</td>
            <td>to receive/send data over a <em>serial</em> line</td>
            </tr>
            <tr>
            <td>pulse width modulation (PWM)</td>
            <td>so that the CPU can control <strong>motors</strong>
            (significant for us in autonomous/automotive systems), power
            systems, resistive loads, etc.</td>
            </tr>
            <tr>
            <td>JTAG interace</td>
            <td>debugging interface</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Some examples of popular microcontroller families:</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 25%" />
            <col style="width: 25%" />
            <col style="width: 25%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="./img/embedded_arch/ATmega169-MLF.jpg" height="100"><br>Atmel
            ATmega</td>
            <td><img src="./img/embedded_arch/Microchip_PIC24HJ32GP202.jpg" height="100">
            <br> Microchip Technology</td>
            <td><img src="./img/embedded_arch/Motorola_68HC11.jpg" height="100">
            <br> Motorola (Freescale)</td>
            <td><img src="./img/embedded_arch/NXP_LPC2387FBD100-5543.jpg" height="100">
            <br> NXP</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Microcontroller programs and data,</p>
            <ul>
            <li>are small –&gt; must fit in memory (since very little
            expandable memory exists)</li>
            <li>often directly programmed in <strong>assembly</strong>!
            <ul>
            <li>sometimes the assembly code might need <em>hand
            tuning</em> –&gt; for both, performance as well as fitting
            into the limited memory</li>
            </ul></li>
            <li><strong>C</strong> is another popular language</li>
            <li><strong>no operating systems</strong> (or very
            rare)!</li>
            <li>sometimes have their own special-purpose programming
            languages or instructions</li>
            </ul>
            </section>
            <section id="digital-signal-processors-dsps" class="level3"
            data-number="2.2.2">
            <h3 data-number="2.2.2"><span
            class="header-section-number">2.2.2</span> Digital Signal
            Processors (DSPs)</h3>
            <p>DSPs are specialized microcontrollers optimized for
            <em>digital signal processing</em>. They find wide use in
            audio processing, radar and sonar, speech recognition
            systems, image processing, satellites, telecommunications,
            mobile phones, televisions, etc. Their main goals are to
            isoloate, measure, compress and filter <em>analog</em>
            signals in the real world. They often have <strong>stringent
            real-time constraints</strong>.</p>
            <p>The Texas Instruments DSP chip, <a
            href="https://www.ti.com/lit/ug/spruh79c/spruh79c.pdf?ts=1736945981001">TMS320
            Series</a> is one of the most famous example of this type of
            system:</p>
            <p><img src="./img/embedded_arch/TI_DSP.jpg" height="100" title="Texas Instruments DSP Chip"></p>
            <p>Typical digital signal processing (of any kind) requires
            repetitive mathematical operations over a large number of
            samples, in real-time, viz., - analog to digital conversion
            - maniupulation (the core algorithm) - digital to analog
            conversion</p>
            <p>Often, the <em>entire</em> process must be completed with
            low latency, even within a fixed deadline. They also have
            <strong>low power</strong> requirements since DSPs are often
            used in battery-constrained devices such as mobile phones.
            Hence, the proliferation of specialized DSP chips (instead
            of pure <a href="https://liquidsdr.org">software
            implementations</a>, which also exist; MATLAB has an entire
            <a href="https://www.mathworks.com/help/dsp/index.html">DSP
            System Toolbox</a>).</p>
            <p><strong>Typical DSP architecture</strong>/flow (credit:
            <a
            href="https://en.wikipedia.org/wiki/Digital_signal_processor">Wikipedia</a>):</p>
            <p><img src="./img/embedded_arch/dsp_architecture.png" height="100" title="https://en.wikipedia.org/wiki/Digital_signal_processor"></p>
            <p>These types of chips typically have custom instructions
            for optimizing certain (mathematical) operations (apart from
            the typical <code>add</code>, <code>subtract</code>,
            <code>multiply</code> and <code>divide</code>), e.g., -
            <code>saturate</code>; caps the minimum or maximum value
            that can be held in a fixed-point representation -
            <code>ed</code> ; euclidian distance -
            <code>accumulate</code> instructions ; for <a
            href="https://skills.microchip.com/dsp-features-of-the-microchip-dspic-dsc/693207"><em>multiply-and-accumulate</em></a>
            operations, i.e., <span
            class="math inline"><em>a</em> ← <em>a</em> + (<em>b</em> * <em>c</em>)</span></p>
            <blockquote>
            <p>See the <a
            href="https://ww1.microchip.com/downloads/en/DeviceDoc/sect2.pdf">Microchip
            instruction set</a> details for more information for a
            typical DSP ISA.</p>
            </blockquote>
            <p>DSPs require <em>optimization of streaming data</em> and
            hence, - require <strong>optimized memories and
            caches</strong> → fetch multiple data elements at the same
            time - code may need to be aware of, and
            <strong>explicitly</strong> manipulate caches - may have
            rudimentary OS but <strong>no virtual memory</strong></p>
            </section>
            <section id="microprocessors" class="level3"
            data-number="2.2.3">
            <h3 data-number="2.2.3"><span
            class="header-section-number">2.2.3</span>
            Microprocessors</h3>
            <p>Microprocessors are, then,
            <strong>general-purpose</strong> chips (as opposed to
            microcontrollers and DSPs) that are also used extensively in
            embedded systems. They are used in systems that need more
            heavy duty computing/memory and/or more flexibility in terms
            of programming and management of the system. They use a
            number of commodity processor architectures (e.g,, ARM,
            Intel x86).</p>
            <p>Main features of microprocessors:</p>
            <table>
            <colgroup>
            <col style="width: 55%" />
            <col style="width: 45%" />
            </colgroup>
            <thead>
            <tr>
            <th>component</th>
            <th>details</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>cores</td>
            <td>single or multicore; powerful</td>
            </tr>
            <tr>
            <td>pipelines</td>
            <td>more complex pipelines; better performance, harder to
            analyze (e.g., wcet)</td>
            </tr>
            <tr>
            <td>clock speeds</td>
            <td>higher clock speeds; <code>100s</code> of khz, or even
            GHz</td>
            </tr>
            <tr>
            <td>ISA</td>
            <td>common ISA; well understood, not custom</td>
            </tr>
            <tr>
            <td>memory</td>
            <td>significant memory; megabytes, even gigabytes</td>
            </tr>
            <tr>
            <td>cache hierarchies</td>
            <td>multiple levels, optimized</td>
            </tr>
            <tr>
            <td>power consumption</td>
            <td>much higher, but can be reduced (e.g., via <a
            href="https://developer.arm.com/documentation/ddi0375/a/functional-overview/intelligent-energy-management--iem-/dynamic-voltage-scaling--dvs-">voltage
            and frequency scaling</a>)</td>
            </tr>
            <tr>
            <td>size, cost</td>
            <td>often higher</td>
            </tr>
            <tr>
            <td>interrupts, timers</td>
            <td>more varied, easily programmable</td>
            </tr>
            <tr>
            <td>I/O</td>
            <td>more interfaces, including commodity ones like USB</td>
            </tr>
            <tr>
            <td>security</td>
            <td>often includes additional hardware security features,
            e.g., <a
            href="https://sefcom.asu.edu/publications/trustzone-explained-cic2016.pdf">ARM
            TrustZone</a>.</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>The <a
            href="https://armkeil.blob.core.windows.net/developer/Files/pdf/product-brief/arm-cortex-m85-product-brief.pdf">ARM
            M-85</a> Embedded Microprocessor architecture:</p>
            <p><img src="./img/embedded_arch/arm_cortex_m85.png" width="300" title="Arm M-85"></p>
            <p>When compared to microcontrollers (or even SoCs), most
            microprpcessors <strong>do not</strong> include components
            such as DSPs, ADCs, DACs, etc. It is possible to
            <em>augment</em> the microprocessor to include this
            functionality → usually by <em>connecting one or more
            microcontrollers to it</em>!</p>
            <p>On the software side, microprocessors typically have the
            <strong>most flexibility</strong>:</p>
            <ul>
            <li>general purpose operating systems (e.g., Linux, Android,
            Windows, UNIX, etc.)</li>
            <li>most programming languages and infrastructures (even <a
            href="https://www.docker.com/blog/getting-started-with-docker-for-arm-on-linux/">Docker</a>!)</li>
            <li>large number of tooling, analysis, debugging
            capabilities</li>
            <li>complex code can run, but <strong>increases analysis
            difficulty</strong></li>
            </ul>
            <p>Due to their power (and cost) these types of systems are
            only used when really necessary or in higher-end systems
            such as mobile phones and autonomous cars.</p>
            </section>
            <section id="system-on-a-chip-soc" class="level3"
            data-number="2.2.4">
            <h3 data-number="2.2.4"><span
            class="header-section-number">2.2.4</span> System-on-a-Chip
            (SoC)</h3>
            <p>An SoC <strong>integrates</strong> most components in and
            around a processor into a <strong>single</strong> circuit,
            viz.,</p>
            <ul>
            <li>processor/chip → could be a microcontroller or even a
            microprocessor</li>
            <li>memory and memory interfaces</li>
            <li>I/O devices</li>
            <li>buses (memory and I/O)</li>
            <li>storage (e.g., flash) and sometimes even secondary
            storage</li>
            <li>radio modems</li>
            <li>(sometimes) accelerators such as GPUs</li>
            </ul>
            <p>All of these are placed on a <strong>single
            substrate</strong>.</p>
            <p>SoCs are often designed in <code>C++</code>,
            <code>MATLAB</code>, <code>SystemC</code>, etc. Once the
            hardware architectures are defined, additional hardware
            elements are written in hardware description languages,
            e.g., register transfer levels (<code>RTL</code>) <a
            href="#fn2" class="footnote-ref" id="fnref2"
            role="doc-noteref"><sup>2</sup></a>.</p>
            <p>Additional components could include,</p>
            <ul>
            <li>DAC</li>
            <li>ADC</li>
            <li>radio and signal processing</li>
            <li>wireless modems</li>
            <li><a
            href="https://www.amd.com/en/products/adaptive-socs-and-fpgas/soc/zynq-7000.html"><em>programmable
            logic</em></a>.</li>
            <li>networks on chip (NoC) <a href="#fn3"
            class="footnote-ref" id="fnref3"
            role="doc-noteref"><sup>3</sup></a></li>
            </ul>
            <p>In some sense, an SoC is an <em>integration of a
            processor with peripherals</em>. New hardware elements</p>
            <p>Some examples of modern SoCs:</p>
            <div class="multicolumn">
            <div>
            <p><img src="./img/embedded_arch/broadcom_pi_chip.png" width="200" title="Broadcom SoC chip used in the Raspberry Pi"></p>
            <p>Broadcom Soc from Raspberry Pi</p>
            </div>
            <div>
            <p><img src="./img/embedded_arch/Apple_M1.jpg" width="200" title="Apple M1 SoC"></p>
            <p>Apple M1 SoC</p>
            </div>
            </div>
            <p>The integration of all hardware components has some
            interesting side-effects:</p>
            <table>
            <colgroup>
            <col style="width: 34%" />
            <col style="width: 31%" />
            <col style="width: 34%" />
            </colgroup>
            <thead>
            <tr>
            <th>effect</th>
            <th>benefit</th>
            <th>problems</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>tight integration</td>
            <td>better performance, fewer latencies</td>
            <td>cannot replace individual components</td>
            </tr>
            <tr>
            <td>custom code/firmware</td>
            <td>better use of hardware</td>
            <td>not reusable in other systems</td>
            </tr>
            <tr>
            <td>custom software libraries</td>
            <td>easier programming of SoC</td>
            <td>reduces code reusability in other systems</td>
            </tr>
            <tr>
            <td>low power consumption</td>
            <td>better battery life, less heat</td>
            <td>(potentially) slower</td>
            </tr>
            </tbody>
            </table>
            <p>Depending on the processor/microcontroller that sits at
            the center of the SoC, the software stack/capabilities can
            vary. Many commons SoCs exhibit the following software
            properties:</p>
            <ul>
            <li>usually use contemporary operating systems, though
            optimized for embedded/SoC systems → e.g., <a
            href="http://www.raspbian.org">Raspbian</a> aka Rasberry Pi
            OS. Hence, they can handle multiprocessing, virtual memory,
            different scheduling policies, etc.</li>
            <li>can be programmed using most common programming
            languages → <code>C</code>, <code>C++</code>,
            <code>python</code>, <code>java</code>, even <a
            href="https://medium.com/@kenichisasagawa/rediscovering-the-joy-of-hardware-hacking-with-raspberry-pi-and-lisp-574c833ab20e"><code>lisp</code></a>!</li>
            </ul>
            <p>The Raspberry Pi is a common example of a system that
            uses a <a
            href="https://www.raspberrypi.com/documentation/computers/processors.html">Broadcom
            BCM series of SoCs</a>. We use the <a
            href="https://www.raspberrypi.com/documentation/computers/processors.html#bcm2711">BCM2711</a>
            SoC in our course for the Raspberry Pi 4-B.</p>
            </section>
            <section id="embedded-accelarators-e.g.-gpu-enabled-systems"
            class="level3" data-number="2.2.5">
            <h3 data-number="2.2.5"><span
            class="header-section-number">2.2.5</span> Embedded
            Accelarators (e.g. GPU-enabled systems)</h3>
            <p>There are hardware platforms that include
            <strong>accelerators</strong> in embedded systems, e.g., <a
            href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/">GPUs</a>,
            <a
            href="https://www.nature.com/articles/s41928-022-00778-y">AI-enabled
            silicon</a>, <a
            href="https://www.amd.com/en/products/adaptive-socs-and-fpgas/soc/zynq-7000.html">extra
            programmable FPGA fabric</a>, <a
            href="https://developer.arm.com/documentation/100230/0002/functional-description/external-coprocessors/configuring-which-coprocessors-are-included-in-secure-and-non-secure-states">security
            features</a>, etc. The main idea is that certain computation
            can be <em>offloaded</em> to these accelerators while the
            main CPU continues to process other code/requests. The
            accelerators are specialized for certain computations (e.g.,
            parallel matrix multiplications on GPUs, AES encryption).
            Some chips include FPGA fabric where the designer/user can
            <em>implement their own custom logic/accelerators</em>.</p>
            <p>In a loose sense, the <a
            href="https://navio2.hipi.io">Navio2</a> can be considered
            as a hardware coprocessor for the Raspbery Pi.</p>
            <p>The <a
            href="https://www.nvidia.com/en-us/autonomous-machines/embedded-systems/jetson-orin/">NVidia
            Jetson Orin</a> is a good example of an AI/GPU focussed
            embedded processor:</p>
            <p><img src="./img/embedded_arch/jetson-agx-orin-4c25-d-2x.png" width="300" title="NVIDIA Jetson AGX Orin 64 GB"></p>
            <p><br></p>
            <p>This system’s <a
            href="https://www.techpowerup.com/gpu-specs/jetson-agx-orin-64-gb.c4085">specifications</a>:</p>
            <ul>
            <li>1300 MHz clock speeds</li>
            <li>64 GB Memory</li>
            <li>256 bit memory bus</li>
            <li>204 GB/s bandwidth</li>
            <li>supports a variety of graphics features (DirectX,
            OpenGL, OpenCL, CUDA, Vulkan and Shader Models )</li>
            <li>maximum of 60W power</li>
            <li><strong>275 trillion</strong> operations/s (TOPS)!</li>
            </ul>
            <p>These systems are finding a lot of use in autonomous
            systems since they pack so much processing power into such a
            small form factor</p>
            </section>
            <section id="asics-and-fpgas" class="level3"
            data-number="2.2.6">
            <h3 data-number="2.2.6"><span
            class="header-section-number">2.2.6</span> ASICs and
            FPGAs</h3>
            <p>Application-specific integrated circuits (ASICs) and
            field programmable gate arrays (FPGAs). These platforms
            combine the advantages of both, hardware (<em>speed</em>)
            and software (<em>flexibility/programmability</em>). They
            are similar, yet different. Both are semiconductor devices
            that include <strong>programmable logic gates</strong> but
            an ASIC is <em>static</em> – i.e., once the board has been
            “programmed” it cannot be changed while an FPGA, as the name
            implies, allows for “reprogramming”.</p>
            <p>ASICs are <strong>custom-designed</strong> for specific
            applications and provide high efficiency and performance.
            FPGAs are <strong>reprogramamble</strong> devices that
            provide significant flexibility. Many designers also used it
            for prototyping hardware components (before they are
            eventually included either in the processors or custom
            ASICs). The <a
            href="https://www.wevolver.com/article/asic-vs-fpga">choice
            between ASICs and FPGAs</a> depends entirely on the
            application requirements and other factors such as cost.</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="./img/embedded_arch/asic.webp" width="200"></td>
            <td><img src="./img/embedded_arch/xilinx_spartan_fpga.webp" width="200"></td>
            </tr>
            <tr>
            <td>An ASIC</td>
            <td>Xilinx Spartan FPGA</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <section id="asics" class="level4" data-number="2.2.6.1">
            <h4 data-number="2.2.6.1"><span
            class="header-section-number">2.2.6.1</span> ASICs</h4>
            <p>These are specialized semiconductor devices – to
            implement a <em>custom</em> function, e.g., cryptocurrency
            mining, nuclear reactor control, televisions. ASICs are
            tailored to their specific applications. Once created, it
            cannot be reprogrammed or modified. ASICs are created using
            a process known as <a
            href="https://www.sciencedirect.com/topics/physics-and-astronomy/photolithography">photolithography</a>,
            a method to prepare nanoparticles, that allows components to
            be “etched” on to a silicon wafer.</p>
            <p>The <a
            href="https://www.wevolver.com/article/the-ultimate-guide-to-asic-design-from-concept-to-production">ASIC
            design process</a>, while expensive and time consuming,
            becomes valuable for <em>high-volume</em> products as the
            per-unit cost decrease when production nunbers increase.</p>
            <table>
            <thead>
            <tr>
            <th>advantages</th>
            <th>disadvantages</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>high performance</td>
            <td>lack of flexibility</td>
            </tr>
            <tr>
            <td>low power consumption</td>
            <td>high initial costs</td>
            </tr>
            <tr>
            <td>small form factor</td>
            <td>long development time</td>
            </tr>
            <tr>
            <td>ip protection</td>
            <td>obsolescence risk</td>
            </tr>
            <tr>
            <td>good for mass production</td>
            <td>risks with manufacturing yields</td>
            </tr>
            <tr>
            <td>can integrate multiple functions</td>
            <td>design complexity</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            <section id="fpgas" class="level4" data-number="2.2.6.2">
            <h4 data-number="2.2.6.2"><span
            class="header-section-number">2.2.6.2</span> FPGAs</h4>
            <p>These are also semiconductor devices but they can be
            <strong>preprogrammed</strong> to implement various circuits
            and functions. Designers can change the functionality
            <strong>after</strong> the curcuits have been embossed onto
            the hardware. Hence, they’re good for systems that might
            require changes at design time and rapid prototyping. An
            FPGA is a collection of programmable logic and
            interconnects. They include lookup tables (LUTs) and other
            parts that can be used to develop multiple, fairly
            wide-ranging, functions. The programmable blocks can be
            connected to each other via the interconnects. Some FPGAs
            even come with additional flash memory.</p>
            <p><a href="https://www.wevolver.com/article/fpga">FPGAs are
            programmed</a> using hardware description languages such as
            Verilog/VHDL.</p>
            <table>
            <thead>
            <tr>
            <th>advantages</th>
            <th>disadvantages</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>flexibility</td>
            <td>lower performance</td>
            </tr>
            <tr>
            <td>shorter development time</td>
            <td>higher power consumption</td>
            </tr>
            <tr>
            <td>upgradability</td>
            <td>high design complexity</td>
            </tr>
            <tr>
            <td>lower (initial) costs</td>
            <td>higher per-unit costs</td>
            </tr>
            <tr>
            <td>better processing capabilities</td>
            <td>design complexity</td>
            </tr>
            <tr>
            <td>lower obsolescence risks</td>
            <td>larger form factor</td>
            </tr>
            </tbody>
            </table>
            </section>
            </section>
            </section>
            <section id="communication-and-io" class="level2"
            data-number="2.3">
            <h2 data-number="2.3"><span
            class="header-section-number">2.3</span> Communication and
            I/O</h2>
            <p>Embedded systems need to <strong>communicate</strong>
            and/or <strong>interface</strong> with various elements:</p>
            <ul>
            <li>the physical world via sensors and actuators</li>
            <li>computers for programming (of the embedded system) or
            for data transfer</li>
            <li>with other embedded systems/nodes</li>
            <li>handheld devices</li>
            <li>with the internet (either public or to access back end
            servers)</li>
            <li>satellites?</li>
            </ul>
            <p>Hence a large number of communication standards and I/O
            interfaces have been developed over the years. Let’s look at
            a few of them:</p>
            <ol type="1">
            <li><a href="#uart--rs-232">serial (UART)</a> → e.g., RS
            232</li>
            <li><a href="#synchronous--i2c-and-spi">synchronous</a> →
            I2C, SPI</li>
            <li><a href="#general-purpose-io-gpio">general-purpose
            I/O</a> → GPIO</li>
            <li><a href="#jtag-debugging-interface">debugging
            interface</a> → JTAG</li>
            <li><a href="#controller-area-network-can">embedded internal
            communication</a> → CAN</li>
            <li><a href="#other-broadly-used-protocols">other broadly
            used protocols</a> → USB, Ethernet/WiFi, Radio,
            Bluetooth</li>
            </ol>
            <section id="uart-rs-232" class="level3"
            data-number="2.3.1">
            <h3 data-number="2.3.1"><span
            class="header-section-number">2.3.1</span> UART |
            RS-232</h3>
            <p>Serial communication standards are used extensively
            across many domains, mainly due to their
            <strong>simplicity</strong> and <strong>low hardware
            overheads</strong>. The most common among these are the
            <em>asynchronous serial communication systems</em>.</p>
            <p>From <a
            href="https://en.wikipedia.org/wiki/Asynchronous_serial_communication">Wikipedia</a>:</p>
            <blockquote>
            <p>Asynchronous serial communication is a form of serial
            communication in which the communicating endpoints’
            interfaces are not continuously synchronized by a common
            clock signal. Instead of a common synchronization signal,
            the data stream contains synchronization information in form
            of start and stop signals, before and after each unit of
            transmission, respectively. The start signal prepares the
            receiver for arrival of data and the stop signal resets its
            state to enable triggering of a new sequence.</p>
            </blockquote>
            <p>The following figure shows a communication sample that
            demonstrates these principles:</p>
            <p><img src="img/embedded_arch/comms/Puerto_serie_Rs232.png" width="300"></p>
            <p>We see that each byte has a <code>start</code> bit,
            <code>stop</code> bit and eight <code>data</code> bits. The
            last bit is often used as a <code>parity</code> bit. All of
            these “standards” (i.e., the start/stop/parity bits) must be
            <em>agreed upon ahead of time</em>.</p>
            <p>A <strong>universal asynchronous
            receiver-transmitter</strong> (<strong>UART</strong>) then
            is a peripheral device for such asynchronous commnication;
            the data format and transmission speeds are configurable. It
            sends data bits <em>one-by-one</em> (from least significant
            to most). The precise timing is handlded by the
            communication channel.</p>
            <p>The electric <em>signalling levels</em> are handled by an
            external driver circuit. Common signal levels:</p>
            <ul>
            <li><a
            href="https://www.analog.com/en/resources/technical-articles/fundamentals-of-rs232-serial-communications.html">RS
            232</a></li>
            <li><a
            href="https://www.renkeer.com/what-is-rs485/">RS-485</a></li>
            <li>raw <a
            href="https://www.seeedstudio.com/blog/2019/12/11/rs232-vs-ttl-beginner-guide-to-serial-communication">TTL</a></li>
            </ul>
            <p>Here we will focus on the <strong>RS-232</strong>
            standard since it is most widely used UART signaling level
            standard today. The full name of the standard is:
            “EIA/TIA-232-E Interface Between Data Terminal Equipment and
            Data Circuit-Termination Equipment Employing Serial Binary
            Data Interchange” (“EIA/TIA” stands for the Electronic
            Industry Association and the Telecommunications Industry
            Association). It was introduced in 1962 and has since been
            updated <em>four</em> times to meet evolving needs.</p>
            <p>The RS-232 is a <em>complete</em> standard in that it
            specifies,</p>
            <ul>
            <li>(common) voltage and signal levels</li>
            <li>(common) pin and wiring configurations</li>
            <li>(minimal) control information between
            host/peripherals</li>
            </ul>
            <p>The RS-232 specifies the electrical, functional and
            mechanical characteristics to meet all of the above
            criteria.</p>
            <p>For instance, the <em>electrical</em> characteristics are
            defined in the following figure:</p>
            <p><img src="img/embedded_arch/comms/rs232-electrical.gif" width="400"></p>
            <p>Details:</p>
            <ul>
            <li><strong>high</strong> level [<strong>logical
            <code>0</code></strong>] (aka “marking”) → <code>+5V</code>
            to <code>+15V</code> (realistically <code>+3V</code> to
            <code>+15V</code>)</li>
            <li><strong>low</strong> level [<strong>logical
            <code>1</code></strong>] (aka “spacing”) → <code>-5V</code>
            to <code>-15V</code> (realistically <code>-3V</code> to
            <code>-15V</code>)</li>
            </ul>
            <p>Other properties also defined, <em>e.g.</em>, “<a
            href="https://en.wikipedia.org/wiki/Slew_rate">slew
            rate</a>”, impedance, capacitive loads, etc.</p>
            <p>The standard also defines the mechanical interfaces,
            i.e., the <em>pin connector</em>:</p>
            <p><img src="img/embedded_arch/comms/rs232_pins.gif" width="400"></p>
            <p>While the official standard calls for a 25-pin connector,
            it is rarely used. Instead, the <strong>9-pin</strong>
            connector (shown on the right in the above figure) is in
            common use.</p>
            <p>You can read more details about the standard here: <a
            href="https://www.analog.com/en/resources/technical-articles/fundamentals-of-rs232-serial-communications.html">RS
            232</a></p>
            </section>
            <section id="synchronous-i2c-and-spi" class="level3"
            data-number="2.3.2">
            <h3 data-number="2.3.2"><span
            class="header-section-number">2.3.2</span> Synchronous |
            I<sup>2</sup>C and SPI</h3>
            <p>Synchronous Serial Interfaces (SSIs) are a widely used in
            industrial applications between a master device
            (e.g. controller) and a slave device (e.g. sensor). It is
            based on the <a
            href="https://www.analog.com/media/en/technical-documentation/tech-articles/guide-to-selecting-and-using-rs232-rs422-and-rs485-serial-data-standards--maxim-integrated.pdf">RS-422</a>
            standards and has a high protocol efficiency as well
            multiple hardware implementations.</p>
            <p>SSI properties:</p>
            <ul>
            <li><a
            href="https://en.wikipedia.org/wiki/Differential_signalling">differential
            signalling</a></li>
            <li>simplex (i.e., unidirectional communication only)</li>
            <li>non-multiplexed</li>
            <li>point-to-point and</li>
            <li>uses time-outs to frame the data.</li>
            </ul>
            <section id="i2c" class="level4" data-number="2.3.2.1">
            <h4 data-number="2.3.2.1"><span
            class="header-section-number">2.3.2.1</span>
            I<sup>2</sup>C</h4>
            <p>The <a
            href="https://www.ti.com/lit/an/sbaa565/sbaa565.pdf">Inter-Integrated
            Circuit</a> (I<sup>2</sup>C, IIC, I2C) is a synchronous,
            multi-controller/multi-target (historically termed as
            multi-master/multi-slave), single-ended, serial
            communication bus. I2C systems are used for <em>attaching
            low-power integrated circuits to processors and
            microcontrollers</em> – usually for short distance or
            <em>intra-board communication</em>.</p>
            <p>I2C components are found in a wide variety of products,
            <em>e.g.,</em></p>
            <ul>
            <li>EEPROMs</li>
            <li>VGA/DVI/HDMI connectors</li>
            <li>NVRAM chips</li>
            <li>real-time clocks</li>
            <li>reading hardware monitors and sensors</li>
            <li>controlling actuators</li>
            <li>DAC/ADC</li>
            <li>controlling LCD/OLEDs displays</li>
            <li>changing computer display settings (contrast,
            brightness, etc.)</li>
            <li>controlling speaker volume</li>
            <li>and many many more</li>
            </ul>
            <p>The main advantage of I2C is that a microcontroller can
            control a <em>network</em> of chips with just
            <strong>two</strong> general-purpose I/O pins (serial data
            line and a serial clock line) and software. A controller
            device can communicate with any target device through a
            unique I2C address sent through the serial data line. Hence
            the two signals are:</p>
            <table style="width:100%;">
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 35%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>line</th>
            <th>voltage</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>serial data line (SDL)</td>
            <td><code>+5V</code></td>
            <td>transmit data to or from target devices</td>
            </tr>
            <tr>
            <td>serial clock line (SCL)</td>
            <td><code>+3V</code></td>
            <td>synchronously clock data in or out of the target
            device</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Both are bidirectional and pulled up with resistors.</p>
            <p>Here is a typical implementation of I2C:</p>
            <p><img src="img/embedded_arch/comms/i2c_implementation.png" width="400"></p>
            <p>An I2C chip example (used for controlling certain TV
            signals):</p>
            <p><img src="img/embedded_arch/comms/i2c_tv_control.jpg" width="100"></p>
            <p>I2C is half-duplex communication where only a single
            controller or a target device is sending data on the bus at
            a time. In comparison, the serial peripheral interface (SPI)
            is a full-duplex protocol where data can be sent to and
            received back at the same time. An I2C controller device
            starts and stops communication, which removes the potential
            problem of bus contention. Communication with a target
            device is sent through a unique address on the bus. This
            allows for both multiple controllers and multiple target
            devices on the I2C bus.</p>
            <p>I2C communication details (initiated from the controller
            device):</p>
            <table>
            <colgroup>
            <col style="width: 70%" />
            <col style="width: 29%" />
            </colgroup>
            <thead>
            <tr>
            <th>condition</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>I2C <code>START</code></td>
            <td>the controller device first pulls the SDA low and then
            pulls the SCL low</td>
            </tr>
            <tr>
            <td>I2C <code>STOP</code></td>
            <td>the SCL releases high and then SDA releases high</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/embedded_arch/comms/i2c_start_stop.png" width="300"></p>
            <p><br></p>
            <p>I2C communication is split into: <strong>frames</strong>.
            Communciation starts when one controller sends an
            <code>address frame</code> after a <code>START</code>. This
            is followed by one or more <code>data frames</code>, each
            consisting of <strong>one byte</strong>. Each frame also has
            an <code>acknowledgement</code> bit. An example of two I2C
            communication frames:</p>
            <p><img src="img/embedded_arch/comms/i2c_frames.png"></p>
            <p><br></p>
            <p>You can read more at: <a
            href="https://www.ti.com/lit/an/sbaa565/sbaa565.pdf">I2C</a>.</p>
            </section>
            <section id="spi" class="level4" data-number="2.3.2.2">
            <h4 data-number="2.3.2.2"><span
            class="header-section-number">2.3.2.2</span> SPI</h4>
            <p>The <a
            href="https://www.analog.com/en/resources/analog-dialogue/articles/introduction-to-spi-interface.html">Serial
            Peripheral Interface</a> (SPI) has become the de facto
            standard for <em>synchronous</em> serial communication. It
            is used in embedded systems, especially between
            microcontrollers and peripheral ICs such as sensors, ADCs,
            DACs, shift registers, SRAM, <em>etc.</em></p>
            <p>The main aspect of SPI is that one main device
            <strong>orchestrates communication</strong> with one ore
            more sub/peripheral devices by <strong>driving the clock and
            chip select signals</strong>.</p>
            <p>SPI interface properties:</p>
            <ul>
            <li><em>synchronous</em></li>
            <li><em>full duplex</em></li>
            <li><em>main-subnode</em> (formerly called
            “master-slave”)</li>
            <li>data from the main or the subnode is synchronized on the
            rising or falling clock edge</li>
            <li>main and subnode can transmit data at the same time</li>
            <li>interface can be 3 or 4-wire (4 wire version is more
            popular)</li>
            </ul>
            <table>
            <colgroup>
            <col style="width: 46%" />
            <col style="width: 53%" />
            </colgroup>
            <thead>
            <tr>
            <th>microchip SPI</th>
            <th>basic SPI Interface</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/embedded_arch/comms/spi_microchip.avif" width="100"></td>
            <td><img src="img/embedded_arch/comms/spi_basic.png" width="500"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>The SPI interface contains the following wires:</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 41%" />
            <col style="width: 32%" />
            </colgroup>
            <thead>
            <tr>
            <th>signal</th>
            <th>description</th>
            <th>function</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>SCLK</code></td>
            <td>serial clock</td>
            <td>clock signal from main</td>
            </tr>
            <tr>
            <td><code>CS</code></td>
            <td>chip/serial select</td>
            <td>To select which host to communicate with</td>
            </tr>
            <tr>
            <td><code>MOSI</code></td>
            <td>main out, subnode In</td>
            <td>serial data out (SDO) for host to target
            communication</td>
            </tr>
            <tr>
            <td><code>MISO</code></td>
            <td>main in, subnode Out</td>
            <td>serial data in (SDI) for target to host
            communication</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>The main node generates the clock signal. Data
            transmissions between main ahd sub nodes is synchronized by
            that clock signal generated by main. SPI devices support
            <em>much higher clock frequencies</em> than I2C. The
            <code>CS</code> signal is used to select the subnode. Note
            that this is an <strong>active low signal</strong>,
            <em>i.e.,</em> a low (<code>0</code>) is a selection and a
            high (<code>1</code>) is a disconnect. SPI is a full-duplex
            interface; both main and subnode can send data at the same
            time via the MOSI and MISO lines respectively. During SPI
            communication, the data is simultaneously transmitted
            (shifted out serially onto the MOSI/SDO bus) and received
            (the data on the bus (MISO/SDI) is sampled or read in).</p>
            <p><strong>Example</strong>: the <a
            href="https://www.analog.com/en/resources/analog-dialogue/articles/introduction-to-spi-interface.html">following
            example</a> demonstrates the significant savings and
            simplification in systems design (reduce the number of GPIO
            pins required).</p>
            <p>Consider the ADG1412 switch being managed by a
            microcontroller as follows:</p>
            <p><img src="img/embedded_arch/comms/spi_adg_example1.svg" width="300"></p>
            <p>Now, as the number of switches increases, the requirement
            on GPIO pins also increases significantly. A
            <code>4x4</code> configuration requires <code>16</code> GPI
            pins, thus reducing the number of pins available for the
            microcontroller for other tasks, as follows:</p>
            <p><img src="img/embedded_arch/comms/spi_adg_example2.svg" width="300"></p>
            <p>One approach to reduce the number of pins would be to use
            a serial-to-parallel convertor:</p>
            <p><img src="img/embedded_arch/comms/spi_adg_example3.svg" width="300"></p>
            <p>This reduces the pressure on the number of GPIO pins but
            still introduces additional circuitry.</p>
            <p>Using an SPI-enabled microcontroller reduces the number
            of GPIOs required and and eliminates the overheads of the
            needing additional chips (serial-to-paralle convertor):</p>
            <p><img src="img/embedded_arch/comms/spi_adg_example4.svg" width="300"></p>
            <p>In fact, using a different SPI configuration
            (“<strong>daisy-chain</strong>”), we can optimize the GPIO
            count even further!</p>
            <p><img src="img/embedded_arch/comms/spi_adg_example5.svg" width="300" height="250"></p>
            <p>You can read more about <a
            href="https://www.analog.com/en/resources/analog-dialogue/articles/introduction-to-spi-interface.html">SPI
            here</a>.</p>
            </section>
            </section>
            <section id="general-purpose-io-gpio" class="level3"
            data-number="2.3.3">
            <h3 data-number="2.3.3"><span
            class="header-section-number">2.3.3</span> General-Purpose
            I/O (GPIO)</h3>
            <p>A GPIO is a <strong>signal pin</strong> on an integrated
            circuit or board that can be used to perform <em>digital I/O
            operations</em>. By design, it <strong>has no predefined
            purpose</strong> → can be used by hardware/software
            developers to perform functions <em>they choose</em>,
            <em>e.g.,</em></p>
            <ul>
            <li>GPIO pins can be enabled or disabled.</li>
            <li>GPIO pins can be configured to be input or output.</li>
            <li>input values are readable, often with a 1 representing a
            high voltage, and a 0 representing a low voltage.</li>
            <li>input GPIO pins can be used as “interrupt” lines, which
            allow a peripheral board connected via multiple pins to
            signal to the primary embedded board that it requires
            attention.</li>
            <li>output pin values are both readable and writable.</li>
            </ul>
            <p>GPIOs can be implemented in a variety of ways,</p>
            <ul>
            <li>as a <em>primary</em> function of the microcontrollers,
            <em>e.g.</em>, <a
            href="https://www.geeksforgeeks.org/programmable-peripheral-interface-8255/">Intel
            8255</a></li>
            <li>as an <em>accessory</em> to the chip</li>
            </ul>
            <p>While microcontrollers may use GPIOs are their primary
            external interface, many a time the pins may be capable of
            other functions as well. In such instances, it may be
            necessary to configure the pins using other functions.</p>
            <p>Some examples of chips with GPIO pins:</p>
            <table>
            <colgroup>
            <col style="width: 27%" />
            <col style="width: 31%" />
            <col style="width: 40%" />
            </colgroup>
            <thead>
            <tr>
            <th>Intel 8255</th>
            <th>PIC microchip</th>
            <th>ASUS Tinker</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/embedded_arch/comms/gpio_Ic-photo-Intel--D8255.JPG" width="250"></td>
            <td><img src="img/embedded_arch/comms/gpio_microchip_PIC18F8720.jpg" width="150"></td>
            <td><img src="img/embedded_arch/comms/gpio_Asus_Tinker_Board.jpg" width ="200"></td>
            </tr>
            <tr>
            <td>24 GPIO pins</td>
            <td>29 GPIO pins</td>
            <td>28 GPIO pins</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>GPIOs are used in a diverse variety of applications,
            limited only by the electrical and timing specifications of
            the GPIO interface and the ability of software to interact
            with GPIOs in a sufficiently timely manner.</p>
            <p>Some “properties”/applications of GPIOs:</p>
            <ul>
            <li>GPIOs use standard logic levels and cannot supply
            significant current to output loads</li>
            <li>high-current output buffers or relays can be used to
            control high-power devices</li>
            <li>input buffers, relays, or opto-isolators translate
            incompatible signals to GPIO logic levels</li>
            <li>GPIOs can control or monitor other circuitry on a board,
            such as enabling/disabling circuits, reading switch states,
            and driving LEDs</li>
            <li>multiple GPIOs can implement bit banging communication
            interfaces like I²C or SPI</li>
            <li>GPIOs can control analog processes via PWM, adjusting
            motor speed, light intensity, or temperature</li>
            <li>PWM signals from GPIOs can be converted to analog
            control voltages using RC filters</li>
            </ul>
            <p>GPIO interfaces vary widely. Most commonly, they’re
            simple <em>groups of pins</em> that can switch between
            input/output. On the other hand, each pin can be set up
            differently → set up/accept/source different voltages/drive
            strengths/pull ups and downs.</p>
            <p>Programming the GPIO:</p>
            <ul>
            <li>usually pin states are exposed via different interfaces,
            <em>e.g.,</em> <strong>memory-mapped I/O</strong>
            peripherals or dedicated I/O port instructions</li>
            <li>input values can be used as interrupts (IRQs)</li>
            </ul>
            <p>For more information on programming/using GPIOs, read
            these: <a
            href="https://docs.oracle.com/javame/8.0/me-dev-guide/gpio.htm">GPIO
            setup and use</a>, <a
            href="https://www.instructables.com/Raspberry-Pi-Python-scripting-the-GPIO/">Python
            scripting the GPIO in Raspberry Pis</a>, <a
            href="https://docs.nordicsemi.com/bundle/ps_nrf52810/page/gpio.html">general
            purpose I/O</a>, <a
            href="https://projects.raspberrypi.org/en/projects/physical-computing/1">GPIO
            setup in Raspberry Pi</a>.</p>
            </section>
            <section id="jtag-debugging-interface" class="level3"
            data-number="2.3.4">
            <h3 data-number="2.3.4"><span
            class="header-section-number">2.3.4</span> JTAG Debugging
            Interface</h3>
            <p>The JTAG standard (named after the “Joint Test Action
            Group”), technically the <a
            href="https://web.archive.org/web/20170830070123/http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/jtag-101-ieee-1149x-paper.pdf">IEEE
            Std 1149.1-1990 IEEE Standard Test Access Port and
            Boundary-Scan Architecture</a>, is an industry standard for
            <strong>testing and verification of printed circuit
            boards</strong>, <em>after manufacture</em>.</p>
            <p>“JTAG”, depending on the context, could stand for one or
            more of the following:</p>
            <ul>
            <li>implementation of IEEE 1149.x for Board Test, or
            Boundary Scan testing</li>
            <li>appliance used to program on board flash or eeprom
            devices on a circuit board</li>
            <li>hardware device used to debug microprocessor
            software</li>
            <li>hardware device used to test a board using Boundary
            Scan</li>
            </ul>
            <p>The basic building block of a JTAG OCD is the
            <strong>Test Access Point</strong> or <strong>TAP
            controller</strong>. This allows access to all the custom
            features within a specific processor, and must support a
            minimum set of commands. On-chip debugging is a
            <em>combination of hardware and software</em>.</p>
            <table>
            <thead>
            <tr>
            <th>type</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>hardaware</td>
            <td><strong>on chip debug</strong> (OCD)</td>
            </tr>
            <tr>
            <td>software</td>
            <td><strong>in-circuit-emulator</strong> (ICE)/JTAG
            emulator</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>The off-chip parts are actually PC peripherals that need
            corresponding drivers running on a separate computer. On
            most systems, JTAG-based debugging is available from the
            very first instruction after CPU reset, letting it assist
            with development of early boot software which runs before
            anything is set up. The JTAG emulator allows developers to
            access the embedded system at the <strong>machine code
            level</strong> if needed! Many silicon architectures (Intel,
            ARM, PowerPC, etc.) have built entire infrastructures and
            extensions around JTAG.</p>
            <p>A high-level overview of the JTAG architecture/use:</p>
            <p><img src="img/embedded_arch/comms/jtag_high_level.png" width="400"></p>
            <p><br></p>
            <p>JTAG now allows for,</p>
            <ul>
            <li>processors can not be <em>halted</em>,
            <em>single-stepped</em> or <em>run freely</em></li>
            <li>can set code <em>breakpoints</em> for both, code in RAM
            as well as ROM/flash</li>
            <li><em>data breakpoints</em> are available</li>
            <li><em>bulk data download</em> to RAM</li>
            <li><em>access to registers and buses</em>, even without
            halting the processors!</li>
            <li><em>complex logic routines</em>, <em>e.g.,</em> ignore
            the first seven accesses to a register from one particular
            subroutine</li>
            </ul>
            <p>JTAG allows for <em>device programmer hardware</em>
            allows for transfering data into internal,
            <em>non-volatile</em> memory of the system! Hence, we can
            use JTAGs to <strong>program</strong> devices such as FPGAs.
            In fact, many memory chips also have JTAG interfaces. Some
            modern chips also allow access to the the (internal and
            external) data buses via JTAG.</p>
            <p><strong>JTAG interface</strong>: depending on the actual
            interface, JTAG has 2/4/5 pins. The 4/5 pin versions are
            designed so that <em>multiple chips</em> on a board can have
            their JTAG lines <strong>daisy-chained</strong> together if
            specific conditions are met.</p>
            <p>Schematic Diagram of a JTAG enabled device:</p>
            <p><img src="img/embedded_arch/comms/jtag_schematic_diagram.gif" width="300"></p>
            <p>The various pins signals in the JTAG TAP are:</p>
            <table>
            <colgroup>
            <col style="width: 38%" />
            <col style="width: 61%" />
            </colgroup>
            <thead>
            <tr>
            <th>signal</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>TCK</code></td>
            <td>synchronizes the internal state machine operations</td>
            </tr>
            <tr>
            <td><code>TMS</code></td>
            <td>sampled at the rising edge of <code>TCK</code> to
            determine the next state</td>
            </tr>
            <tr>
            <td><code>TDI</code></td>
            <td>data shifted into the device’s test or programming
            logic; sampled at the rising edge of <code>TCK</code> when
            the internal state machine is in the correct state</td>
            </tr>
            <tr>
            <td><code>TDO</code></td>
            <td>represents the data shifted out of the device’s test or
            programming logic and is valid on the falling edge of
            <code>TCK</code> when the internal state machine is in the
            correct state</td>
            </tr>
            <tr>
            <td><code>TRST</code></td>
            <td>optional pin which, when available, can reset the tap
            controller’s state machine</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>The TAP controller implements the following state
            machine:</p>
            <p><img src="img/embedded_arch/comms/jtag_tap_state_machine.gif" width="300"></p>
            <p><br></p>
            <p>To use the JTAG interface,</p>
            <ul>
            <li>host is connected to the target’s JTAG signals
            (<code>TMS</code>, <code>TCK</code>, <code>TDI</code>,
            <code>TDO</code>, etc.) through some kind of JTAG
            adapter</li>
            <li>adapter connects to the host using some interface such
            as USB, PCI, Ethernet, etc.</li>
            <li>host communicates with the TAPs by manipulating
            <code>TMS</code> and <code>TDI</code> in conjunction with
            <code>TCK</code></li>
            <li>host reads results through <code>TDO</code> (which is
            the only standard host-side input)</li>
            <li><code>TMS</code>/<code>TDI</code>/<code>TCK</code>
            output transitions create the basic JTAG communication
            primitive on which higher layer protocols build</li>
            </ul>
            <p><br></p>
            <p>For more information about JTAG, read: <a
            href="https://web.archive.org/web/20170830070123/http://www.intel.com/content/dam/www/public/us/en/documents/white-papers/jtag-101-ieee-1149x-paper.pdf">Intel
            JTAG Overview</a>, <a
            href="https://forums.raspberrypi.com/viewtopic.php?t=286115">Raspberry
            Pi JTAG programming</a>, <a
            href="https://www.xjtag.com/about-jtag/jtag-a-technical-overview/">Technical
            Guide to JTAG</a> and the <a
            href="https://en.wikipedia.org/wiki/JTAG">JTAG Wikipedia
            Entry</a> is quite detailed.</p>
            </section>
            <section id="controller-area-network-can" class="level3"
            data-number="2.3.5">
            <h3 data-number="2.3.5"><span
            class="header-section-number">2.3.5</span> Controller Area
            Network (CAN)</h3>
            <p>CAN is a vehicle bus standard to enable efficient
            communication between electronic control units (ECUs). CAN
            is,</p>
            <ul>
            <li>broadcast-based</li>
            <li>message-oriented</li>
            <li>uses arbitration → for data
            integrity/prioritization</li>
            </ul>
            <p>CAN <strong>does not</strong> need a a host controller.
            ECUs connected via the CAN bus can easily share information
            with each other. all ECUs are connected on a two-wire bus
            consisting of a twisted pair: CAN high and CAN low. The
            wires are often color coded:</p>
            <table>
            <tbody>
            <tr>
            <td>CAN high</td>
            <td>yellow</td>
            </tr>
            <tr>
            <td>CAN low</td>
            <td>green</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 57%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="img/embedded_arch/comms/can-twisted-can-bus-wiring-harness-high-low-green-yellow.svg" width="200"></td>
            <td><img src="img/embedded_arch/comms/CAN-bus_basic.svg" width="300"></td>
            </tr>
            <tr>
            <td>CAN wiring</td>
            <td>multi-ecu CAN setup</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>An ECU in a vehicle consists of:</p>
            <table>
            <tr>
            <th>
            components
            </th>
            <th>
            internal architecture
            </th>
            </tr>
            <tr>
            <td>
            <ul>
            <li>
            <b>microcontroller</b> to interpret/send out CAN messages
            </li>
            <li>
            <b>CAN controller</b> ensures all communication adheres to
            CAN protocols
            </li>
            <li>
            <b>CAN transceiver</b> connects CAN controller to the
            physical wires
            </li>
            </ul>
            </td>
            <td>
            <img src="img/embedded_arch/comms/can_ecu_internals.svg" width="250">
            </td>
            </tr>
            <tr>
            <td>
            </td>
            <td>
            </td>
            </tr>
            </table>
            <p><em>Any</em> ECU can broadcast on the CAN bus and the
            messages are accepted by <em>all</em> ECUs connected to it.
            Each ECU can either choose to ignore the message or act on
            it.</p>
            <blockquote>
            <p>what are the implications for
            <strong>security</strong>?</p>
            </blockquote>
            <p>While there is no “standard” CAN connector (each vehicle
            may use different ones), the <strong>CAN Bus DB9</strong>
            connector has become the de facto standard:</p>
            <p><img src="img/embedded_arch/comms/can-bus-db9-connector-pinout-d-sub.svg" width="350"></p>
            <p>The above figure shows the various pins and their
            signals.</p>
            <p><br></p>
            <p><strong>CAN Communication Protocols</strong>: CAN is
            split into:</p>
            <table>
            <tr>
            <th>
            layer
            </th>
            <th>
            relation to OSI stack
            </th>
            </tr>
            <tr>
            <td>
            <ul>
            <li>
            <b>data link</b>: CAN frame formats, <br>error handling,
            data transmission, <br>data integrity
            </li>
            <li>
            <b>physical</b>: cable types, <br>electrical signal levels,
            <br>node requirements, <br>cable impedance, etc.
            </li>
            </ul>
            </td>
            <td>
            <img src="img/embedded_arch/comms/can-bus-osi-model-7-layer-iso-11898-physical-data.svg" width="350">
            </td>
            </tr>
            <tr>
            <td>
            </td>
            <td>
            </td>
            </tr>
            </table>
            <p><br></p>
            <p>All communication over the CAN bus is done via the
            <strong>CAN frames</strong>. The <em>standard</em> CAN frame
            (with an <code>11-bit</code> identifier) is shown below:</p>
            <p><img src="img/embedded_arch/comms/CAN-bus-frame-standard-message-SOF-ID-RTR-Control-Data-CRC-ACK-EOF.svg" width="400"></p>
            <p><br></p>
            <p>While the lower-level CAN protocols described so far work
            on the two lowest layers of the OSI networking stack, it is
            still limiting. For instance, the CAN standard doesn’t
            discuss how to,</p>
            <ul>
            <li>decode RAW data</li>
            <li>handle larger data (more than 8 bytes)</li>
            </ul>
            <p>Hence, some <strong>higher-order</strong> protocols have
            been developed, <em>viz.,</em></p>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 57%" />
            </colgroup>
            <thead>
            <tr>
            <th>protocol</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><a
            href="https://www.csselectronics.com/pages/obd2-explained-simple-intro">OBD2</a></td>
            <td>on-board diagnostics in cars/trucks for diagnostics,
            maintenance, emissions tests</td>
            </tr>
            <tr>
            <td><a
            href="https://www.csselectronics.com/pages/uds-protocol-tutorial-unified-diagnostic-services">UDS</a></td>
            <td>Unified Diagnostic Services (UDS) used in automotive
            ECUs for diagnostics, firmware updates, routine testing</td>
            </tr>
            <tr>
            <td><a
            href="https://www.csselectronics.com/pages/ccp-xcp-on-can-bus-calibration-protocol">CCP/XCP</a></td>
            <td>used in embedded control/industrial automation for
            <em>off-the-shelf interoperability</em> between CAN
            devices</td>
            </tr>
            <tr>
            <td><a
            href="https://www.csselectronics.com/pages/j1939-explained-simple-intro-tutorial">SAE
            J1939</a></td>
            <td>for heavy-duty vehicles</td>
            </tr>
            <tr>
            <td><a
            href="https://www.csselectronics.com/pages/nmea-2000-n2k-intro-tutorial">NMEA
            2000</a></td>
            <td> used in maritime industry for connecting e.g. engines,
            instruments, sensors on boats</td>
            </tr>
            <tr>
            <td><a
            href="https://www.csselectronics.com/pages/isobus-introduction-tutorial-iso-11783">ISOBUS</a></td>
            <td>used in agriculture and forestry machinery to enable
            plug and play integration between vehicles/implements,
            <em>across brands</em></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>There also exist other higher-order protocols (numbering
            in the thousands) the most prominent of which are: ARINC,
            UAVCAN, DeviceNet, SafetyBUS p, MilCAN, HVAC CAN.</p>
            <p><br></p>
            <p>More details about CAN and its variants: <a
            href="https://www.csselectronics.com/pages/can-bus-simple-intro-tutorial">CAN
            Bus Explained</a>.</p>
            </section>
            <section id="other-broadly-used-protocols" class="level3"
            data-number="2.3.6">
            <h3 data-number="2.3.6"><span
            class="header-section-number">2.3.6</span> Other Broadly
            Used Protocols</h3>
            <p>Autonomous (and other embedded systems) use a variety of
            other communication protocols in order to interface with the
            external world and/or other systems (either other nodes in
            the system or external components such as back end
            clouds).</p>
            <p>Note that since many of these are well known and publicly
            documented, we won’t elaborate much here.</p>
            <p>Here are some of the well known communication protocols,
            also used in embedded systems:</p>
            <table>
            <colgroup>
            <col style="width: 57%" />
            <col style="width: 42%" />
            </colgroup>
            <thead>
            <tr>
            <th>protocol</th>
            <th>links</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>USB</td>
            <td>How USB works: <a
            href="https://www.circuitbread.com/tutorials/how-usb-works-introduction-part-1">part
            1</a>, <a
            href="https://www.circuitbread.com/tutorials/how-usb-works-communication-protocol-part-2">part2</a>,
            <a
            href="https://www.circuitbread.com/tutorials/how-usb-works-enumeration-and-configuration-part-3">part
            3</a>; <a
            href="https://www.beyondlogic.org/usbnutshell/usb1.shtml">USB
            in a Nutshell (very detailed)</a>.</td>
            </tr>
            <tr>
            <td>Ethernet</td>
            <td><a
            href="https://www.embedded.com/implement-reliable-embedded-ethernet-connectivity/">Reliable
            Embedded Ethernet</a>, <a
            href="https://www.google.com/books/edition/_/3ZPPBgAAQBAJ?hl=en&amp;gbpv=1&amp;pg=PA1">Embedded
            Ethernet and Internet (book, online)</a></td>
            </tr>
            <tr>
            <td>WiFi</td>
            <td><a
            href="https://ebulutvcu.github.io/COMST22_WiFi_Sensing_Survey.pdf">WiFi
            Sensing on the Edge (paper)</a></td>
            </tr>
            <tr>
            <td>Bluetooth</td>
            <td><a
            href="https://learn.sparkfun.com/tutorials/bluetooth-basics/all">Bluetooth
            Basics</a>, <a
            href="https://novelbits.io/bluetooth-low-energy-ble-complete-guide/">Bluetooth
            Low Energy</a></td>
            </tr>
            <tr>
            <td>Radio</td>
            <td><a
            href="https://wiki.gnuradio.org/index.php/Embedded_Development_with_GNU_Radio">Embedded
            Development with GNU Radio</a></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            </section>
            <section id="raspberry-pi-and-navio2" class="level2"
            data-number="2.4">
            <h2 data-number="2.4"><span
            class="header-section-number">2.4</span> Raspberry Pi and
            Navio2</h2>
            <p>Let us look at the two architectures we use extensively
            in this course:</p>
            <ul>
            <li><a
            href="https://www.raspberrypi.com/products/raspberry-pi-4-model-b/specifications/">Raspberry
            Pi</a> model 4(b)</li>
            <li><a href="https://navio2.hipi.io">Navio2</a> → autopilot
            hat for the Raspberry Pi</li>
            </ul>
            <p>The high-level architecture of the Pi shows many of the
            components we have discussed so far:</p>
            <p><img src="img/embedded_arch/pi-4-architectural_features.png" width="400"></p>
            <p>In particular, the Pi has,</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 66%" />
            </colgroup>
            <thead>
            <tr>
            <th>component</th>
            <th>description/details</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>processor</td>
            <td>Broadcomm <strong>BCM2711</strong>, Quad core Cortex-A72
            (ARM v8) 64-bit SoC at 1.8GHz</td>
            </tr>
            <tr>
            <td>memory</td>
            <td>1GB, 2GB, 4GB or 8GB LPDDR4-3200 SDRAM</td>
            </tr>
            <tr>
            <td>network</td>
            <td>Wifi (2.4/5.0 GHz), Gigabit ethernet, Bluetooth/BLE</td>
            </tr>
            <tr>
            <td>I/O</td>
            <td>40 pin GPIO, USB 3.0/2.0/C</td>
            </tr>
            <tr>
            <td>storage</td>
            <td>Micro-SD Card</td>
            </tr>
            <tr>
            <td>misc</td>
            <td>micro-hdmi, stereo audio/video, displayport, camera
            port, power</td>
            </tr>
            <tr>
            <td>os</td>
            <td><a
            href="https://www.raspberrypi.com/software/">Raspberry Pi
            OS</a> (formerly called Raspbian)</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Read more about the Raspberry Pi: <a
            href="https://www.electronics-lab.com/project/raspberry-pi-4-look-hood-make/">Raspberry
            PI – A Look Under the Hood</a></p>
            <p><br></p>
            <p>The <strong>Navio2</strong> is a “hat” that adds the
            following to a Raspberry Pi:</p>
            <ul>
            <li>autopilot functionality</li>
            <li>multiple sensors</li>
            </ul>
            <p>The high-level architecture,</p>
            <p><img src="img/embedded_arch/navio2_features.jpg" width="400"></p>
            <p>As the figure shows, the Navio2 adds the following
            components:</p>
            <table>
            <colgroup>
            <col style="width: 32%" />
            <col style="width: 67%" />
            </colgroup>
            <thead>
            <tr>
            <th>component</th>
            <th>description/details</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>GNSS receiver</td>
            <td>for GPS signals</td>
            </tr>
            <tr>
            <td>high-precision barometer</td>
            <td>for measuring pressure (and altitude)</td>
            </tr>
            <tr>
            <td>(dual) IMU</td>
            <td>two 9 DOF with gyroscope, accelerometer, magnetometer,
            each</td>
            </tr>
            <tr>
            <td>RC I/O co-processor</td>
            <td>PWM, ADC, SBUS, PPM</td>
            </tr>
            <tr>
            <td>extension ports</td>
            <td>ADC, I2C, UART</td>
            </tr>
            <tr>
            <td>power supply</td>
            <td>triple redundant</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>More details about the Navio2 and how to program it: <a
            href="https://docs.emlid.com/navio2/">Navio2
            Documentation</a>.</p>
            <p><br> <br></p>
            </section>
            <section id="references" class="level2" data-number="2.5">
            <h2 data-number="2.5"><span
            class="header-section-number">2.5</span> References</h2>
            </section>
            </section>
            <section id="sensors-and-sensing" class="level1"
            data-number="3">
            <h1 data-number="3"><span
            class="header-section-number">3</span> Sensors and
            Sensing</h1>
            <p>An embedded/autonomous system <em>perceives</em> the
            physical world via sensors – either to gather information
            about its environment or to model its <em>own</em> state.
            Hence it is a critical component in the <em>sensing →
            planning → actuation</em> loop and a critical component in
            the design of embedded and autonomous systems.</p>
            <table>
            <colgroup>
            <col style="width: 46%" />
            <col style="width: 53%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="img/sense_planning_actuation.png" width="400"></td>
            <td><img src="img/stack_architecture/stack_overview.2.png" width="300"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Modern autonomous systems used a <em>wide array</em> of
            sensors. This is necessary due to:</p>
            <ul>
            <li>there is a need to measure <strong>different</strong>
            quantities, <em>e.g.,</em> GPS, velocity, objects,
            <em>etc.</em></li>
            <li>sensor measurements often have <strong>errors</strong> →
            hence, we need multiple sensors, often using
            <strong>different physical properties</strong> to measure
            the <em>same thing</em>; <em>e.g.,</em> LiDar and cameras
            can both be used to detect objects in front of, and around,
            an autonomous vehicle.</li>
            </ul>
            <p>At its core,</p>
            <blockquote>
            <p>a sensor captures a physical/chemical/environmental
            quantity and <strong>converts it to a digital
            quantity</strong>.</p>
            </blockquote>
            <p>(hence the need for an Analog-to-Digital Convertor (ADC)
            as we shall see later)</p>
            <p>By definition, sensors generate <strong>signals</strong>.
            A signal, <code>s</code>, is defined as a mapping from the
            <em>time</em> domain to a <em>value</em> domain:</p>
            <p><span
            class="math display"><em>s</em> : <em>D</em><sub><em>t</em></sub> ↦ <em>D</em><sub><em>v</em></sub></span></p>
            <p>where,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>D</em><sub><em>t</em></sub></span></td>
            <td>continuous or discrete <strong>time</strong> domain</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>D</em><sub><em>v</em></sub></span></td>
            <td>continuous or discrete <strong>value</strong>
            domain</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Note:</strong> remember that computers require
            <strong>discrete</strong> sequences of physical values.
            Hence, we need to <strong>convert</strong> the above into
            the discrete domain. The way to achieve this:
            <strong>sampling</strong>:</p>
            <p><img src="img/sensors/discretization_sampled.signal.svg" title="Sampling image from Wikipedia" width="300"></p>
            <p>The figure shows a continuous signal being sampled (in
            <font color="red"><b>red</b></font> arrows). We will discuss
            sampling and related issues later in this topic.</p>
            <section id="types-of-sensors" class="level2"
            data-number="3.1">
            <h2 data-number="3.1"><span
            class="header-section-number">3.1</span> Types of
            Sensors</h2>
            <p>Sensors come in various shapes and sizes. Usually
            designers of autonomous systems will develop a
            “<strong>sensor plan</strong> that will consider,</p>
            <ul>
            <li>required functionality</li>
            <li>sensor range(s)</li>
            <li>cost</li>
            </ul>
            <p>Hence, each autonomous system will likely have its own
            set of sensors (or sensor plan). <em>Typical</em> sensors
            found on modern autonomous systems can be classified based
            on the underlying physics used:</p>
            <table>
            <colgroup>
            <col style="width: 70%" />
            <col style="width: 29%" />
            </colgroup>
            <thead>
            <tr>
            <th>physical property</th>
            <th>sensor</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><a
            href="#inertial-measurement-units-imu"><em>internal</em>
            measurements</a></td>
            <td>IMU</td>
            </tr>
            <tr>
            <td><em>external</em> measurements</td>
            <td>GPS</td>
            </tr>
            <tr>
            <td><a
            href="#bouncing-of-electromagnetic-waves--lidar-and-mmwave">“bouncing”
            electromagnetic waves</a></td>
            <td>LiDAR, RADAR, mmWave Radar</td>
            </tr>
            <tr>
            <td>optical</td>
            <td>cameras, infrared sensors</td>
            </tr>
            <tr>
            <td><a href="#ultrasonic">accoustic</a></td>
            <td>ultrasonic sensors</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Some of the above can be combined to generate other
            sensing patterns, <em>e.g.,</em> <strong>stereo
            vision</strong> using multiple cameras or camera+LiDAR.</p>
            <p>We will go over <strong>some</strong> of these sensors
            and their underlying physical principles.</p>
            <section id="inertial-measurement-units-imu" class="level3"
            data-number="3.1.1">
            <h3 data-number="3.1.1"><span
            class="header-section-number">3.1.1</span> Inertial
            Measurement Units (IMU)</h3>
            <p>These sensors define the <strong>movement of a
            vehicle</strong>, along the three axes, in addition to other
            behaviors like acceleration and directionality. An IMU
            typically includes the following sensors:</p>
            <table>
            <colgroup>
            <col style="width: 24%" />
            <col style="width: 21%" />
            <col style="width: 24%" />
            <col style="width: 29%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="img/sensors/imu_exploded_view.jpg" width="600"></td>
            <td><img src="img/sensors/imu_accelerometer.png" width="400"></td>
            <td><img src="img/sensors/imu_gyro.png" width="400"></td>
            <td><img src="img/sensors/imu_magnetometer.png" width="400"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>As we see from the first picture above, an IMU also has a
            CPU (typically a microcontroller) to manage/collect/process
            the data from the sensors.</p>
            <p>The functions of the three sensors are:</p>
            <ol type="1">
            <li><strong>gyroscope</strong>: is an inertial sensor that
            measure an object’s angular rate with respect to an inertial
            reference frame. It measures the following movements:</li>
            </ol>
            <table>
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 37%" />
            <col style="width: 33%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="img/sensors/imu_yaw.gif"></td>
            <td><img src="img/sensors/imu_pitch.gif"></td>
            <td><img src="img/sensors/imu_roll.gif"></td>
            </tr>
            <tr>
            <td>“yaw”</td>
            <td>“pitch”</td>
            <td>“roll”</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>IMUs come in all shapes and sizes. These days they’re
            very small but the original IMU’s ver really large, as
            evidenced by the one used in the <a
            href="http://klabs.org/history/history_docs/mit_docs/1690.pdf">Apollo
            space missions</a>:</p>
            <p><img src="img/sensors/imu_apollo.jpg" width="300"></p>
            <p><br></p>
            <ol start="2" type="1">
            <li><p><strong>accelerometer</strong>: is the primary sensor
            responsible for measuring inertial acceleration, or the
            change in velocity over time.</p></li>
            <li><p><strong>magnetometer</strong>: measures the strength
            and direction of magnetic field – to find the magnetic
            north</p></li>
            </ol>
            </section>
            <section
            id="bouncing-of-electromagnetic-waves-lidar-and-mmwave"
            class="level3" data-number="3.1.2">
            <h3 data-number="3.1.2"><span
            class="header-section-number">3.1.2</span> Bouncing of
            Electromagnetic Waves | LiDAR and mmWave</h3>
            <p>A very common principle for measuring surroundings is to
            bounce electromagnetic waves off nearby objects and
            measuring the round trip times. Shorter times indicate
            closer objects while longer times indicate objects that are
            farther away. <a
            href="https://www.noaa.gov/jetstream/doppler/how-radar-works">RADAR</a>
            is a classic example of this type of sensor and its (basic)
            operation is shown in the following image (courtesy
            NOAA):</p>
            <p><img src="img/sensors/radar_doppler_ani.gif" width="400"></p>
            <p>While many autonomous vehicles use RADAR, we will focus
            on other technologies that are more prevalent and provide
            much higher precision, <em>viz.,</em></p>
            <ol type="1">
            <li><a
            href="#light-detection-and-ranging-lidar">LiDAR</a></li>
            <li>millimeter Wave RADAR (mmWave)</li>
            </ol>
            <section id="light-detection-and-ranging-lidar"
            class="level4" data-number="3.1.2.1">
            <h4 data-number="3.1.2.1"><span
            class="header-section-number">3.1.2.1</span> Light Detection
            and Ranging (LiDAR)</h4>
            <p><a
            href="https://web.stanford.edu/class/ee259/lectures/ee259_05_lidar.pdf">LiDAR</a>
            is a sensor that uses (<em>eye safe</em>) <strong>laser
            beams</strong> for mapping surroundings and creating
            <strong>3D representation</strong> of the environment. So
            lasers are used for,</p>
            <ul>
            <li>imaging</li>
            <li>detection</li>
            <li>ranging</li>
            </ul>
            <p>We can use LiDAR to distance, angle as well as the
            <em>radial velocity</em> of some objects – all relative to
            the autonomous system (rather the sensor). So, in practice,
            this is how it operates:</p>
            <p><img src="img/sensors/lidar_principle_operation.png" width="400"></p>
            <p>We define a <strong>roundtrip time</strong>, $ au$, as
            the time between when a pulse is sent out from the
            transmitter (<code>TX</code>) to when light reflected from
            the object is detected at the receiver
            (<code>RX</code>).</p>
            <p>So, the <strong>target range</strong> (<em>i.e.,</em> the
            distance to te object), <span
            class="math inline"><em>R</em></span>, is measured as:</p>
            <p><span
            class="math display"><em>R</em> = <em>r</em><em>a</em><em>c</em><em>c</em><em>a</em><em>u</em>2</span></p>
            <p>where, <code>c</code> is the speed of light.</p>
            <p>More details (from <a
            href="https://web.stanford.edu/class/ee259/lectures/ee259_05_lidar.pdf">Mahalati</a>):
            &gt; Lasers used in lidars have frequencies in the <span
            class="math inline">100<em>s</em></span> of Terahetrz.
            Compared to RF waves, lasers have significantly smaller
            wavelengths and can hence be easily collected into narrow
            beams using lenses. This makes DOA estimation almost trivial
            in lidar and gives it significantly better reso- lution than
            MIMO imaging radar.</p>
            <p>The <em>end product</em> of LiDAR is essentially a
            <strong>point cloud</strong>, defined as:</p>
            <blockquote>
            <p>a collection of points generated by a sensor. Such
            collections can be very dense and contain billions of
            points, which enables the creation of highly detailed 3D
            representations of an area.</p>
            </blockquote>
            <p><img src="img/sensors/lidar_point_cloud_torus.gif" title="3D point cloud of a Torus. Courtesy Wikipedia"></p>
            <p>In reality, point cloud representations around autonomous
            vehicles end up looking like:</p>
            <video controls width="500">
            <source src="https://sibin.github.io/teaching/csci6907_88-gwu-secure_autonomous/fall_2022/other_docs/What-is-Lidar-video.mp4">
            </video>
            <p><a
            href="https://www.yellowscan.com/knowledge/lidar-point-cloud-basics/">Point
            clouds</a> provide valuable information, <em>viz.,</em></p>
            <ul>
            <li>3D coordinates, <span
            class="math inline">(<em>x</em>, <em>y</em>, <em>z</em>)</span></li>
            <li><strong>strength</strong> of returned signal → provides
            valuable information about the <strong>density</strong> of
            the object (or even material composition)!</li>
            <li>additional attributes: return number, scan angle, scan
            direction, point density, RGB color values, and time stamps
            → each can be used for refining the scan.</li>
            </ul>
            <p>There are <strong>two types</strong> of <em>scene
            illumination</em> techniques for LiDAR:</p>
            <table>
            <colgroup>
            <col style="width: 20%" />
            <col style="width: 52%" />
            <col style="width: 27%" />
            </colgroup>
            <thead>
            <tr>
            <th>type</th>
            <th>illumination method</th>
            <th>detector</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>flash lidar</td>
            <td><em>entire</em> scene using wide laser</td>
            <td>receives all echoes on a photodetector array</td>
            </tr>
            <tr>
            <td>scanning lidar</td>
            <td>very narrow laser beams, scan illumination spot with
            laser beam scanner</td>
            <td>single photodetector to sequentially estimate $ au$ for
            each spot</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <table>
            <colgroup>
            <col style="width: 28%" />
            <col style="width: 28%" />
            <col style="width: 42%" />
            </colgroup>
            <thead>
            <tr>
            <th></th>
            <th>flash lidar</th>
            <th>scan lidar</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>architecture</strong></td>
            <td><img src="img/sensors/lidar_flash.png" width="400"></td>
            <td><img src="img/sensors/lidar_scan.png" width="400"></td>
            </tr>
            <tr>
            <td><strong>resolution</strong> determined by</td>
            <td>photodetector array pizel size (like camera)</td>
            <td>laser beam size and spot fixing</td>
            </tr>
            <tr>
            <td><strong>frame rates</strong></td>
            <td>higher (up to <code>100 fps</code>)</td>
            <td>lower (&lt; <code>30 fps</code>)</td>
            </tr>
            <tr>
            <td><strong>range</strong></td>
            <td>shorter (quick beam divergence, like photography)</td>
            <td>longer (<code>100m+</code>)</td>
            </tr>
            <tr>
            <td><strong>use</strong></td>
            <td>less common</td>
            <td><strong>most common</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Now, consider the following scene (captured by a
            camera):</p>
            <p><img src="img/sensors/lidar_camera_image.png" width="400"></p>
            <p><br> <br></p>
            <p>Compare this to the LiDAR images captured by the two
            methods:</p>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 30%" />
            <col style="width: 38%" />
            </colgroup>
            <thead>
            <tr>
            <th>flash lidar</th>
            <th>scan lidar (16 scan lines)</th>
            <th>scan lidar (32 scan lines)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/sensors/lidar_flash_image.png" width="400"></td>
            <td><img src="img/sensors/lidar_scan_16.png" width="400"></td>
            <td><img src="img/sensors/lidar_scan_32.png" width="400"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <blockquote>
            <p>A “LiDAR scan line” refers to a <strong>single horizontal
            line</strong> of laser pulses emitted by a LiDAR sensor,
            essentially capturing a cross-section of the environment at
            a specific angle as the sensor rotates, creating a 3D point
            cloud by combining multiple scan lines across the field of
            view; it’s the basic building block of a LiDAR scan, similar
            to how a single horizontal line is a building block of an
            image.</p>
            </blockquote>
            <p><strong>Potential Problems</strong>:</p>
            <p>Atmospheric/environmental conditions can
            <strong>negatively</strong> affect the quality of the data
            captured by the LiDAR. For instance, <strong>fog</strong>
            can scatter the laser photons resulting in <strong>false
            positives</strong>.</p>
            <p><img src="img/sensors/lidar_fog.png" width="400"></p>
            <p>As we see from the above image, the scattering due to the
            fog results in the system “identifying” multiple objects
            even though there is only <em>one</em> person in the
            scene.</p>
            <p>Here are additional examples from the <a
            href="https://www.mapix.com/lidar-scanner-sensors/velodyne/velodyne-vlp-32c/">Velodyne
            VLP-32C</a> sensor:</p>
            <ol type="1">
            <li><strong>light</strong> fog (camera vs LiDAR)</li>
            </ol>
            <p><img src="img/sensors/lidar_veoldyne_lightfog.png" width="600"></p>
            <p>The LiDAR does a good job isolating the main subject with
            very few false positives.</p>
            <ol start="2" type="1">
            <li><strong>heavy</strong> fog (camera vs LiDAR)</li>
            </ol>
            <p><img src="img/sensors/lidar_velodyne_heavyfog.png" width="600"></p>
            <p>The LiDAR <em>struggles</em> to isolate the main subject
            with very <em>high</em> false positives.</p>
            <p>In spite of these issues, LiDAR is one of the most
            popular sensors used in autonomous vehicles. They’re getting
            smaller and more precise by the day; also decreasing costs
            means that we will see a proliferation of these types of
            sensors in many autonomous systems.</p>
            <p>For an in-depth study on LiDARs, check this out: <a
            href="https://web.stanford.edu/class/ee259/lectures/ee259_05_lidar.pdf">Stanford
            EE 259 LiDAR Lecture</a>.</p>
            </section>
            <section id="millimeter-wave-radar-mmwave" class="level4"
            data-number="3.1.2.2">
            <h4 data-number="3.1.2.2"><span
            class="header-section-number">3.1.2.2</span> Millimeter Wave
            Radar [mmWave]</h4>
            <p>Short wavelengths like the *millimeter wave<strong>
            (</strong>mmWave**) in the electromagnetic spectrum allows
            for:</p>
            <ul>
            <li>smaller antennae</li>
            <li>integration of entire RADAR circuitry in a single
            chip!</li>
            <li>spectrum of 10 millimeters (<code>30 GHz</code>) to 1
            millimeter (<code>300 GHz</code>)</li>
            </ul>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="img/sensors/mmwave.jpg" width="300"></td>
            <td><img src="img/sensors/mmwave_ucsdavif.avif" width="300"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>As we see from the above images, the sensors can be
            <strong>very small</strong>, yet <strong>very
            precise</strong> → some can detect movements up to <em>4
            millionths of a meter</em>!</p>
            <p><strong>Advantages</strong> of mmWave:</p>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <thead>
            <tr>
            <th>Advantage</th>
            <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>small antenna caliber</td>
            <td>narrow beam gives high tracking, accuracy; high-level
            resolution, high-resistance interference performance of
            narrow beam; high antenna gain; smaller object
            detection</td>
            </tr>
            <tr>
            <td>large bandwidth</td>
            <td>high information rate, details structural features of
            the target; reduces multipath, and enhances
            anti-interference ability; overcomes mutual interference;
            high-distance resolution</td>
            </tr>
            <tr>
            <td>high doppler frequency</td>
            <td>good detection and recognition ability of slow
            objectives and vibration targets; can work in snow
            conditions</td>
            </tr>
            <tr>
            <td>good anti-blanking performance</td>
            <td>works on the most used stealth material</td>
            </tr>
            <tr>
            <td>robustness to atmospheric conditions</td>
            <td>such as dust, smoke, and fog compared to other
            sensors</td>
            </tr>
            <tr>
            <td>operation under different lights</td>
            <td>radar can operate under bright lights, dazzling lights,
            or no lights</td>
            </tr>
            <tr>
            <td>insusceptible to ground clutter</td>
            <td>allowing for close-range observations; the low
            reflectivity can be measured using mmwave radar</td>
            </tr>
            <tr>
            <td>fine spatial resolution</td>
            <td>for the same range, mmwave radar offers finer spatial
            resolution than microwave radar &gt;</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>mmWave is also used for <strong>in-cabin monitoring of
            drivers</strong>!</p>
            <p><br></p>
            <p><strong>Limitations</strong>:</p>
            <ul>
            <li>line of sight operations</li>
            <li>affected by water content, gases in environments</li>
            <li>affected by contaminated environment and physical
            obstacles</li>
            </ul>
            <p><br></p>
            <p><strong>Resources</strong>:</p>
            <p>For a more detailed description of mmWave RADAR, read: <a
            href="https://www.design-reuse.com/articles/55851/mmwave-radar-principle-applications.html">Understanding
            mmWave RADAR, its Principle &amp; Applications</a></p>
            <p>For programming a LiDAR, see: <a
            href="h.ttps://www.engineersgarage.com/how-to-use-a-lidar-sensor-with-arduino/">how
            to program a LiDAR with an Arduino</a>.</p>
            </section>
            </section>
            <section id="ultrasonic" class="level3" data-number="3.1.3">
            <h3 data-number="3.1.3"><span
            class="header-section-number">3.1.3</span> Ultrasonic</h3>
            <p>Much like lidars, we can use reflected sounds waves to
            detect objects. They work by emitting high-frequency sound
            waves, typically above human hearing, and then listening for
            the echoes that bounce back from nearby objects. The sensor
            calculates the distance based on the time it takes for the
            echo to return, using the speed of sound. Popular modules
            like the HC-SR04 (Used in Lab#2) are easy to integrate with
            microcontrollers such as Arduino and Raspberry Pi. These
            sensors are widely used in robotics for obstacle avoidance,
            automated navigation, and liquid level sensing.</p>
            <p>However, unlike optical (electromagnetic waves)
            detectors, ultrasonic sensors, while useful for basic
            distance measurements, cannot replicate the functionalities
            of LiDAR systems due to several key limitations. Unlike
            LiDAR, which employs laser beams to generate
            high-resolution, three-dimensional point clouds, ultrasonic
            sensors emit sound waves that provide only limited,
            single-point distance data with lower precision. LiDAR
            offers greater accuracy and longer range, enabling detailed
            mapping and object recognition essential for applications
            like autonomous vehicles and advanced robotics.
            Additionally, LiDAR systems can cover a wider field of view
            and operate effectively in diverse environments by rapidly
            scanning multiple directions, whereas ultrasonic sensors
            typically have a narrow detection cone and struggle with
            complex or cluttered scenes. Furthermore, LiDAR’s ability to
            capture data at high speeds allows for real-time processing
            and dynamic obstacle detection, which ultrasonics cannot
            match. This is because comparitively, it sounds waves take a
            lot of time to return since they’re much slower in speed
            compared to light waves (360m/s vs 299,792,458m/s). These
            differences in data richness, accuracy, and versatility make
            ultrasonic sensors unsuitable substitutes for the
            sophisticated capabilities offered by LiDAR technology.</p>
            <p>We’ll be using ultrasonic distance finders in futures MPs
            to stop our rovers from colliding into objects. Since our
            rovers don’t moove to fast and complexity is relatively low,
            only a ultrasonic sensor would suffice.</p>
            </section>
            </section>
            <section id="errors-in-sensing" class="level2"
            data-number="3.2">
            <h2 data-number="3.2"><span
            class="header-section-number">3.2</span> Errors in
            Sensing</h2>
            <p>Since sensors deal with and measure the <em>physical</em>
            world, <strong>errors</strong> will creep in over time.</p>
            <p>Some typical errors in the use of physical sensors:</p>
            <table>
            <colgroup>
            <col style="width: 55%" />
            <col style="width: 44%" />
            </colgroup>
            <thead>
            <tr>
            <th>error type</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>sensor drift</strong></td>
            <td>over time the sensor measurements will “drift”, i.e., a
            gradual change in its output → away from average values
            (e.g., due to wear and tear)</td>
            </tr>
            <tr>
            <td><strong>constant bias</strong></td>
            <td>bias of an accelerometer is the offset of its output
            signal from the actual acceleration value. A constant bias
            error causes an error in position which grows with time</td>
            </tr>
            <tr>
            <td><strong>calibration errors</strong></td>
            <td>‘calibration errors’ refers to errors in the scale
            factors, alignments and linearities of the gyros. Such
            errors tend to produce errors when the device is turning.
            These errors can result in additional drift</td>
            </tr>
            <tr>
            <td><strong>scale factor</strong></td>
            <td>scale factor is the relation of the accelerometer input
            to the actual sensor output for the measurement. Scale
            factor, expressed in ppm, is therefore the linear growth of
            input variation to actual measurement</td>
            </tr>
            <tr>
            <td><strong>vibration rectification errors</strong></td>
            <td>vibration rectification error (VRE) is the response of
            an accelerometer to current rectification in the sensor,
            causing a shift in the offset of the accelerometer. This can
            be a significant cumulative error, which propagates with
            time and can lead to over compensation in stabilization</td>
            </tr>
            <tr>
            <td><strong>noise</strong></td>
            <td>random variations in the sensor output that do not
            correspond to the actual measured value</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Each error type must be dealt with in different ways
            though one of the commomn ways to prevent sensor errors from
            causing harm to autonomous systems → <strong>sensor
            fusion</strong>, <em>i.e.,</em> use information from
            <strong>multiple sensors</strong> before making any
            decisions. We will dicuss sensor fusion later in this
            course.</p>
            </section>
            <section id="analog-to-digital-convertors-adcs"
            class="level2" data-number="3.3">
            <h2 data-number="3.3"><span
            class="header-section-number">3.3</span> Analog to Digital
            Convertors (ADCs)</h2>
            <p>As <a href="#sensors-and-sensing">mentioned earlier</a>,
            a sensor maps a physical quantity from the time domain to
            the value domain,</p>
            <p><span
            class="math display"><em>s</em> : <em>D</em><sub><em>t</em></sub> ↦ <em>D</em><sub><em>v</em></sub></span></p>
            <p>where,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>D</em><sub><em>t</em></sub></span></td>
            <td>continuous or discrete <strong>time</strong> domain</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>D</em><sub><em>v</em></sub></span></td>
            <td>continuous or discrete <strong>value</strong>
            domain</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Remember that computers require <strong>discrete</strong>
            sequences of physical values since <strong>microcontrollers
            cannot read values unless it is digital data</strong>.
            Microcontrollers can only see “levels” of voltage, which
            depends on the resolution of the ADC and the system
            voltage.</p>
            <p>Hence, we need to <strong>convert</strong> the above into
            the discrete domain, <em>i.e.,</em> we require <span
            class="math inline"><em>D</em><sub><em>v</em></sub></span>
            to be composed of discrete values.</p>
            <p>According to <a
            href="https://en.wikipedia.org/wiki/Discrete_time_and_continuous_time#">Wikipedia</a>,</p>
            <blockquote>
            <p>A discrete signal or discrete-time signal is a time
            series consisting of a sequence of quantities. Unlike a
            continuous-time signal, a discrete-time signal is not a
            function of a continuous argument; however, it may have been
            obtained by sampling from a continuous-time signal. When a
            discrete-time signal is obtained by sampling a sequence at
            uniformly spaced times, it has an associated
            <strong>sampling rate</strong>.</p>
            </blockquote>
            <p><br></p>
            <p>A visual respresentation of the sampling rate and how it
            correlates to the sampling of an analog signal:</p>
            <table>
            <colgroup>
            <col style="width: 38%" />
            <col style="width: 38%" />
            <col style="width: 23%" />
            </colgroup>
            <thead>
            <tr>
            <th>analog signal</th>
            <th>sampling rate</th>
            <th>sampling</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/sensors/adc_analog_signal.png"></td>
            <td><img src="img/sensors/adc_sampling_rate.png"></td>
            <td><img src="img/sensors/adc_sampling.png"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Hence, a device that converts analog signals to digital
            data values is called → an <strong>analog-to-digital
            convertor</strong> (<strong>ADC</strong>). This is one of
            the most common circuits/microcontrollers in embedded (and
            hence, autonomous) systems. <em>Any</em> sensor that
            measures a physical property must pass its values through an
            ADC so that the sensor values can be used by the system (the
            embedded processor/microcontroller, really).</p>
            <p>This is best described using an example:</p>
            <p><img src="img/sensors/adc_example.jpg" width="400"></p>
            <p>The <font color="blue"><b>analog</b></font> signal is
            <strong>discretized</strong> into the
            <font color="red"><b>digital</b></font> signal after passing
            through an ADC.</p>
            <p>ADCs follow a sequence:</p>
            <ul>
            <li><strong>sample</strong> the signal</li>
            <li><strong>quantify</strong> it to determine the resolution
            of the signal</li>
            <li>set <strong>binary values</strong></li>
            <li><strong>send it to the system</strong> to read the
            digital signal</li>
            </ul>
            <p>Hence, two important aspects of an ADC are:</p>
            <ul>
            <li><a href="#adc-sampling-rate">sampling rate</a></li>
            <li><a href="#adc-resolution">resolution</a></li>
            </ul>
            <section id="adc-sampling-rate" class="level3"
            data-number="3.3.1">
            <h3 data-number="3.3.1"><span
            class="header-section-number">3.3.1</span> ADC Sampling
            Rate</h3>
            <p>The sampling rate (aka Sampling Frequency) is measured in
            <strong>samples per second</strong> (SPS or S/s). It
            dictates <em>how many samples</em> (data points) are taken
            in one second. If an ADC records more samples, then it can
            handle higher frequencies.</p>
            <p>The sample rate, <span
            class="math inline"><em>f</em><sub><em>s</em></sub></span>
            is defined as,</p>
            <p><span
            class="math display"><em>f</em><sub><em>s</em></sub> = <em>r</em><em>a</em><em>c</em>1<em>T</em></span></p>
            <p>where,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>definition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>f</em><sub><em>s</em></sub></span></td>
            <td>sampling rate/frequency</td>
            </tr>
            <tr>
            <td><span class="math inline"><em>T</em></span></td>
            <td>period of the sample</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Hence, in the previous example,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>value</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>f</em><sub><em>s</em></sub></span></td>
            <td><code>20 Hz</code></td>
            </tr>
            <tr>
            <td><span class="math inline"><em>T</em></span></td>
            <td><code>50 ms</code></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>While this looks slow (<code>20 Hz</code>), the digital
            signal tracks the original analog signal quite faithfully →
            the original signal itself is quite slow
            (<code>1 Hz</code>).</p>
            <p>Now, if the sampling signal is <em>considerably
            slower</em> than the analog signal, then it loses fidelity
            and we see <strong>aliasing</strong>, where the
            reconstructed signal (the digital one in the case)
            <strong>differs from the original</strong>. Consider the
            following example of such a case:</p>
            <p><img src="img/sensors/adc_aliasing_example.jpg" width="400"></p>
            <p>As we see from the above figure, the digital output is
            <strong>nothing</strong> like the original. Hence, this
            (digital) output will not be of much use to the system.</p>
            <p><br></p>
            <p><a
            href="https://fab.cba.mit.edu/classes/S62.12/docs/Shannon_noise.pdf"><strong>Nyquist-Shannon
            Sampling Theorem</strong></a>:</p>
            <blockquote>
            <p>to accurately reconstruct a signal from its samples, the
            sampling rate must be <strong>at least twice the highest
            frequency component</strong> present in the signal</p>
            </blockquote>
            <p>If the sampling frequency is less than the Nyquist rate,
            then aliasing starts to creep in.</p>
            <p>Hence,</p>
            <p><span
            class="math display"><em>f</em><sub><em>N</em><em>y</em><em>q</em><em>u</em><em>i</em><em>s</em><em>t</em></sub> = 2 * <em>f</em><sub><em>m</em><em>a</em><em>x</em></sub></span></p>
            <p>where,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>definition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>f</em><sub><em>N</em><em>y</em><em>q</em><em>u</em><em>i</em><em>s</em><em>t</em></sub></span></td>
            <td>Nyquist sampling rate/frequency</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>f</em><sub><em>m</em><em>a</em><em>x</em></sub></span></td>
            <td>the maximum frequency that appears in the signal</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>For instance, if your analog signal has a maximum
            frequency of <code>50 Hz</code> then your sampling frequency
            must be <em>at least</em>, <code>100 Hz</code>. If this
            principle is followed, then it is possible to
            <strong>accurately reconstruct</strong> the original signal
            and its values.</p>
            <p>Note that sometimes <em>noise</em> can introduce
            additonal (high) frequencies into the system but we don’t
            want to sample those (for obvious purposes). Hence, it is a
            good idea to add <a
            href="https://www.analog.com/en/resources/technical-articles/guide-to-antialiasing-filter-basics.html">anti-aliasing
            fitlers</a> to the analog signal <em>before</em> it is
            passed to the ADC.</p>
            </section>
            <section id="adc-resolution" class="level3"
            data-number="3.3.2">
            <h3 data-number="3.3.2"><span
            class="header-section-number">3.3.2</span> ADC
            Resolution</h3>
            <p>An ADC’s resolution is directly related to the
            <strong>precision</strong> of the ADC, determined by its
            <strong>bit length</strong>. The following examples shows
            the fidelity of the reconstruction, based on various bit
            lengths:</p>
            <p><img src="img/sensors/adc_resolution_example.jpg" width="400"></p>
            <p>Increasing bit lengths the digital signal more closely
            represents the analog one.</p>
            <p>There exists a correlation between the bit length and the
            <strong>voltage</strong> of the signal. Hence, the
            <strong>true resolution</strong> of the ADC is calculated
            using the bit length <strong>and</strong> the voltage as
            follows:</p>
            <p><span
            class="math display"><em>S</em><em>t</em><em>e</em><em>p</em><em>S</em><em>i</em><em>z</em><em>e</em> = <em>r</em><em>a</em><em>c</em><em>V</em><sub><em>r</em><em>e</em><em>f</em></sub><em>N</em></span></p>
            <p>where,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>definition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>S</em><em>t</em><em>e</em><em>p</em><em>S</em><em>i</em><em>z</em><em>e</em></span></td>
            <td>resolution of each level in terms of voltage</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>V</em><sub><em>r</em><em>e</em><em>f</em></sub></span></td>
            <td>voltage reference/range of voltages</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>N</em> = 2<sup><em>n</em></sup></span></td>
            <td>total “size” of the ADC</td>
            </tr>
            <tr>
            <td><span class="math inline"><em>n</em></span></td>
            <td>bit size</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>This is easier to understand with a concrete example:</p>
            <blockquote>
            <p>consider a sine wave with a voltage, <code>5 V</code>
            that must be digitized. <br> <br> If our ADC precision is
            <code>12 bits</code>, then we get <br> <span
            class="math inline"><em>N</em> = 2<sup>12</sup> = 4096</span>
            <br> <br> Hence, <span
            class="math inline"><em>S</em><em>t</em><em>e</em><em>p</em><em>S</em><em>i</em><em>z</em><em>e</em> = 5<em>V</em>/ 4096</span>
            which is <code>0.00122V</code> (or <code>1.22mV</code>)<br>
            <br> Hence, the system can tell when a voltage level changes
            by <code>1.22 mV</code>!</p>
            </blockquote>
            <p>(Repeat the exercise for say, bit length, <span
            class="math inline"><em>n</em> = 4</span>)</p>
            <p><br></p>
            <p><strong>Visual Example:</strong></p>
            <p>The above maybe intuitively understood as follows:</p>
            <p>Consider the following signal:</p>
            <p><img src="img/sensors/adc_bits/adc_bits.1.png" width="300"></p>
            <p>Now, if we want to sample this signal, we can obtain
            measurements at:</p>
            <p><img src="img/sensors/adc_bits/adc_bits.2.png" width="300"></p>
            <p><br></p>
            <p>The figure shows <code>9</code> measurements.</p>
            <p>Suppose, the ADC registers have a width of:
            <code>2 bits</code>. Hence it can store at most:
            <code>4 values</code>.</p>
            <p>Since is is <strong>not</strong> possible to store
            <code>9</code> values → <code>2</code> bits, we must select
            <strong>only <code>4</code> values</strong> omn the digital
            side.</p>
            <p>We then get the following representation:</p>
            <p><img src="img/sensors/adc_bits/adc_bits.3.png" width="300"></p>
            <p><br></p>
            <p>which, to be honest, is not really a good representation
            of the original signal!</p>
            <p>Now, consider the case where the ADC registers have a bit
            width: <strong><code>4 bits</code></strong> →
            <code>16 values</code>! Hence, we can easily store
            <strong>all <code>9 values</code></strong> easily.</p>
            <p>So, we can get a digital representation as follows:</p>
            <p><img src="img/sensors/adc_bits/adc_bits.4.png" width="300"></p>
            <p><br></p>
            <p>We see that this is a better representation, <em>but
            still not exact</em>. We can increase the bit length but at
            this point we are limited by the sampling as well. Since we
            only have <code>9</code> samples, adding more bits won’t
            help.</p>
            <p>Hence, to get a better fidelity representation of the
            original signal, we see that <strong>sampling
            frequency</strong> and <strong>resolution</strong> need to
            be increased, since they determine the quality of output we
            get from an ADC.</p>
            <p><strong>Resources</strong></p>
            <ul>
            <li>for more details about ADC, read: <a
            href="https://www.arrow.com/en/research-and-events/articles/engineering-resource-basics-of-analog-to-digital-converters">Analog-to-Digital
            Convertor Basics</a></li>
            <li>an <strong>in-depth</strong> explanation of how ADCs
            work: <a
            href="http://class.ece.iastate.edu/cpre288/lectures/lect12_13.pdf">Iowa
            State CpreE 288 Course Slides</a></li>
            <li>more details with videos: <a
            href="https://users.ece.utexas.edu/~valvano/Volume1/E-Book/C14_ADCdataAcquisition.htm">Analog
            to Digital Conversion, EE319K Univ. of Texas</a></li>
            <li>Programming an ADC: <a
            href="https://blog.embeddedexpert.io/?p=68">1</a>, <a
            href="https://labs.dese.iisc.ac.in/embeddedlab/tm4c123-adc-programming/">2</a>
            <!--link rel="stylesheet" href="./custom.sibin.css"--></li>
            </ul>
            </section>
            </section>
            </section>
            <section id="real-time-operating-systems" class="level1"
            data-number="4">
            <h1 data-number="4"><span
            class="header-section-number">4</span> Real-Time Operating
            Systems</h1>
            <p>Real-Time Operating Systems (RTOS) are specialized
            operating systems designed to manage hardware resources,
            execute applications and process data in a
            <strong>predictable</strong> manner. The main aim of this
            focus on “predictability” is to ensure that critical tasks
            complete in a <strong>timely</strong> fashion. Unlike
            general-purpose operating systems (GPOS) like Windows or
            Linux, which prioritize multitasking and user experience,
            RTOS focuses on meeting strict timing constraints, ensuring
            that tasks are completed within defined
            <strong>deadlines</strong>. This makes RTOS essential for
            systems where timing accuracy and reliability are critical,
            such as in embedded systems, autonomous driving, industrial
            automation, automotive systems, medical devices and
            aerospace applications, among others.</p>
            <p>Hence, real-time systems (RTS), and RTOSes in general,
            have <em>two</em> criteria for “correctness”:</p>
            <table>
            <colgroup>
            <col style="width: 23%" />
            <col style="width: 76%" />
            </colgroup>
            <thead>
            <tr>
            <th>criteria</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>functional</strong> correctness</td>
            <td>the system should work as expected, <em>i.e.</em>, carry
            out its intended function without errors</td>
            </tr>
            <tr>
            <td><strong>temporal</strong> correctness</td>
            <td>the functionally correct operations must be completed
            within a predefined timing constraint
            (<strong>deadline</strong>)</td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>To place ourselves in the context of this course, this is
            where we are:</p>
            <p><img src="img/stack_architecture/stack_overview.4.png" width="300"></p>
            <p><br></p>
            <p>We haven’t looked at the actuation part but we will come
            back to it later.</p>
            <section id="key-characteristics-for-rtos" class="level3"
            data-number="4.0.1">
            <h3 data-number="4.0.1"><span
            class="header-section-number">4.0.1</span> Key
            characteristics for RTOS</h3>
            <table>
            <colgroup>
            <col style="width: 55%" />
            <col style="width: 44%" />
            </colgroup>
            <thead>
            <tr>
            <th>characteristic</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>determinism</strong></td>
            <td>primary feature of an RTOS is its ability to perform
            tasks within guaranteed time frames; this predictability
            ensures that high-priority tasks are executed without delay,
            even under varying system loads</td>
            </tr>
            <tr>
            <td><strong>task scheduling</strong></td>
            <td>RTOS uses advanced scheduling algorithms (e.g.,
            priority-based, round-robin or earliest-deadline-first) to
            manage task execution; RT tasks are often assigned
            priorities and the scheduler ensures that higher-priority
            tasks preempt lower-priority ones when necessary</td>
            </tr>
            <tr>
            <td><strong>low latency</strong></td>
            <td>RTOS minimizes interrupt response times and
            context-switching overhead, enabling rapid task execution
            and efficient handling of time-sensitive operations
            (<em>e.g.</em>, Linux spends <strong>many
            milliseconds</strong> handling interrupts such as disk
            access!)</td>
            </tr>
            <tr>
            <td><strong>resource management</strong></td>
            <td>RTOS provides mechanisms for efficient allocation and
            management of system resources, such as memory, CPU and
            peripherals, to ensure optimal performance</td>
            </tr>
            <tr>
            <td><strong>scalability</strong></td>
            <td>RTOS is often lightweight and modular, making it
            suitable for resource-constrained environments like
            microcontrollers and embedded systems</td>
            </tr>
            <tr>
            <td><strong>reliability and fault tolerance</strong></td>
            <td>many RTOS implementations include features to enhance
            system stability, such as error detection, recovery
            mechanisms and redundancy</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            <section id="kernels-in-rtos" class="level2"
            data-number="4.1">
            <h2 data-number="4.1"><span
            class="header-section-number">4.1</span> Kernels in
            RTOS</h2>
            <p>As with most operating systems, the kernel provides the
            essential services in an RTOS. In hard real-time systems,
            the kernel must guarantee predictable and deterministic
            behavior to ensure that all tasks meet their deadlines. In
            this chapter we focus on kernel aspects that are
            <em>specific to RTS</em>.</p>
            <p>The RTOS kernel deals with,</p>
            <ol type="1">
            <li><a href="#tasks-jobs-threads">task management</a></li>
            <li><a
            href="#inter-task-communication-and-synchronization">communication
            and synchronization</a></li>
            <li><a href="#memory-management">memory management</a></li>
            <li><a href="#timer-and-interrupt-management">timer and
            interrupt handling</a></li>
            <li><a href="#kernel-performance-metrics">performance
            metrics</a></li>
            </ol>
            <section id="tasks-jobs-threads" class="level3"
            data-number="4.1.1">
            <h3 data-number="4.1.1"><span
            class="header-section-number">4.1.1</span> Tasks, Jobs,
            Threads</h3>
            <p>The design of RTOSes (and RTS in general) deal with
            <strong>tasks</strong>, <strong>jobs</strong> and, for
            implementation-specific details,
            <strong>threads</strong>.</p>
            <p>A real-time <strong>task</strong>, <span
            class="math inline"><em>τ</em><sub><em>i</em></sub></span>
            is defined using the following parameters: <span
            class="math inline">(<em>ϕ</em><sub><em>i</em></sub>, <em>p</em><sub><em>i</em></sub>, <em>c</em><sub><em>i</em></sub>, <em>d</em><sub><em>i</em></sub>)</span>
            where,</p>
            <table>
            <thead>
            <tr>
            <th>Symbol</th>
            <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>ϕ</em><sub><em>i</em></sub></span></td>
            <td>Phase (offset for the first job of a task)</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>p</em><sub><em>i</em></sub></span></td>
            <td>Period</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>c</em><sub><em>i</em></sub></span></td>
            <td>Worst-case execution time</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>d</em><sub><em>i</em></sub></span></td>
            <td>Deadline</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Hence, a real-time tast <em>set</em> (of size
            ‘<em>n</em>’) is collection of such tasks, <em>i.e.,</em>
            <span
            class="math inline"><em>τ</em> = <em>τ</em><sub>1</sub>, <em>τ</em><sub>2</sub>, ...<em>τ</em><sub><em>n</em></sub></span>.
            Given a real-time task set, the <em>first</em> step is to
            check if the task set is <strong>schedulable</strong>,
            <em>i.e.,</em> check whether all <strong>jobs</strong> of a
            task will meet their deadlines (a <strong>job</strong> is an
            <strong>instance</strong> of a task). For this purpose,
            multiple <strong>schedulability tests</strong> have been
            developed, each depending on the scheduling algorithm being
            used.</p>
            <blockquote>
            <ul>
            <li>remember that task is a set of parameters.</li>
            <li>We “release” multiple “<em>jobs</em>” of each task, each
            with its own deadline</li>
            <li>if all jobs of all tasks meet their deadlines, then the
            system remains <em>safe</em>.</li>
            </ul>
            </blockquote>
            <p>A <strong>thread</strong>, then, is an
            <strong>implementation</strong> of task/job – depending on
            the actual OS, it could be either, or both.</p>
            <p>At a high level, here is a comparison between tasks, jobs
            and threads (<strong>note:</strong> these details may vary
            depending on the <em>specific</em> RTOS):</p>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 20%" />
            <col style="width: 18%" />
            <col style="width: 30%" />
            </colgroup>
            <thead>
            <tr>
            <th><strong>aspect</strong></th>
            <th><strong>task</strong></th>
            <th><strong>job</strong></th>
            <th><strong>thread</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>definition</strong></td>
            <td>a task is a <strong>unit of work</strong> that
            represents a program or function executing in the RTOS</td>
            <td>a job is a <strong>specific instance</strong> or
            execution of a task, often tied to a particular event or
            trigger</td>
            <td>a thread is the <strong>smallest unit of
            execution</strong> within a task, sharing the task’s
            resources</td>
            </tr>
            <tr>
            <td><strong>granularity</strong></td>
            <td>coarse-grained; represents a complete function or
            program</td>
            <td>fine-grained; represents a single execution of a
            task</td>
            <td>fine-grained; represents a single flow of execution
            within a task</td>
            </tr>
            <tr>
            <td><strong>resource ownership</strong></td>
            <td>owns its resources (e.g., stack, memory, state)</td>
            <td>does not own resources; relies on the task’s
            resources</td>
            <td>shares resources (e.g., memory, address space) with
            other threads in the same task</td>
            </tr>
            <tr>
            <td><strong>scheduling</strong></td>
            <td>scheduled by the RTOS kernel based on priority or
            scheduling algorithm</td>
            <td>not directly scheduled; executed as part of a task’s
            execution</td>
            <td>scheduled by the RTOS kernel, often within the context
            of a task</td>
            </tr>
            <tr>
            <td><strong>concurrency</strong></td>
            <td>tasks run concurrently, managed by the RTOS
            scheduler</td>
            <td>jobs are sequential within a task but may overlap across
            tasks</td>
            <td>threads run concurrently, even within the same task</td>
            </tr>
            <tr>
            <td><strong>state management</strong></td>
            <td>maintains its own state (e.g., ready, running,
            blocked)</td>
            <td>state is transient and tied to the task’s execution</td>
            <td>maintains its own state but shares the task’s overall
            context</td>
            </tr>
            <tr>
            <td><strong>isolation</strong></td>
            <td>high isolation; tasks do not share memory or resources
            by default <strong>++</strong></td>
            <td>no isolation; jobs are part of a task’s execution</td>
            <td>low isolation; threads share memory and resources within
            a task</td>
            </tr>
            <tr>
            <td><strong>overhead</strong></td>
            <td>higher overhead due to separate stacks and contexts</td>
            <td>minimal overhead, as it relies on the task’s
            resources</td>
            <td>moderate overhead, as threads share resources but
            require context switching</td>
            </tr>
            <tr>
            <td><strong>use case</strong></td>
            <td>used to model independent functions or processes (e.g.,
            control loops)</td>
            <td>used to represent a single execution of a task (e.g.,
            processing a sensor reading)</td>
            <td>used to parallelize work within a task (e.g., handling
            multiple i/o operations)</td>
            </tr>
            <tr>
            <td><strong>example</strong></td>
            <td>a task for controlling a motor</td>
            <td>a job for processing a specific motor command</td>
            <td>a thread for reading sensor data while another thread
            logs the data</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>(<strong>++</strong> sometimes tasks <strong>do</strong>
            contend for resources, so we need to mitigate access to
            them, via locks, semaphores, etc. and then have to deal with
            thorny issues such as <strong>priority
            inversions</strong>)</p>
            <p>A task is often described using a <strong>task control
            block</strong> (TCB):</p>
            <p><img src="img/rtos/tcb_sequence_png/tcb_12.png"></p>
            <p>Tasks typically cycle through a set of states, for
            instance (taken from the <a
            href="https://www.freertos.org/Documentation/02-Kernel/02-Kernel-features/01-Tasks-and-co-routines/02-Task-states">FreeRTOS</a>
            real-time OS):</p>
            <p><img src="img/rtos/free_rtos/freertos_taskstate.gif" width="300"></p>
            <p><br></p>
            <p>While the <code>READY</code>, <code>RUNNING</code> and
            <code>BLOCKED</code> states are similar to those in
            general-purpose operating systems (GPOS), <em>periodic</em>
            RTOSes must introduce an additional state:
            <strong><code>IDLE</code></strong> or
            <strong><code>SUSPENDED</code></strong>:</p>
            <ul>
            <li>periodic task enters this state when it (rather one
            ‘job’) completes its execution → has to wait for the
            beginning of the next period</li>
            <li>to be awakened by the timer (<em>i.e.,</em> to launch
            the next instance/job), the task must notify the end of its
            cycle by executing a specific system call,
            <code>end cycle</code> → puts the job in the IDLE state and
            assigns the processor to another ready job</li>
            <li>at the right time, each periodic task in IDLE state →
            awakened by kernel and inserted in the ready queue</li>
            </ul>
            <p>This operation is carried out by a routine
            <strong>activated by a timer</strong> → verifies, at each
            tick, whether some task(job) has to be awakened.</p>
            <p>TCBs are usually managed in kernel
            <strong>queues</strong> (the implementation details may vary
            depending on the particular RTOS).</p>
            <p><strong>Context Switch Overheads</strong>:</p>
            <p>One of the main issues with multitasking and preepmtion
            is that of <strong>context switch overheads</strong>,
            <em>i.e.,</em> the time and resources required to switch
            from one task to another. For instance, consider this
            example of two tasks running on an ARM Cortex-M4:</p>
            <div class="sourceCode" id="cb5"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> Task1<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span><span class="op">(</span><span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Task 1 operations</span></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a>        LED_Toggle<span class="op">();</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a>        delay_ms<span class="op">(</span><span class="dv">100</span><span class="op">);</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <p>and</p>
            <div class="sourceCode" id="cb6"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> Task2<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span><span class="op">(</span><span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Task 2 operations</span></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a>        ReadSensor<span class="op">();</span></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a>        delay_ms<span class="op">(</span><span class="dv">200</span><span class="op">);</span></span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <p>When switching between Task1 and Task2, an RTOS might
            need to:</p>
            <ul>
            <li>save <code>16</code> general-purpose registers</li>
            <li>save the program counter and stack pointer</li>
            <li>update the memory protection unit settings</li>
            <li>load the new task’s context (program into memory,
            registers, cache, <em>etc.</em>)</li>
            </ul>
            <p>So, on the ARM Cortex-M4,</p>
            <table>
            <colgroup>
            <col style="width: 38%" />
            <col style="width: 61%" />
            </colgroup>
            <thead>
            <tr>
            <th>effect</th>
            <th>cost</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>basic context switch</td>
            <td><code>200-400</code> CPU cycles</td>
            </tr>
            <tr>
            <td>cache and pipeline effects, total overhead</td>
            <td><code>1000+</code> cycles</td>
            </tr>
            <tr>
            <td>frequent switching (e.g., every <code>1 ms</code>)</td>
            <td>could consume <code>1-2%</code> of CPU time!</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>These costs can add up, especially if the system has,</p>
            <ul>
            <li>many RT tasks and frequent
            <strong>preemption</strong></li>
            <li>high-frequency/short period jobs that execute
            frequently</li>
            <li>if tasks contend with each other for shared
            resources</li>
            </ul>
            <p>Hence and RTOS must not only be cognizant of such
            overheads but also <strong>actively manage/mitigate</strong>
            them. Some strategies could include:</p>
            <ol type="1">
            <li><strong>better task/schedule design</strong>:
            <em>e.g.,</em> group related operations to reduce context
            switches</li>
            </ol>
            <div class="sourceCode" id="cb7"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> Task_Sensors<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span><span class="op">(</span><span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Handle multiple sensors in one task</span></span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        ReadTemperature<span class="op">();</span></span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        ReadPressure<span class="op">();</span></span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        ReadHumidity<span class="op">();</span></span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        delay_ms<span class="op">(</span><span class="dv">500</span><span class="op">);</span></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <ol start="2" type="1">
            <li><strong>priority-based scheduling</strong>:
            <em>e.g.,</em> high priority task gets more CPU</li>
            </ol>
            <div class="sourceCode" id="cb8"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> CriticalTask<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Set high priority</span></span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>    setPriority<span class="op">(</span>HIGH_PRIORITY<span class="op">);</span></span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span><span class="op">(</span><span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>        ProcessCriticalData<span class="op">();</span></span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        delay_ms<span class="op">(</span><span class="dv">50</span><span class="op">);</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <ol start="3" type="1">
            <li><strong>optimizing memory layouts</strong>:
            <em>e.g.</em>, align task stacks to cache line
            boundaries</li>
            </ol>
            <div class="sourceCode" id="cb9"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#define STACK_SIZE </span><span class="dv">1024</span></span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a><span class="dt">static</span> __attribute__<span class="op">((</span>aligned<span class="op">(</span><span class="dv">32</span><span class="op">)))</span> </span>
<span id="cb9-3"><a href="#cb9-3" aria-hidden="true" tabindex="-1"></a><span class="dt">uint8_t</span> task1_stack<span class="op">[</span>STACK_SIZE<span class="op">];</span></span></code></pre></div>
            <p><strong>Note:</strong> these are not comprehensive and
            other strategies could be followed, for instance
            <strong>avoiding multitasking altogether</strong>! All
            functions could be implemented in a <strong>single</strong>
            process that runs a giant, infinite loop known as a <a
            href="https://my.eng.utah.edu/~cs5785/slides-f10/22-1up.pdf"><strong>cyclic
            executive</strong></a>. Newer RTOSes shun ths cyclic
            executive in favor of the multitasking model since the
            latter provides more flexibility, control and adaptability
            but many critical systems (especially older, long-running
            ones) still use the cyclic executive. For instance, nuclear
            reactors, chemical plants, <em>etc.</em></p>
            <p>In any case, a <strong>precise</strong> understanding of
            these overheads is crucial for:</p>
            <ul>
            <li>setting appropriate task priorities</li>
            <li>determining minimum task periods</li>
            <li>calculating worst-case execution times</li>
            <li>meeting real-time deadlines</li>
            <li>optimizing system performance</li>
            </ul>
            <p>There is significant (ongoing) work, both in industry as
            well as academia, on how to get a handle on context switch
            overheads while still allowing for flexibility and
            modularity in the development of RTS.</p>
            </section>
            <section id="inter-task-communication-and-synchronization"
            class="level3" data-number="4.1.2">
            <h3 data-number="4.1.2"><span
            class="header-section-number">4.1.2</span> (Inter-Task)
            Communication and Synchronization</h3>
            <p>RTOSes use various mechanisms like semaphores, mutexes,
            message queues and event flags for communication and
            synchronization between tasks. Here are some examples:</p>
            <ol type="1">
            <li><strong>Semaphores</strong>:</li>
            </ol>
            <ul>
            <li>binary semaphores: work like a mutex, with values 0 or
            1</li>
            <li>counting semaphores: can have multiple values, useful
            for managing resource pools</li>
            </ul>
            <div class="sourceCode" id="cb10"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="co">// Example of binary semaphore usage</span></span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>semaphore_t sem<span class="op">;</span></span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>sem_init<span class="op">(&amp;</span>sem<span class="op">,</span> <span class="dv">1</span><span class="op">);</span>  <span class="co">// Initialize with 1</span></span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> TaskA<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span><span class="op">(</span><span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a>        sem_wait<span class="op">(&amp;</span>sem<span class="op">);</span></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>        <span class="co">// Critical section</span></span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        accessSharedResource<span class="op">();</span></span>
<span id="cb10-10"><a href="#cb10-10" aria-hidden="true" tabindex="-1"></a>        sem_post<span class="op">(&amp;</span>sem<span class="op">);</span></span>
<span id="cb10-11"><a href="#cb10-11" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb10-12"><a href="#cb10-12" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <ol start="2" type="1">
            <li><strong>Mutexes</strong> (mutual exclusion):</li>
            </ol>
            <ul>
            <li>mutexes provide exclusive access to shared
            resources</li>
            <li>they include <strong>priority inheritance</strong> to
            prevent <strong>priority inversion</strong></li>
            </ul>
            <div class="sourceCode" id="cb11"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>mutex_t mutex<span class="op">;</span></span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>mutex_init<span class="op">(&amp;</span>mutex<span class="op">);</span></span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb11-4"><a href="#cb11-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> TaskB<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb11-5"><a href="#cb11-5" aria-hidden="true" tabindex="-1"></a>    mutex_lock<span class="op">(&amp;</span>mutex<span class="op">);</span></span>
<span id="cb11-6"><a href="#cb11-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Protected shared resource access</span></span>
<span id="cb11-7"><a href="#cb11-7" aria-hidden="true" tabindex="-1"></a>    updateSharedData<span class="op">();</span></span>
<span id="cb11-8"><a href="#cb11-8" aria-hidden="true" tabindex="-1"></a>    mutex_unlock<span class="op">(&amp;</span>mutex<span class="op">);</span></span>
<span id="cb11-9"><a href="#cb11-9" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <ol start="3" type="1">
            <li><strong>Message Queues</strong>:</li>
            </ol>
            <ul>
            <li>they allow <strong>ordered data transfer</strong>
            between tasks</li>
            <li>provide for buffering capabilities</li>
            </ul>
            <div class="sourceCode" id="cb12"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a>queue_t msgQueue<span class="op">;</span></span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>queue_create<span class="op">(&amp;</span>msgQueue<span class="op">,</span> MSG_SIZE<span class="op">,</span> MAX_MSGS<span class="op">);</span></span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> SenderTask<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a>    message_t msg <span class="op">=</span> prepareMessage<span class="op">();</span></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    queue_send<span class="op">(&amp;</span>msgQueue<span class="op">,</span> <span class="op">&amp;</span>msg<span class="op">,</span> TIMEOUT<span class="op">);</span></span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ReceiverTask<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>    message_t msg<span class="op">;</span></span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>    queue_receive<span class="op">(&amp;</span>msgQueue<span class="op">,</span> <span class="op">&amp;</span>msg<span class="op">,</span> TIMEOUT<span class="op">);</span></span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>    processMessage<span class="op">(&amp;</span>msg<span class="op">);</span></span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <ol start="4" type="1">
            <li><strong>Event Flags</strong>:</li>
            </ol>
            <ul>
            <li>enable <strong>multiple tasks</strong> to wait for one
            or more events</li>
            <li>support <code>AND</code>/<code>OR</code> conditions for
            event combinations</li>
            </ul>
            <div class="sourceCode" id="cb13"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>event_flags_t events<span class="op">;</span></span>
<span id="cb13-2"><a href="#cb13-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#define EVENT_SENSOR_DATA </span><span class="bn">0x01</span></span>
<span id="cb13-3"><a href="#cb13-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#define EVENT_USER_INPUT  </span><span class="bn">0x02</span></span>
<span id="cb13-4"><a href="#cb13-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb13-5"><a href="#cb13-5" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> TaskC<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb13-6"><a href="#cb13-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Wait for both events</span></span>
<span id="cb13-7"><a href="#cb13-7" aria-hidden="true" tabindex="-1"></a>    event_wait<span class="op">(&amp;</span>events<span class="op">,</span> EVENT_SENSOR_DATA <span class="op">|</span> EVENT_USER_INPUT<span class="op">,</span> </span>
<span id="cb13-8"><a href="#cb13-8" aria-hidden="true" tabindex="-1"></a>               EVENT_ALL<span class="op">,</span> TIMEOUT<span class="op">);</span></span>
<span id="cb13-9"><a href="#cb13-9" aria-hidden="true" tabindex="-1"></a>    processEvents<span class="op">();</span></span>
<span id="cb13-10"><a href="#cb13-10" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <ol start="5" type="1">
            <li><strong>Condition Variables</strong>:</li>
            </ol>
            <ul>
            <li>tasks can wait for <strong>specific
            conditions</strong></li>
            <li>used with mutexes for complex synchronization</li>
            </ul>
            <div class="sourceCode" id="cb14"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>mutex_t mutex<span class="op">;</span></span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>cond_t condition<span class="op">;</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> ConsumerTask<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>    mutex_lock<span class="op">(&amp;</span>mutex<span class="op">);</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span><span class="op">(</span>bufferEmpty<span class="op">())</span> <span class="op">{</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a>        cond_wait<span class="op">(&amp;</span>condition<span class="op">,</span> <span class="op">&amp;</span>mutex<span class="op">);</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>    processData<span class="op">();</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a>    mutex_unlock<span class="op">(&amp;</span>mutex<span class="op">);</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <p><br></p>
            <p>Each mechanism has specific use cases:</p>
            <table>
            <thead>
            <tr>
            <th>mechanism</th>
            <th>use case</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>semaphores</strong></td>
            <td>resource management and simple synchronization</td>
            </tr>
            <tr>
            <td><strong>mutexes</strong></td>
            <td>exclusive access to shared resources</td>
            </tr>
            <tr>
            <td><strong>message queues</strong></td>
            <td>data exchange and task communication</td>
            </tr>
            <tr>
            <td><strong>event flags</strong></td>
            <td>multiple event synchronization</td>
            </tr>
            <tr>
            <td><strong>condition variables</strong></td>
            <td>complex state-dependent synchronization</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Common considerations:</p>
            <ol type="1">
            <li>Priority Inversion Prevention: a high-priority (HP) task
            is <strong>indirectly preempted</strong> by a lower-priority
            (LP) task; HP → needs resource (R); R held by → LP, LP
            preempted by medium-priority (MP) task. So <strong>HP waits
            for MP</strong> → inversion of priorities! We will discuss
            solutions (priority inheritance/priority ceiling)
            later.</li>
            <li>Deadlock Avoidance: tasks are *permanently blocked**
            waiting on resources from each other; <span
            class="math inline"><em>τ</em><sub>1</sub></span> holds
            resource <span
            class="math inline"><em>R</em><sub><em>A</em></sub></span>
            and waits for <span
            class="math inline"><em>R</em><sub><em>B</em></sub></span>;
            <span class="math inline"><em>τ</em><sub>2</sub></span>
            holds resource <span
            class="math inline"><em>R</em><sub><em>B</em></sub></span>
            and waits for <span
            class="math inline"><em>R</em><sub><em>A</em></sub></span>.</li>
            <li>Timeout Handling: <em>every</em> synchronization
            mechanism should have a <strong>timeout</strong> to avoid
            indefinite blocking of critical tasks.</li>
            <li>Error Handling: detecting errors and handling them in a
            <strong>robust</strong> manner is critical to maintain
            system reliability; RTOSes use <em>retry mechanisms</em>,
            <em>logging</em> and, most importantly, have <strong>clear
            recovery procedures</strong> for failure scenarios.</li>
            </ol>
            <p>These considerations are crucial for ensuring system
            reliability, maintaining real-time performance, preventing
            system deadlocks, managing system resources effectively and
            handling error conditions gracefully.</p>
            </section>
            <section id="memory-management" class="level3"
            data-number="4.1.3">
            <h3 data-number="4.1.3"><span
            class="header-section-number">4.1.3</span> Memory
            Management</h3>
            <p>Real-time systems require <strong>predictable memory
            allocation and deallocation</strong> to avoid delays or
            fragmentation. Hence, they often use <strong>limited memory
            management techniques</strong> often eschewing even the use
            of dynamic memory allocation in favor of <strong>static
            memory allocation</strong>. For instance, many RTS don’t
            even use <code>malloc()</code> or <code>new</code>
            (<em>i.e.,</em> no heap allocated memory) and very often
            avoid garbage collection. The main goal is for tight control
            of the memory management → this makes <em>timing behavior
            more predictable</em>. Hence, the following become
            easier:</p>
            <ul>
            <li>wcet analysis</li>
            <li>schedulability and other analyses</li>
            <li>runtime monitoring and management</li>
            <li>recovery/restart</li>
            </ul>
            <p>Some <strong>goals</strong> for memory management in
            RTOSes:</p>
            <ol type="1">
            <li>predictable execution times for memory operations</li>
            <li>fast allocation/deallocation</li>
            <li>minimal fragmentation, if any</li>
            <li>protection mechanisms between tasks</li>
            </ol>
            <p>In fact, to achieve these goals, many RTSes <strong>don’t
            even use caches</strong> since they can be a major source of
            non-determinism in terms of timing behavior,
            <em>e.g.,</em></p>
            <blockquote>
            <p>if we cannot <strong>exactly calculate</strong> when some
            data/code will hit/miss in cache, then we cannot estimate
            its true timing behavior, leading to a lot of uncertainty →
            <strong>bad</strong>!</p>
            </blockquote>
            <p>Some RTSes use <a
            href="http://www.irisa.fr/alf/downloads/puaut/papers/date07.pdf"><strong>scratchpads</strong></a>
            since they provide cache-like performance but have higher
            predictability since the data onboarding/offloading is
            <strong>explicitly managed</strong> (either by the program
            or the <a
            href="https://cs-people.bu.edu/rmancuso/files/papers/SPM-OS_RTSJ19.pdf">RTOS</a>).</p>
            <p><strong>Some common memory-management techniques for
            RTOSes</strong>:</p>
            <ol type="1">
            <li><strong>static memory allocation</strong>: all memory
            used is allocated/deallocated at <strong>compile
            time</strong>.</li>
            <li><strong>memory pools</strong>: fixed-size blocks are
            pre-allocated for specific purposes → fragmentation and
            provides deterministic allocation times.</li>
            <li><strong>careful stack management</strong>: careful
            sizing/placing/management of the stack</li>
            <li><strong>limited heap memory</strong>: using “safe”
            versions of <code>malloc()</code> for instance</li>
            <li><strong>memory protection</strong>: using hardware such
            as memory protection units (MPUs)</li>
            <li><strong>memory partitioning</strong>: explicitly
            partition memory/caches so that tasks cannot read/write in
            each others’ memory regions</li>
            <li><strong>runtime mechanisms</strong>: such as memory
            usage monitoring, leak detection and managing the
            fragmentation</li>
            </ol>
            <blockquote>
            <p>Of course, each of these mechanisms have their own
            problems and a deliberation on those is left as an exercise
            for the reader.</p>
            </blockquote>
            </section>
            <section id="timer-and-interrupt-management" class="level3"
            data-number="4.1.4">
            <h3 data-number="4.1.4"><span
            class="header-section-number">4.1.4</span> Timer and
            Interrupt Management</h3>
            <p>Timer and interrupt management are crucial components of
            an RTOS, ensuring that tasks are <strong>executed at precise
            intervals</strong> and that the system responds promptly to
            (internal and) external events. The role between timers and
            interrupts is closely related, since they offer the very
            <strong>basic</strong> timing mechanism in RTOSes (from <a
            href="https://link.springer.com/book/10.1007/978-1-4614-0676-1">Hard
            Real-Time Computing Systems: Predictable Scheduling
            Algorithms and Applications</a>):</p>
            <blockquote>
            <p>to generate a <strong>time reference</strong>, a timer
            circuit is programmed to interrupt the processor at a
            <strong>fixed rate</strong> and the internal system time is
            represented by an integer variable, which is reset at system
            initialization and is incremented at each <strong>timer
            interrupt</strong>. The interval of time with which the
            timer is programmed to interrupt defines the unit of time in
            the system; that is, the minimum interval of time handled by
            the kernel (time resolution). The unit of time in the system
            is also called a system
            <strong><code>tick</code></strong>.</p>
            </blockquote>
            <p><br></p>
            <p>Timers, in general, play important roles in such systems,
            <em>viz.,</em></p>
            <table>
            <thead>
            <tr>
            <th>role</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>task scheduling</strong></td>
            <td>enable periodic execution of tasks</td>
            </tr>
            <tr>
            <td><strong>timeout management</strong></td>
            <td>prevent indefinite blocking of resources</td>
            </tr>
            <tr>
            <td><strong>event timing</strong></td>
            <td>measure intervals between events</td>
            </tr>
            <tr>
            <td><strong>system timing</strong></td>
            <td>maintain system clock and timestamps</td>
            </tr>
            <tr>
            <td><strong>watchdog functions</strong></td>
            <td>monitor system health and detect lockups</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Typically these systems have the following <em>three</em>
            types of timers:</p>
            <table>
            <colgroup>
            <col style="width: 29%" />
            <col style="width: 70%" />
            </colgroup>
            <thead>
            <tr>
            <th>type</th>
            <th>properties</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>hardware</strong></td>
            <td>- direct access to hardware timing resources<br>-
            highest precision and accuracy<br>- limited in number
            (hardware dependent)<br>- used for critical timing
            functions</td>
            </tr>
            <tr>
            <td><strong>software</strong></td>
            <td>- implemented in software, using hardware timer as
            base<br>- more flexibility, less precise<br>- limited only
            by memory<br>- more suitable for non-critical timing
            functions</td>
            </tr>
            <tr>
            <td><strong>system <code>tick</code></strong></td>
            <td>- <strong>core</strong> timer for RTOS <br> - drives
            task scheduling <br> - fixed frequency</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><img src="img/mermaid_figs/6.realtime.system_tick.png" width="400"></p>
            <p>There are various <strong>design considerations</strong>
            for timers in an RTOS, <em>viz.,</em></p>
            <ol type="1">
            <li><strong>resolution</strong> → the smaller the
            resolution, the higher the system/hardware/software/runtime
            overheads</li>
            <li><strong>accuracy</strong> → need to understand and
            manage <em>drift</em> and <em>jitter</em>; timers may need
            to be calibrated often++</li>
            <li><strong>power consumption</strong> → more
            accurate/high-precision a timer, higher the power
            consumption; also the <code>tick</code> can result in
            significant power consumption if not implemented/managed
            well</li>
            </ol>
            <p>(++ drift indicates a <em>gradual, long-term change</em>
            in the timer’s frequency over time, whereas jitter refers to
            <em>short-term, random fluctuations</em> in the timing of
            individual clock pulses)</p>
            <p><strong>Interrupt Latencies</strong> → time from when an
            interrupt occurs to when the corresponding interrupt service
            routine (ISR) starts executing. As interrupts are integral
            to the operation of an RTOS, from the implementation of the
            system <code>tick</code> to notifcations of internal
            (watchdog timers) and external events (new sensor data), it
            is important to <strong>minimize interrupt
            latencies</strong>.</p>
            <p>Optimization Techniques (to minimize latencies):</p>
            <ul>
            <li>minimize interrupt frequency → oftean an RTOS will
            disable interrupts in critical sections</li>
            <li>efficient timer and interrupt queue management →
            “nesting” interrupts,</li>
            <li>power-aware timing strategies → “<em>tickless</em>”
            operating systems have been tried</li>
            <li>optimize ISRs → keep them short, use other methods (<a
            href="https://www.osr.com/nt-insider/2009-issue1/deferred-procedure-call-details/">deferred
            procedure calls</a> or “<a
            href="http://www.cs.otago.ac.nz/cosc440/labs/lab08.pdf">bottom
            halves</a>”).</li>
            </ul>
            </section>
            <section id="kernel-performance-metrics" class="level3"
            data-number="4.1.5">
            <h3 data-number="4.1.5"><span
            class="header-section-number">4.1.5</span> Kernel
            Performance Metrics</h3>
            <blockquote>
            <p>Essentially, the kernel must be designed to
            <strong>minimize jitter</strong> and ensure that all
            operations have bounded and predictable execution times.</p>
            </blockquote>
            <p>Hence, we can try to evaluate whether an RTOS kernel
            meets these goals using the following metrics
            (<strong>note</strong>: not exhaustive):</p>
            <table>
            <colgroup>
            <col style="width: 48%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th>metric</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>interrupt latency</strong></td>
            <td>the time taken to respond to an interrupt</td>
            </tr>
            <tr>
            <td><strong>context switch time</strong></td>
            <td>time to switch between tasks</td>
            </tr>
            <tr>
            <td><strong>dispatch latency</strong></td>
            <td>time difference between task being ready and when it
            starts executing</td>
            </tr>
            <tr>
            <td><strong>throughput</strong></td>
            <td>number of tasks?operations kernel can handle per unit
            time</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            </section>
            <section id="examples-of-rtos" class="level2"
            data-number="4.2">
            <h2 data-number="4.2"><span
            class="header-section-number">4.2</span> Examples of
            RTOS</h2>
            <table>
            <colgroup>
            <col style="width: 24%" />
            <col style="width: 41%" />
            <col style="width: 34%" />
            </colgroup>
            <thead>
            <tr>
            <th><strong>name</strong></th>
            <th><strong>description</strong></th>
            <th><strong>features</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><a href="https://www.freertos.org">FreeRTOS</a></td>
            <td>a widely used, <strong>open-source</strong> RTOS for
            embedded systems</td>
            <td>small footprint, portable, supports a wide range of
            microcontrollers</td>
            </tr>
            <tr>
            <td><a
            href="https://www.windriver.com/products/vxworks">VxWorks</a></td>
            <td><strong>commercial</strong> RTOS used in aerospace,
            defense, applications</td>
            <td>high reliability, real-time performance, and support for
            multi-core processors</td>
            </tr>
            <tr>
            <td><a href="https://blackberry.qnx.com/en">QNX</a></td>
            <td>a <strong>commercial</strong> RTOS known for its
            reliability and use in automotive and medical systems</td>
            <td>microkernel architecture, high security, support for
            posix apis</td>
            </tr>
            <tr>
            <td><a href="https://www.zephyrproject.org">Zephyr</a></td>
            <td><strong>open-source</strong> RTOS designed for IoT and
            Edge devices</td>
            <td>modular, scalable, supports a wide range of hardware
            architectures</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Why is Linux not on the list? While it has many
            (increasing) <a
            href="https://www.zdnet.com/article/real-time-linux-leads-kernel-v6-12s-list-of-new-features/">list
            of real-time features</a>, it is still far from a
            <strong>hard real-time system</strong>, mainly due to its
            complexity. It is difficult to analyze WCETs on Linux or
            completely control its timers → the list is endless. It
            still sees use in many real-time and embedded systems and we
            will (brielfy) explore its real-time capabilities soon.</p>
            <section id="freertos" class="level3" data-number="4.2.1">
            <h3 data-number="4.2.1"><span
            class="header-section-number">4.2.1</span> FreeRTOS</h3>
            <p>As mentioned earlier, FreeRTOS is one of the most popular
            open-source RTOS options, widely used in embedded systems
            due to its simplicity, portability and extensive community
            support. It supports,</p>
            <ul>
            <li>creation of multiple tasks, each with its own
            priority</li>
            <li>preemptive and cooperative scheduling</li>
            <li>mechanisms like queues, semaphores, and mutexes for
            communication and synchronization between tasks</li>
            <li>several memory management schemes, including heap_1,
            heap_2, heap_3, heap_4, and heap_5, to suit different
            application requirements</li>
            <li><strong>highly portable</strong> and supports a wide
            range of microcontrollers and development boards, including
            ARM Cortex-M, ESP32 and STM32</li>
            <li>a large and active community, with [extensive
            documentation, tutorials and examples available online</li>
            </ul>
            <p>Here is an example that uses FreeRTOS to blink the LEDs
            on a microcontroller:</p>
            <div class="sourceCode" id="cb15"><pre
            class="sourceCode c"><code class="sourceCode c"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;FreeRTOS.h&gt;</span></span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;task.h&gt;</span></span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a><span class="pp">#include </span><span class="im">&lt;gpio.h&gt;</span></span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-5"><a href="#cb15-5" aria-hidden="true" tabindex="-1"></a><span class="co">// Task to blink an LED</span></span>
<span id="cb15-6"><a href="#cb15-6" aria-hidden="true" tabindex="-1"></a><span class="dt">void</span> vBlinkTask<span class="op">(</span><span class="dt">void</span> <span class="op">*</span>pvParameters<span class="op">)</span> <span class="op">{</span></span>
<span id="cb15-7"><a href="#cb15-7" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="op">(</span><span class="dv">1</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb15-8"><a href="#cb15-8" aria-hidden="true" tabindex="-1"></a>        GPIO_TogglePin<span class="op">(</span>LED_PIN<span class="op">);</span>  <span class="co">// Toggle LED</span></span>
<span id="cb15-9"><a href="#cb15-9" aria-hidden="true" tabindex="-1"></a>        vTaskDelay<span class="op">(</span>pdMS_TO_TICKS<span class="op">(</span><span class="dv">500</span><span class="op">));</span>  <span class="co">// Delay for 500ms</span></span>
<span id="cb15-10"><a href="#cb15-10" aria-hidden="true" tabindex="-1"></a>    <span class="op">}</span></span>
<span id="cb15-11"><a href="#cb15-11" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span>
<span id="cb15-12"><a href="#cb15-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-13"><a href="#cb15-13" aria-hidden="true" tabindex="-1"></a><span class="dt">int</span> main<span class="op">(</span><span class="dt">void</span><span class="op">)</span> <span class="op">{</span></span>
<span id="cb15-14"><a href="#cb15-14" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Initialize hardware</span></span>
<span id="cb15-15"><a href="#cb15-15" aria-hidden="true" tabindex="-1"></a>    GPIO_Init<span class="op">(</span>LED_PIN<span class="op">,</span> GPIO_MODE_OUTPUT<span class="op">);</span></span>
<span id="cb15-16"><a href="#cb15-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-17"><a href="#cb15-17" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Create the blink task</span></span>
<span id="cb15-18"><a href="#cb15-18" aria-hidden="true" tabindex="-1"></a>    xTaskCreate<span class="op">(</span>vBlinkTask<span class="op">,</span> <span class="st">&quot;Blink&quot;</span><span class="op">,</span> configMINIMAL_STACK_SIZE<span class="op">,</span> NULL<span class="op">,</span> <span class="dv">1</span><span class="op">,</span> NULL<span class="op">);</span></span>
<span id="cb15-19"><a href="#cb15-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-20"><a href="#cb15-20" aria-hidden="true" tabindex="-1"></a>    <span class="co">// Start the scheduler</span></span>
<span id="cb15-21"><a href="#cb15-21" aria-hidden="true" tabindex="-1"></a>    vTaskStartScheduler<span class="op">();</span></span>
<span id="cb15-22"><a href="#cb15-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb15-23"><a href="#cb15-23" aria-hidden="true" tabindex="-1"></a>    <span class="co">// The program should never reach here</span></span>
<span id="cb15-24"><a href="#cb15-24" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> <span class="op">(;;);</span></span>
<span id="cb15-25"><a href="#cb15-25" aria-hidden="true" tabindex="-1"></a><span class="op">}</span></span></code></pre></div>
            <p><strong>Resources</strong>:</p>
            <ol type="1">
            <li><a
            href="https://www.freertos.org/Documentation/RTOS_book.html">FreeRTOS
            Documentation</a></li>
            <li><a
            href="https://www.freertos.org/Why-FreeRTOS/Features-and-demos/RAM_constrained_design_tutorial/Real-time-application-design">FreeRTOS
            Tutorials</a></li>
            <li><a
            href="https://forums.freertos.org/t/using-freertos-with-the-raspberry-pi-pico-blog-series/16497"><strong>Raspberry
            Pi and FreeRTOS</strong></a> [<a
            href="https://github.com/aws-iot-builder-tools/freertos-pi-pico">GitHub
            Repo</a>]</li>
            </ol>
            </section>
            <section id="linuxreal-time" class="level3"
            data-number="4.2.2">
            <h3 data-number="4.2.2"><span
            class="header-section-number">4.2.2</span>
            Linux+Real-Time</h3>
            <p>As mentioned earlier, Linux, as a general-purpose
            operating system, is not inherently a real-time operating
            system (RTOS). However, it does provide several features and
            mechanisms that can be used to achieve real-time
            performance, especially when combined with real-time patches
            or specialized configurations.</p>
            <p>Some of the real-time features of Linux include:</p>
            <ul>
            <li><p><strong><a
            href="https://wiki.linuxfoundation.org/realtime/start">Preempt-RT
            Patch</a></strong>: a set of patches that convert the Linux
            kernel into a fully preemptible kernel, reducing latency and
            improving real-time performance; the Preempt-RT patch
            achieves this by:</p>
            <ul>
            <li>making almost <strong>all kernel code
            preemptible</strong>: allows higher-priority tasks to
            preempt lower-priority tasks, even when the lower-priority
            tasks are executing kernel code</li>
            <li><strong>converting interrupt handlers to kernel
            threads</strong>: reduces time spent with interrupts
            disabled, for better predictability and lower latency</li>
            <li><strong>implementing priority inheritance</strong>:
            helps prevent priority inversion by temporarily elevating
            priority of lower-priority tasks holding a resource needed
            by higher-priority tasks</li>
            <li><strong>reducing non-preemptible sections</strong>:
            minimizes time during which preemption is disabled, further
            reducing latency</li>
            <li><strong>enhancing timer granularity</strong>: allows for
            more precise timing and scheduling of tasks, crucial for
            real-time applications</li>
            </ul>
            <p>the Preempt-RT patch is widely used in industries such as
            telecommunications, industrial automation and audio
            processing. It is actively maintained and supported by the
            Linux Foundation’s <a
            href="https://wiki.linuxfoundation.org/realtime/rtl/start">Real-Time
            Linux</a> (RTL) collaborative project</p></li>
            <li><p><strong><a
            href="https://man7.org/linux/man-pages/man7/sched.7.html">Real-Time
            scheduling policies</a></strong>: support for real-time
            scheduling policies such as <code>SCHED_FIFO</code> and
            <code>SCHED_RR</code>, which provide deterministic
            scheduling behavior</p></li>
            <li><p><strong><a
            href="https://www.kernel.org/doc/html/latest/timers/hrtimers.html">High-resolution
            timers</a></strong>: support for high-resolution timers that
            allow for precise timing and scheduling of tasks</p></li>
            <li><p>basic <strong><a
            href="https://www.kernel.org/doc/Documentation/locking/priority-inheritance.txt">priority
            inheritance</a></strong>: mechanism to prevent priority
            inversion by temporarily elevating the priority of
            lower-priority tasks holding a resource needed by
            higher-priority tasks</p></li>
            <li><p><strong><a
            href="https://www.kernel.org/doc/Documentation/admin-guide/kernel-parameters.html#isolcpus">CPU
            isolation</a></strong>: ability to isolate CPUs from the
            general scheduler, dedicating them to specific real-time
            tasks; also pinning processes to certain cores</p></li>
            <li><p><strong><a
            href="https://www.kernel.org/doc/Documentation/core-api/genericirq.rst">Threaded
            interrupts</a></strong>: support for handling interrupts in
            kernel threads, reducing interrupt latency and improving
            predictability</p></li>
            <li><p><strong><a
            href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux_for_real_time/8/html/understanding_rhel_for_real_time/assembly_memory-management-on-rhel-for-real-time-_understanding-rhel-for-real-time-core-concepts#con_demand-paging_assembly_memory-management-on-rhel-for-real-time-">Memory
            management</a></strong> techniques: such as <a
            href="https://linux.die.net/man/2/mlock"><strong>memory
            locking</strong></a> to prevent pages from being swapped,
            the use of “<a
            href="https://docs.redhat.com/en/documentation/red_hat_enterprise_linux/6/html/performance_tuning_guide/s-memory-transhuge"><strong>huge</strong></a>”
            pages and memory <a
            href="https://docs.kernel.org/core-api/memory-allocation.html"><strong>pre-allocation</strong></a></p></li>
            <li><p><strong><a
            href="https://www.kernel.org/doc/Documentation/cgroup-v1/cgroups.txt">Control
            groups (cgroups)</a></strong>: mechanism to allocate CPU,
            memory and I/O resources to specific groups of tasks,
            ensuring resource availability for real-time tasks</p></li>
            </ul>
            <p>These features, when properly configured, can help
            achieve real-time performance on Linux, making it suitable
            for certain real-time and embedded applications.</p>
            </section>
            <section id="raspberry-pi-osreal-time" class="level3"
            data-number="4.2.3">
            <h3 data-number="4.2.3"><span
            class="header-section-number">4.2.3</span> Raspberry Pi
            OS+Real-Time</h3>
            <p>The <a
            href="https://www.raspberrypi.com/software/">Raspberry Pi
            OS</a> can also be made “real-time” in the same manner as
            decribed above, since it is a Linux variant.</p>
            <p>Though, there are some attempts at getting the Pi to
            behave in a real-time fashion, <em>e.g.,</em>: <a
            href="https://www.socallinuxexpo.org/sites/default/files/presentations/Steven_Doran_SCALE_13x.pdf"><strong>1</strong></a>,
            <a
            href="https://all3dp.com/2/rtos-raspberry-pi-real-time-os/#google_vignette"><strong>2</strong></a>,
            <a
            href="https://floating.io/2023/04/raspberry-pi-in-real-time/"><strong>3</strong></a>.</p>
            <p><br></p>
            </section>
            </section>
            <section id="robot-operating-system-ros" class="level2"
            data-number="4.3">
            <h2 data-number="4.3"><span
            class="header-section-number">4.3</span> Robot Operating
            System (ROS)</h2>
            <p>ROS is an <strong>open source middleware</strong>
            framework built for robotics applications. The main goal →
            develop <strong>standards</strong> for robotic software. ROS
            provides many <strong>reusable modules</strong> for
            developing robotic applications.</p>
            <p>Embedded/autonomous programs that do simple tasks (or
            operate with a single sensor/motor) are relatively easy to
            program. As more sensing, actuation, functionality is added
            (consider a larege industrial robot or even an autonomous
            car), programs quickly become quite complex – coordination
            of the data and system states becomes challenging.</p>
            <p><img src="img/rtos/ros/ros.complexity.webp" width="400"></p>
            <p><br></p>
            <p>ROS helps to develop and <strong>scale</strong> such
            applications and also <strong>manages
            communications</strong> between various parts of the
            software. As mentioned earlier, ROS is <a
            href="https://www.redhat.com/en/topics/middleware/what-is-middleware"><strong>middleware</strong></a>:</p>
            <blockquote>
            <p>Middleware is a software layer that connects the
            operating system to applications, data, and users. It
            provides common services and capabilities, like single-sign
            on (SSO), easy communication/coordination (like ROS) or
            application programming interface (API) management.
            Developers can rely on middleware to provide consistent,
            simplified integrations between application components. This
            frees up developers to build core features of applications,
            rather than spend time connecting those features to
            different endpoints and environments, including legacy
            systems.</p>
            </blockquote>
            <p>At a high level, ROS,</p>
            <ul>
            <li>creates a <em>separation</em> of code blocks → into
            reusable blocks</li>
            <li>provides <em>tools</em> → easy communication between
            sub-programs</li>
            <li>is <em>language agnostic</em> → allows different
            components to be written in, say Python and C and yet
            communicate using the <strong>ROS communication
            protocol</strong></li>
            </ul>
            <p>A simple example: <a
            href="https://dilipkumar.medium.com/ros-v1-robot-operating-system-88039990e913">control
            of a robotic arm+camera</a>:</p>
            <p><img src="img/rtos/ros/ros.robot_camera_example.webp" width="400"></p>
            <p><br></p>
            <p>To write a ROS application to control this robotic arm,
            we first create a few <strong>subprograms</strong>:</p>
            <ul>
            <li>one for the camera → <code>node</code></li>
            <li>another for → <code>motion planning</code></li>
            <li>one for → <code>hardware drivers</code></li>
            <li>finally one for → <code>joystick</code></li>
            </ul>
            <p>Now we use ROS → <strong>communication</strong> between
            these nodes.</p>
            <p>ROS even provides <strong>plug and play
            libraries</strong> for designing your system, <em>e.g.,</em>
            <a
            href="https://moveit.ai/moveit/ros2/2020/02/18/moveit-2-beta-feature-list.html">inverse
            kinematics libraries</a>, <a
            href="https://roboticseabass.com/2024/06/30/how-do-robot-manipulators-move/">trajectory
            planning for robotic arms</a>, <em>etc.</em></p>
            <section id="ros-components" class="level3"
            data-number="4.3.1">
            <h3 data-number="4.3.1"><span
            class="header-section-number">4.3.1</span> ROS
            Components</h3>
            <p>Some important <strong>components</strong> of ROS:</p>
            <p><img src="img/mermaid_figs/6.realtime.ros_architecture_legends.png" width="400">
            <br>
            <img src="img/mermaid_figs/6.realtime.ros_architecture.png" width="400"></p>
            <ol type="1">
            <li><a href="http://wiki.ros.org/Nodes">node</a></li>
            </ol>
            <ul>
            <li>a process that performs <strong>computation</strong> (a
            program/subprogram)</li>
            <li>combined together into a graph</li>
            <li>communicate via “topics”</li>
            <li>operate at a fine-grained scale</li>
            <li>a full system will have <em>multiple</em> nodes,
            <em>e.g.,</em>
            <ul>
            <li>one node controls a laser range-finder</li>
            <li>one Node controls the robot’s wheel motors</li>
            <li>one node performs localization</li>
            <li>one node performs path planning</li>
            <li>one node provides a graphical view of the system</li>
            </ul></li>
            </ul>
            <p>The use of nodes has several benefits such as
            <strong>fault tolerance</strong>, <strong>reduced
            complexity</strong> and <strong>modularity</strong>.</p>
            <ol start="2" type="1">
            <li><a href="http://wiki.ros.org/Topics">topics</a></li>
            </ol>
            <ul>
            <li>they’re <strong>named buses</strong> over which nodes
            exchange “messages”</li>
            <li><strong>anonymous publish/subscribe semantics</strong> →
            decouples production of information from its
            consumption</li>
            <li>nodes are not aware of who they are communicating
            with</li>
            <li>nodes that are interested in data
            <strong>subscribe</strong> to the <em>relevant
            topic</em></li>
            <li>nodes that <em>generate</em> data
            <strong>publish</strong> to the relevant topic</li>
            <li>can be <strong>multiple</strong> publishers and
            subscribers to a topic</li>
            <li>topic is <strong>strongly typed</strong> by publisher →
            nodes can only receive messages with a matching type</li>
            </ul>
            <p>Topics are meant for <em>unidirectional</em>,
            <em>streaming</em> communication. ROS includes other
            mechanisms such as <a
            href="http://wiki.ros.org/Services">services</a> and
            [parameter servers]http://wiki.ros.org/Parameter%20Server)
            for different types of communciations.</p>
            <ol start="3" type="1">
            <li><a href="http://wiki.ros.org/Messages">messages</a></li>
            </ol>
            <ul>
            <li>nodes communicate with each other by publishing messages
            to topics</li>
            <li>simple text files</li>
            <li>simple data structure → <strong>typed
            fields</strong></li>
            <li>support standard primitives (<code>int</code>,
            <code>float</code>, <code>boolean</code>)</li>
            <li>can include arbitrarily nested <code>structs</code> and
            <code>arrays</code></li>
            <li>nodes can exchange → <code>request</code> an
            <code>response</code> messages</li>
            </ul>
            <p>A simple ROS message:</p>
            <pre class="ros"><code>std_msgs/Header header
  uint32 seq
  time stamp
  string frame_id
geometry_msgs/Point point
  float64 x
  float64 y
  float64 z</code></pre>
            <p>Example: <a
            href="https://classes.cs.uchicago.edu/archive/2022/spring/20600-1/ros_intro.html">our
            first ROS message</a>.</p>
            <ol start="4" type="1">
            <li><a href="http://wiki.ros.org/Master">ROS Master</a></li>
            </ol>
            <ul>
            <li>provides naming and registration services to the rest of
            the nodes in the ROS system</li>
            <li>also runs the <a
            href="http://wiki.ros.org/Parameter%20Server">parameter
            server</a> → a shared, multi-variate dictionary that is
            accessible via network APIs, used by nodes to
            <strong>store/retrieve parameters</strong></li>
            <li>tracks publishers and subscribers to topics as well as
            services</li>
            <li>enable individual ROS nodes to locate one anothe</li>
            <li>once located, they communicate in a
            <strong>peer-to-peer</strong> fashion</li>
            </ul>
            <p>Example:</p>
            <ol type="1">
            <li>consider two nodes → <code>camera</code> node and
            <code>image_viewer</code> node</li>
            <li><code>camera</code> notifies <code>master</code> → wants
            to publish images on the topic, <code>images</code></li>
            </ol>
            <p><img src="img/rtos/ros/ROS_master_example_english_1.png"></p>
            <ol start="3" type="1">
            <li>no one is subscribing to the topic, yet → <strong>no
            images sent</strong></li>
            <li><code>image viewer</code> → subscribe to
            <code>images</code> topic</li>
            </ol>
            <p><img src="img/rtos/ros/ROS_master_example_english_2.png"></p>
            <ol start="5" type="1">
            <li>topic, <code>images</code> has both → publisher and
            subscriber</li>
            <li><code>master</code> notifies both → of each others’
            existence</li>
            </ol>
            <p><img src="img/rtos/ros/ROS_master_example_english_3.png"></p>
            <ol start="7" type="1">
            <li>both start <strong>communicating with each
            other</strong>, directly</li>
            </ol>
            <p><br></p>
            <p>A more intricate example of the same:</p>
            <p><img src="img/mermaid_figs/6.realtime.ros_publish_subscribe.png" width="400"></p>
            <ol start="5" type="1">
            <li><a href="http://wiki.ros.org/tf2">ROS transform</a></li>
            </ol>
            <ul>
            <li>robotic system typically has many 3D coordinate frames
            that change over time
            <ul>
            <li><em>e.g.,</em> world frame, base frame, gripper frame,
            head frame, <em>etc.</em></li>
            </ul></li>
            <li>lets the user keep track of multiple coordinate frames
            over time</li>
            <li>maintains the relationship between coordinate frames →
            manages <strong>spatial relationships</strong></li>
            <li>in a tree structure buffered in time</li>
            <li>lets the user transform points, vectors, <em>etc.</em> →
            at any desired point in time</li>
            <li><strong>distributed</strong> → coordinate frames of
            robot available to <strong>all</strong> ROS components on
            any computer in the system</li>
            <li>sensor fusion, motion planning, and navigation</li>
            <li>organizes all coordinate frames and their relationships
            into a <strong>transform tree</strong></li>
            </ul>
            <p>An example of a ROS transform and tree:</p>
            <p><img src="img/mermaid_figs/6.realtime.ros_transform.png" width="400"></p>
            </section>
            <section id="ros-and-real-time" class="level3"
            data-number="4.3.2">
            <h3 data-number="4.3.2"><span
            class="header-section-number">4.3.2</span> ROS and
            Real-time?</h3>
            <p>ROS (the first version) is <strong>not</strong>
            real-time. Hence, systems that requires <strong>hard
            real-time guarantees</strong> shoud not use it. But ROS can
            be itegrated into systems that require <em>some</em> latency
            guarantees. If needed, ROS can be run on top of the
            <code>RT_PREEMPT</code> real-time patch on Linux. In
            addition, <strong>specific nodes</strong> can be designed to
            handle real-time functions or programmed to behave as
            real-time control systems.</p>
            <p>If better real-time guarantees are required on ROS, then
            <a
            href="https://roscon.ros.org/2015/presentations/RealtimeROS2.pdf"><strong>ROS
            2</strong></a> if your best bet.</p>
            <p><strong>Resources</strong>: more information on real-time
            and ROS2 can be found at <a
            href="https://xilinx.github.io/KRS/sphinx/build/html/docs/features/realtime_ros2.html">RT
            ROS2 Xilinx</a> and <a
            href="https://github.com/ros-realtime">RT ROS
            Github</a>.</p>
            </section>
            <section id="rosnavio2" class="level3" data-number="4.3.3">
            <h3 data-number="4.3.3"><span
            class="header-section-number">4.3.3</span> Ros+Navio2</h3>
            <p>We use ROS (the original version, not ROS2) in our class.
            <strong>Note:</strong> while ROS has no real-time
            capabilities, one some embedded systems, if it <em>fast
            enough</em> that we can use it to control safety-critical
            systems such as drones and other small autonomous
            systems.</p>
            <p>In fact, the basic Raspbian image comes installed with
            ROS. We can use it communicate between the Navio2 and the
            controller running on the Pi to exchange critical
            information, <em>e.g.</em>, sensor data.</p>
            <p><img src="img/rtos/ros/ros.ardupilot_navio.png" width="400"></p>
            <p><strong>Resources</strong>: please read the <a
            href="https://docs.emlid.com/navio2/ros/">step-by-step
            instructions</a> on how to connect/use the Navio2 and the Pi
            using ROS.</p>
            <!--t
             rel="stylesheet" href="./custom.sibin.css"-->
            </section>
            </section>
            </section>
            <section id="scheduling-for-real-time-systems"
            class="level1" data-number="5">
            <h1 data-number="5"><span
            class="header-section-number">5</span> Scheduling for
            Real-Time Systems</h1>
            <p>Consider an engine control system that cycles through the
            various phases of operation for an <a
            href="http://automobile-us.blogspot.com">automotive
            engine</a>:</p>
            <p><img src="img/scheduling/engine_animation.gif"></p>
            <p><br></p>
            <p>This system <strong>periodically</strong> cycles through
            multiple tasks, <em>viz.</em>,</p>
            <ol type="1">
            <li>air intake</li>
            <li>pressure</li>
            <li>fuel injection+combustion</li>
            <li>exhaust</li>
            </ol>
            <p>If we correlate this to task “activations”, then we may
            see the <a
            href="https://retis.sssup.it/~a.biondi/papers/ERIKA_AVR_RTAS16.pdf">following</a>:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <tbody>
            <tr>
            <td><img src="img/scheduling/engine_animation.gif" width="180"></td>
            <td><img src="img/scheduling/angular_task.png" width="300"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>This is a (somewhat) simple execution model known as the
            <a href="#cyclic-executives">cyclic executive</a> that we
            shall return to later. Hence, there is a
            <strong>direct</strong> correlation between the physical
            aspects of a real-time and autonomous system and issues such
            as scheduling.</p>
            <section id="real-time-models" class="level2"
            data-number="5.1">
            <h2 data-number="5.1"><span
            class="header-section-number">5.1</span> Real-Time
            Models</h2>
            <p>Most real-time systems can be classified into:</p>
            <table>
            <colgroup>
            <col style="width: 25%" />
            <col style="width: 25%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>property</th>
            <th>hard</th>
            <th>soft</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>“<em>usefulness</em>” of results</td>
            <td><img src="img/scheduling/hard_soft/pngs/hard_rts.png" width="200"></td>
            <td><img src="img/scheduling/hard_soft/pngs/soft_rts.png" width="200"></td>
            </tr>
            <tr>
            <td>optimality criterion</td>
            <td><strong>all</strong> deadlines must be satisfied</td>
            <td><strong>most</strong> deadlines must be met</td>
            </tr>
            <tr>
            <td>examples</td>
            <td>sensor readings, vehicular control</td>
            <td>video decoding, displaying messages</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>There are many ways to <strong>model</strong> a real-time
            system:</p>
            <ol type="1">
            <li><a href="#workload-model"><strong>workload</strong>
            model</a> → describes applications supported by system,
            using
            <ul>
            <li>functional parameters</li>
            <li>temporal parameters</li>
            <li>precedence constraints and dependencies</li>
            </ul></li>
            <li><a href="#resource-model"><strong>resource</strong>
            model</a> → describes system resources available to
            applications
            <ul>
            <li>modeling resources, e.g., processors, network cards,
            etc.</li>
            <li>resource parameters</li>
            </ul></li>
            <li><a
            href="#scheduling-and-algorithms"><strong>algorithms</strong></a>
            → defines how application uses resources at all times
            <ul>
            <li>scheduling policies</li>
            <li>other resource management algorithms</li>
            </ul></li>
            </ol>
            <section id="workload-model" class="level3"
            data-number="5.1.1">
            <h3 data-number="5.1.1"><span
            class="header-section-number">5.1.1</span> Workload
            Model</h3>
            <p>We have already discussed <a
            href="#tasks-jobs-threads">tasks vs. jobs vs. thread</a>
            earlier so we understand how to model computation. We also
            discussed how actual execution is modeled (via threads).</p>
            <p>Let us focus on <strong>jobs</strong> and some of their
            properties:</p>
            <p><img src="img/scheduling/jobs/job.final.svg" width="400"></p>
            <p><strong>note</strong> → deadlines and periods don’t have
            to match, but they <strong>usually</strong> do, _i.e., <span
            class="math inline"><em>D</em> = <em>P</em></span></p>
            <table>
            <colgroup>
            <col style="width: 41%" />
            <col style="width: 58%" />
            </colgroup>
            <thead>
            <tr>
            <th>property/parameters</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>temporal</strong></td>
            <td>timing constraints and behavior</td>
            </tr>
            <tr>
            <td><strong>functional</strong></td>
            <td>intrinsic properties of the job</td>
            </tr>
            <tr>
            <td><strong>resource</strong></td>
            <td>resource requirements</td>
            </tr>
            <tr>
            <td><strong>interconnection</strong></td>
            <td>how it depends on other jobs and how other jobs depend
            on it</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>As discussed earlier, we need to get a good understanding
            of the <a href="l#the-wcet-problem">wcet</a> of jobs.</p>
            </section>
            <section id="utilization" class="level3"
            data-number="5.1.2">
            <h3 data-number="5.1.2"><span
            class="header-section-number">5.1.2</span> Utilization</h3>
            <p>One of the important ways to understand the
            <strong>workload requirements</strong> for a
            <strong>task</strong> is to compute,</p>
            <blockquote>
            <p>how much <strong>utilization</strong> is taken up by
            <strong>all</strong> jobs of the task?</p>
            </blockquote>
            <p>One might ask: if there are (potentially) an
            <em>infinite</em> number of jobs for each task, since
            they’re periodic++ and long running, then how can one
            campute the utilization?</p>
            <blockquote>
            <p>Recall that a <strong>periodic</strong> function is of
            the type → <span
            class="math inline"><em>f</em>(<em>t</em>) = <em>f</em>(<em>t</em> + <em>T</em>)</span></p>
            <p>where <span class="math inline"><em>T</em></span> is the
            “period”</p>
            </blockquote>
            <p><strong>Note:</strong> utilization is
            <strong>not</strong> the time taken by a task (or its jobs).
            It is,</p>
            <blockquote>
            <p>the <strong>fraction</strong> of the processor’s total
            available utilization that is soaked up by the jobs of a
            task</p>
            </blockquote>
            <p>Hence, the utilization for a <strong>single</strong> task
            is,</p>
            <p><span class="math display">$$ U_i = \frac{c_i}{T_i}
            $$</span></p>
            <p>where,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>c</em><sub><em>i</em></sub></span></td>
            <td>wcet of the task</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>T</em><sub><em>i</em></sub></span></td>
            <td>period of the task</td>
            </tr>
            </tbody>
            </table>
            <p>Now, we can compute the utilization for the
            <strong>entire task set</strong>,</p>
            <p><span class="math display">$$ U = \sum_{i=1}^{n} U_i =
            \sum_{i=1}^{n} \frac{c_i}{T_i} $$</span></p>
            <p><strong>Simple Exercise</strong>: what is the total
            utilization for the following task set?</p>
            <table>
            <thead>
            <tr>
            <th>Task</th>
            <th>c</th>
            <th>T</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>τ1</td>
            <td>1</td>
            <td>4</td>
            </tr>
            <tr>
            <td>τ2</td>
            <td>2</td>
            <td>5</td>
            </tr>
            <tr>
            <td>τ3</td>
            <td>5</td>
            <td>17</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <blockquote>
            <p>what does it mean if <span
            class="math inline"><em>U</em> &gt; 1</span>?</p>
            </blockquote>
            <section id="precedence-constraints" class="level4"
            data-number="5.1.2.1">
            <h4 data-number="5.1.2.1"><span
            class="header-section-number">5.1.2.1</span> Precedence
            Constraints</h4>
            <p>Jobs can be either <strong>precedence
            constrained</strong> or <strong>independent</strong> → we
            can use directed acyclic graphs (DAGs) to specify/capture
            these constraints.</p>
            <p>For instance, $ J_a J_b$ implies,</p>
            <ul>
            <li><span
            class="math inline"><em>J</em><sub><em>a</em></sub></span>
            is a <strong>predecessor</strong> of <span
            class="math inline"><em>J</em><sub><em>b</em></sub></span></li>
            <li><span
            class="math inline"><em>J</em><sub><em>b</em></sub></span>
            is a <strong>successor</strong> of <span
            class="math inline"><em>J</em><sub><em>a</em></sub></span></li>
            </ul>
            <p>$ J_a J_b$ implies</p>
            <ul>
            <li><span
            class="math inline"><em>J</em><sub><em>a</em></sub></span>
            is an <strong>immediate</strong> predecessor of <span
            class="math inline"><em>J</em><sub><em>b</em></sub></span></li>
            </ul>
            <p>Consider the following example:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>dag</th>
            <th>relationships</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/scheduling/jobs/job_precedence.png" width="150"></td>
            <td><span
            class="math inline"><em>J</em><sub>1</sub> ≺ <em>J</em><sub>2</sub></span>
            <br> $ J_1 J_2$ <br> <span
            class="math inline"><em>J</em><sub>1</sub> ≺ <em>J</em><sub>4</sub></span>
            <br> <span
            class="math inline"><em>J</em><sub>1</sub> ↛ <em>J</em><sub>4</sub></span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Here is a more realistic example → the precedence
            constraints in an <strong>autonomous driving
            system</strong>:</p>
            <p><img src="img/scheduling/jobs/autonomous_precedence_constraints_rtss2021.png" width="400"></p>
            </section>
            </section>
            <section id="resource-model" class="level3"
            data-number="5.1.3">
            <h3 data-number="5.1.3"><span
            class="header-section-number">5.1.3</span> Resource
            Model</h3>
            <p>A “resource” is some structure used by task → advance
            execution.</p>
            <p>Type of resources:</p>
            <ul>
            <li><strong>active</strong> → e.g., processors (they execute
            jobs)
            <ul>
            <li>every job → one or more processors</li>
            <li>same type if functionally identical and used
            interchangeably</li>
            </ul></li>
            <li><strong>passive</strong> → <em>e.g.,</em> files, network
            sockets, <em>etc.</em></li>
            <li><strong>private</strong> vs <strong>shared</strong></li>
            </ul>
            <p>We have already discussed <a
            href="#inter-task-communication-and-synchronization">resource
            sharing and synchronization</a> earlier → mutexes, locks,
            <em>etc.</em> Esentially there is a need for <strong>mutual
            exclusion</strong> that is guaranteed via one of these
            methods or by the use of <a
            href="https://cs.gmu.edu/~rcarver/ModernMultithreading/LectureNotes/Chapter2Notes.pdf">critical
            sections</a>.</p>
            </section>
            <section id="scheduling-and-algorithms" class="level3"
            data-number="5.1.4">
            <h3 data-number="5.1.4"><span
            class="header-section-number">5.1.4</span> Scheduling and
            Algorithms</h3>
            <p>Before we proceed, we need to understand →
            <strong>preemption</strong>:</p>
            <blockquote>
            <p>preemption is the process of <strong>suspending a running
            task</strong> in <strong>favor of a higher priority
            task</strong>.</p>
            </blockquote>
            <p><img src="img/scheduling/preemption.png" width="400"></p>
            <p>Most OS (real-time &amp; non real-time) allow preemption
            since they allow,</p>
            <ul>
            <li>greater flexibility in constructing schedules</li>
            <li>exception handling</li>
            <li>interrupt servicing</li>
            </ul>
            <p>Preemptions, among others, help define a variety of
            <strong>scheduling events</strong>; essentially, these are
            the time when the <strong>scheduler is invoked</strong>:</p>
            <ul>
            <li>in a <em>fully</em> preemptive system → task arrival,
            task completion, resource reauest, resource release,
            interrupts and exceptions, <em>etc.</em></li>
            <li>in a <em>non-preemptive</em> system → task
            completion</li>
            <li>in <em>limited</em> preemptive systems → predefined
            preemptions points, <em>e.g.,</em> POSIX thread cancel
            (<code>pthread_testcancel()</code>)</li>
            </ul>
            <section id="task-schedule" class="level4"
            data-number="5.1.4.1">
            <h4 data-number="5.1.4.1"><span
            class="header-section-number">5.1.4.1</span> Task
            Schedule</h4>
            <p>We have been talking about “scheduling” all this while so
            it is time to formally define what a
            <strong>schedule</strong> is.</p>
            <p>Given → a set of jobs, <span
            class="math inline"><em>J</em> = {<em>J</em><sub>1</sub>, <em>J</em><sub>2</sub>, ..., <em>J</em><sub><em>n</em></sub>}</span></p>
            <blockquote>
            <p>A schedule → an <strong>assignment</strong> of Jobs to
            the CPU, so that each task can execute till completion.</p>
            </blockquote>
            </section>
            </section>
            </section>
            <section id="schedulers" class="level2" data-number="5.2">
            <h2 data-number="5.2"><span
            class="header-section-number">5.2</span> Schedulers</h2>
            <p>Finally, we get to the main topic at hand → schedulers
            and scheduling! Historically there have been many scheduling
            policies developed for a variety of systems, <em>e.g.,</em>
            <a
            href="https://www.geeksforgeeks.org/program-for-fcfs-cpu-scheduling-set-1/">FIFO</a>,
            <a
            href="https://www.geeksforgeeks.org/program-for-round-robin-scheduling-for-the-same-arrival-time/">round
            robin</a>, <a
            href="https://dl.acm.org/doi/abs/10.1145/1460833.1460871">time
            sharing</a>, <em>etc.</em></p>
            <p>Here is a good comparison of the various types of
            (historical) CPU/OS schedulers: <a
            href="https://www.geeksforgeeks.org/cpu-scheduling-in-operating-systems/">CPU
            Scheduling in Operating Systems</a>.</p>
            <p>In the realm of real-time systems, to <em>formally</em>
            define a scheduling problem, we need to specify:</p>
            <ol type="1">
            <li>a set of <strong>tasks</strong>, <span
            class="math inline"><em>τ</em></span></li>
            <li>a set of <strong>processors</strong>, <span
            class="math inline"><em>P</em></span></li>
            <li>a set of <strong>resources</strong>, <span
            class="math inline"><em>R</em></span></li>
            </ol>
            <p>Hence, the <strong>general scheduling problem</strong>
            is,</p>
            <blockquote>
            <p>assign processors and resources to tasks, such that the
            following constraints are met:</p>
            <ul>
            <li>timing constraints</li>
            <li>precedence constraints</li>
            <li>resource constraints</li>
            </ul>
            </blockquote>
            <p>There is a <a
            href="https://link.springer.com/book/10.1007/978-1-4614-0676-1">large
            body of literature</a> in the domain of real-time scheduling
            algorithms. In this chapter, we will focus on a few of them,
            <em>viz.</em>,</p>
            <ul>
            <li>completely static → <em>e,g.,</em> <a
            href="#cyclic-executives">cyclic executives</a></li>
            <li><a href="#priority-based-schedulers">priority-based</a>
            → static (<em>e.g.,</em> RM) and dynamic (<em>e.g.,</em>
            EDF)</li>
            <li>dynamic best effort</li>
            </ul>
            <p>One of the main problems with the scheduling problem, as
            defined above (and in general), is that many variants of the
            problem are <strong>intractable</strong>, <em>i.e.,</em>
            NP-Hard or even NP-Complete.</p>
            <blockquote>
            <p>Recall that an NP-Hard problem is one where no
            <em>known</em> polynomial time algorithm exists. So, all
            known algorithms are superlinear or, usually, of
            <em>exponential</em> time complexity!</p>
            </blockquote>
            <p>[Will leave it to the reader to recall or look up the
            definition of NP-Complete.]</p>
            <p>Since the scheduling problems may not be tractable (or
            “solvable” in a realistic time frame), we need to find
            <em>heuristics</em> but they can be “sub-optimal”. Luckily,
            we have a couple of <strong>provably optimal</strong>
            real-time schedulers (in the single core domain).</p>
            <p><strong>Additional, important definitions</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 36%" />
            <col style="width: 63%" />
            </colgroup>
            <thead>
            <tr>
            <th>concept</th>
            <th>definition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>feasibility</strong></td>
            <td>schedule is feasible if all tasks can be completed by
            satisfying a set of specified constraints</td>
            </tr>
            <tr>
            <td><strong>schedulable</strong></td>
            <td>set of tasks is schedulable if there exists <strong>at
            least one algorithm</strong> that can produce a feasible
            schedule</td>
            </tr>
            <tr>
            <td><strong>schedulability analysis</strong></td>
            <td>analysis to confirm that deadlines for a set of threads
            can be guaranteed using a <strong>given scheduling
            policy</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <section id="cyclic-executives" class="level3"
            data-number="5.2.1">
            <h3 data-number="5.2.1"><span
            class="header-section-number">5.2.1</span> Cyclic
            Executives</h3>
            <p>In the automotive enginer example from earlier, we see
            that for each <strong>cycle</strong>, the same set of tasks
            <strong>repeat</strong> (<em>i.e.</em>., “periodic
            behavior”). Note though that the tasks <em>need not</em>
            execute in parallel – rather, they must execute sequentially
            for this application. Usually such applications use a
            scheduling mechanism known as a “<strong>cyclic
            executive</strong>”.</p>
            <p>Consider this simple example with three tasks:</p>
            <table>
            <thead>
            <tr>
            <th>task</th>
            <th>c</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>T</em><sub>1</sub></span></td>
            <td>1</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>T</em><sub>2</sub></span></td>
            <td>2</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>T</em><sub>3</sub></span></td>
            <td>3</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>How would we <strong>schedule</strong> this? Assuming a
            single processors (hence a single timeline).</p>
            <p><img src="img/scheduling/cyclic/pngs/cyclic1.png" width="400"></p>
            <p>Well, the <em>simplest</em> mechanism is to just use a
            <strong>sequential</strong> schedule,</p>
            <p><img src="img/scheduling/cyclic/pngs/cyclic4.png" width="400"></p>
            <p>If, as in the case of the engine control example we saw
            earlier, the tasks repeat <em>ad infinitum</em>, then we see
            the pattern also repeating…</p>
            <p><img src="img/scheduling/cyclic/pngs/cyclic5.png" width="400"></p>
            <p>Cyclic executives were common in many critical RTS, since
            they’re <strong>simple</strong> and
            <strong>deterministic</strong>. An implementation could look
            like,</p>
            <pre><code>while(1)    // an infinite loop
{
    // Some Initialization

    Task_T1() ;

    // Some processing, maybe

    Task_T2() ;

    // Some other processing, maybe

    Task_T3() ;

    // Cleanup
}</code></pre>
            <p><strong>Question</strong>: what problems, if any, can
            happen due to cyclic executives?</p>
            <p>The very simplicity of such systems can also be their
            biggest weakness.</p>
            <ol type="1">
            <li><p><strong>lack of flexibility</strong>: as the example
            and code above demonstrate, once a pattern of executions is
            set, it cannot be changed, <strong>unless the system is
            stopped, redesigned/recompiled and restarted</strong>! This
            may not be possible for critical applications. Even for the
            engine control application in cars, this doesn’t just mean
            stopping and restarting the car, but
            <strong>re-flashing</strong> the firmware for the engine,
            which is quite an involved task.</p></li>
            <li><p><strong>scalability</strong>: along similar lines, it
            is difficult to scale the system to deal with additional
            issues or add functionality.</p></li>
            <li><p><strong>priority</strong>: there is no way to assign
            priority or preemption since all tasks essentially execute a
            the <em>same priority</em>. Hence, if we want to deal with
            higher-priority events (<em>e.g.</em>, read a sensor) or
            even <em>atypical</em> (aperiodic/sporadic) events, such as
            sudden braking in an autonomous car, then a cyclic executive
            is not the right way to go about it.</p></li>
            <li><p><strong>resource management</strong>: certain tasks
            can corral resources and hold on to them while others may
            <em>starve</em> – leading to the system becoming unstable.
            For instance, even in the simple example, we see that <span
            class="math inline"><em>T</em><sub>3</sub></span> can
            dominate the execution time on the CPU:</p></li>
            </ol>
            <p><img src="img/scheduling/cyclic/cyclic6.svg" width="400"></p>
            <p>Since the system is <em>one giant executable</em>, it is
            difficult to stop a “runaway task” – the entire system must
            be stopped and restarted, which can lead to serious
            problems.</p>
            </section>
            <section id="frames" class="level3" data-number="5.2.2">
            <h3 data-number="5.2.2"><span
            class="header-section-number">5.2.2</span> Frames</h3>
            <p>One way to mitigate <em>some</em> of the problems with
            cyclic executives, is to split the resource allocation into
            “frames” → <strong>fixed</strong> chunks of time when a task
            can claim exclusive access to a resource, <em>e.g..</em> the
            processor:</p>
            <ul>
            <li>once a frame starts, the task gets to execute
            <em>uninterrupted</em></li>
            <li>at the end of the frame, the task <em>must give up</em>
            the resource → regardless of whether it was done or not</li>
            </ul>
            <p>So, if we revisit our simple example and break the
            processor schedule into frame sizes of <code>2</code> units,
            each,</p>
            <p><img src="img/scheduling/cyclic/cyclic6_5.frame.svg" width="400"></p>
            <blockquote>
            <p>why <code>2</code>? Well, it is arbitrary for now. But,
            as we shall see later, we can calculate a “good” frame
            size</p>
            </blockquote>
            <p>Now, our schedule looks like,</p>
            <p><img src="img/scheduling/cyclic/cyclic8.frame.svg" width="400"></p>
            <p>As we see from this picture, task <span
            class="math inline"><em>T</em><sub>1</sub></span> doesn’t
            end up using its entire frame and hence, can waste resources
            (one of the pifalls of this method).</p>
            <p>Continuing further,</p>
            <p><img src="img/scheduling/cyclic/cyclic9.frame.svg" width="400"></p>
            <p>Task <span
            class="math inline"><em>T</em><sub>3</sub></span> is
            <em>forced</em> to relinquish the processo at
            <code>t=6</code> even though it has some execution left → on
            account of the frame ending. Now <span
            class="math inline"><em>T</em><sub>1</sub></span> resumes in
            its own frame. <span
            class="math inline"><em>T</em><sub>3</sub></span> has to
            wait until <code>t=10</code> to resume (and complete) its
            execution:</p>
            <p><img src="img/scheduling/cyclic/cyclic12.frame.svg" width="400"></p>
            <p><strong>Other Static/Table-Driven
            Scheduling</strong>:</p>
            <p>Cyclic executives are an example of schedulers where the
            tasks are fixed, <em>ahead of time</em>, and all that a
            scheduler has to do is to <em>dispatch</em> them one at a
            time in the <strong>exact same order</strong>. Often the
            tasks are stored in a lookup table (hence the name
            “table-driven”). Other examples of such systems (with some
            prioritization and other features built in) have been built,
            <em>e.g.,</em> <a
            href="https://par.nsf.gov/servlets/purl/10383232">weighted
            round robin</a> → also finds use in cloud computing and
            networking load balancing, <em>etc.</em></p>
            </section>
            <section id="priority-based-schedulers" class="level3"
            data-number="5.2.3">
            <h3 data-number="5.2.3"><span
            class="header-section-number">5.2.3</span> Priority-Based
            Schedulers</h3>
            <p>One method that overcomes the disadvantages of a
            completely static method is to <strong>assign priorities for
            jobs as they arrive</strong>. Hence, when a job is
            <em>released</em> it gets assigned a
            <strong>priority</strong> and the scheduler then
            dispatches/schedules the job accordingly. Hence, it if is
            the highest priority job so far, it gets scheduled <em>right
            away</em>, by preempting any currenlty running tasks. If, on
            the other hand, it is not the highest priority task then it
            is inserted into the ready queue at the right position
            (priority level).</p>
            <p>To deal with this, we need an <strong>online
            scheduler</strong>, <em>i.e.,</em> one that is always
            available to make scheduling decisions – when tasks arrive,
            complete, miss their deadlines, <em>etc.</em></p>
            <p>Before we go any further, let’s <strong>update the task
            model</strong> a little, to make matters simple for us.</p>
            <ul>
            <li>as before, we have a task set comprised of <span
            class="math inline"><em>n</em></span>
            <strong>periodic</strong> tasks, <span
            class="math inline"><em>τ</em> = <em>τ</em><sub>1</sub>, <em>τ</em><sub>2</sub>...<em>τ</em><sub><em>n</em></sub></span></li>
            <li>deadline <strong>is equal to</strong> period,
            <em>i.e.,</em> <span
            class="math inline"><em>T</em> = <em>D</em></span>; task
            periods are <span
            class="math inline"><em>T</em><sub>1</sub>, <em>T</em><sub>2</sub>, ...<em>T</em><sub><em>n</em></sub></span></li>
            <li>all tasks are <strong>independant</strong> → no
            precedence or resource constraints exist</li>
            <li>tasks <strong>cannot suspend</strong> themselves (or
            others)</li>
            <li>tasks are <strong>preemptible</strong> by the OS → each
            time the highest priority task is executed (under preemptive
            scheduling)</li>
            <li>execution time of each task is <strong>bounded</strong>
            → wcet (<span
            class="math inline"><em>c</em><sub>1</sub>, <em>c</em><sub>2</sub>, ...<em>c</em><sub><em>n</em></sub></span>)</li>
            <li>tasks are released (<em>i.e.,</em> placed into the ready
            queue) <strong>as soon as they arrive</strong></li>
            <li>all kernel overheads (<em>e.g.,</em> context switches) →
            assumed to be <strong>zero</strong></li>
            </ul>
            <p>While these may seem to be overly simplifying, they still
            fit the model of many systems and help us develop
            <em>fundamental results</em>. And we can always add them
            back one-by-one and still retain the correctness of the
            theoretical results we develop, while making the system more
            realistic.</p>
            <p>Now, in the real of <strong>online,
            priority-driven</strong> schedulers, we have
            <strong>two</strong> options:</p>
            <table>
            <colgroup>
            <col style="width: 63%" />
            <col style="width: 36%" />
            </colgroup>
            <thead>
            <tr>
            <th>priority assignment</th>
            <th>algorithms</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>static</strong></td>
            <td><a
            href="#rate-monotonic-scheduler-rm">Rate-Monotonic</a> (RM),
            Deadline-Monotonic (DM)</td>
            </tr>
            <tr>
            <td><strong>dynamic</strong></td>
            <td><a
            href="#earliest-deadline-first-scheduler-edf">Earliest-Deadline
            First</a>, Least-Slack Time (LST)</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Let’s look at one of each (the most popular ones),
            <em>viz.</em> the <a
            href="#rate-monotonic-scheduler-rm">Rate-Monotonic</a> (RM)
            and <a
            href="#earliest-deadline-first-scheduler-edf">Earliest-Deadline
            First</a> schedulers. Note that both were first described
            and analyzed in a <strong>seminal Computer Science
            Paper</strong>, that has since become one of the most cited
            and influential papers in the field: <a
            href="https://dl.acm.org/doi/10.1145/321738.321743">Scheduling
            Algorithms for Multiprogramming in a Hard- Real-Time
            Environment</a> by Liu and Layland.</p>
            <p>Interestingly, both of these algorithms are
            <strong>provably optimal</strong>, <em>i.e.,</em> no other
            static or dynamic algorithm can do better than RM and EDF
            respectively! Not bad for the first paper in the area – talk
            about setting a high bar, or rather <em>bound</em>!</p>
            <section id="rate-monotonic-scheduler-rm" class="level4"
            data-number="5.2.3.1">
            <h4 data-number="5.2.3.1"><span
            class="header-section-number">5.2.3.1</span> Rate-Monotonic
            Scheduler (RM)</h4>
            <p>The Rate-Montonic priority assignment algorithm assigns
            <strong>priority based on the period of the task</strong> →
            shorter the period, the higher the priority!</p>
            <p>Consider the following example:</p>
            <table>
            <thead>
            <tr>
            <th>task</th>
            <th>c</th>
            <th>T</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>1</sub></span></td>
            <td>2</td>
            <td>4</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>2</sub></span></td>
            <td>2</td>
            <td>5</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>3</sub></span></td>
            <td>1</td>
            <td>10</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>So, based on the RM algorithm, the priority will be:</p>
            <p><span
            class="math display"><em>τ</em><sub>1</sub> &gt; <em>τ</em><sub>2</sub> &gt; <em>τ</em><sub>3</sub></span></p>
            <p>since, <span
            class="math inline"><em>T</em><sub>1</sub> &lt; <em>T</em><sub>2</sub> &lt; <em>T</em><sub>3</sub></span>.</p>
            <p>The question now is whether the above task set is
            <strong>schedulable</strong>? Let us use our previous
            utilization-based check, <em>i.e.</em>,</p>
            <p><span class="math display">$$U = \sum_{i=1}^{n}
            \frac{c_i}{T_i}$$</span></p>
            <p>So, plugging in the numbers, we get,</p>
            <p><span class="math display">$$ U = \frac{1}{2} +
            \frac{1}{4} + \frac{2}{6} = 0.5 + 0.4 + 0.1 = 1.0
            $$</span></p>
            <p>Our test was: <span
            class="math inline"><em>U</em> ≤ 1</span>. So, this task set
            is…schedulable? Let us see – by plotting it on the
            timeline:</p>
            <p><img src="img/scheduling/rm_unschedulable/pngs/rm.final.png" width="400" ></p>
            <p>As we see, task <span
            class="math inline"><em>τ</em><sub>3</sub></span> misses its
            deadline! In fact, with the other two tasks, <span
            class="math inline"><em>τ</em><sub>1</sub></span> and <span
            class="math inline"><em>τ</em><sub>2</sub></span> constantly
            executing, <span
            class="math inline"><em>τ</em><sub>3</sub></span> will
            <strong>never</strong> get to execute and
            <strong>all</strong> of its jobs will miss their
            deadlines!</p>
            <blockquote>
            <p>“Wait!”, you say. OK, <em>one</em> job has missed its
            deadline, maybe two (from the picture). So how can I make
            the bold claim that <em>all</em> will miss their deadlines?
            <br> If you pay close attention to the figure, I have only
            checked for <code>10</code> time steps. Why that number?
            Well it so happens that <code>20</code> is the
            <strong>LCM</strong> (lowest common multiple) of
            <strong>all</strong> the task periods, <code>4</code>,
            <code>5</code>, <code>10</code>. <br> Why do we care about
            the LCM? Turns out, in real-time scheduling, the LCM of the
            task periods have a special significance. Turns out that if
            we construct the schedule for <strong>one LCM</strong>
            nunber of time units, then the schedule <strong>repeats
            exactly</strong> after that! Hence, the exact same schedule
            repeats every LCM number of units. <br> The LCM of the
            constituent (periodic) tasks in the system is referred to as
            → <strong>hyperperiod</strong>. So, we only need to check
            our schedule for <strong>one hyperiod</strong>. If the task
            set is schedulable in that timeframe then it will be and, if
            not, it will not be. <br> So, for this example, I can state,
            with confidence, that <strong>all</strong> jobs of <span
            class="math inline"><em>τ</em><sub>3</sub></span> will miss
            their deadlines since within <strong>half</strong> of the
            hyperperiod, one job has missed its deadline!</p>
            </blockquote>
            <p>So, coming back to our analysis, we started with our
            utilization test <span
            class="math inline"><em>U</em> &lt; 1</span> which this task
            set, <em>passed</em>, yet it **failed to schedule*! So, it
            seems we need something <em>better</em>.</p>
            <p>Turns out the <a
            href="https://dl.acm.org/doi/10.1145/321738.321743">Liu and
            Layland paper</a> has figured this out. So they created
            another test, one based on: <strong>utilization upper
            bound</strong>. Since RM is a static priority assignment
            mechanism, there is a <strong>theoretical limit</strong> on
            the utilization for a task set, that <strong>depends on the
            number of tasks</strong>, <code>n</code>, in the system.</p>
            <p>So, we derive (I leave out the details here. Check the
            Liu and Layland paper for more details) another check for
            utilization,</p>
            <p><span class="math display">$$ U = \sum_{i=1}^n
            \frac{c_i}{T_i} \le n.(2^{\frac{1}{n}} -1) $$</span></p>
            <p>If the <strong>total</strong> utilization of the system
            is below this bound, then the task set is schedulable by RM.
            <strong>Note</strong> that this is a <em>necessary but
            <strong>not</strong> sufficient</em> test (more on that
            later).</p>
            <p>As we see from above, the value of the right hand side
            will change with the number of tasks in the system. Hence,
            with more tasks, the upper bound for <span
            class="math inline"><em>U</em></span> will reduce.</p>
            <blockquote>
            <p>let’s open up the simulator-plotter for checking this for
            various values of <code>n</code> and see for <span
            class="math inline"><em>n</em> = 1, 2, ...</span></p>
            </blockquote>
            <p>So, we see that</p>
            <p><span class="math display">$$n = 3\\
            U_{ub} \approx 0.78
            $$</span></p>
            <p>The utilization for our task set was: <span
            class="math inline">≈</span> <code>1.0</code> which is
            significantly higher! No wonder our task set wasn’t
            schedulable!</p>
            <p>Here is a plot that shows the values for different
            numbers of tasks:</p>
            <p><img src="img/scheduling/rm_util_bounds.png" width="400"></p>
            <p>As we see, the value keeps reducing. Does it keep going
            <em>all</em> the way down to zero? What if I schedule
            <code>100</code> tasks? A <code>1000</code>?</p>
            <p>Turns out, we can check! With the exact same
            equation.</p>
            <blockquote>
            <p>let’s open up the simulator-plotter for checking this for
            various values of <code>n</code> and see for <span
            class="math inline"><em>n</em> = 100, 1000, <em>e</em><em>t</em><em>c</em>.</span></p>
            </blockquote>
            <p><img src="img/scheduling/rm_util_bound_100.png" width="400"></p>
            <p>As we see from the figure (and the active plotting), the
            values seem to <em>plateau</em> out and converge…to
            <strong><code>0.69</code></strong>! So, for any real-time
            system scheduled using the RM assignment mechanism, if the
            utilization bound is under <code>0.69</code> then it is
            schedulable.</p>
            <p><strong>Optimality</strong>: as mentioned earlier, RM is
            <strong>optimal</strong>, <em>i.e.,</em></p>
            <ul>
            <li>if a task set is schedulable by RM → then there is no
            other <em>static</em> algorithm that can do better (in terms
            of utilization)</li>
            <li>if a task set is <em>not</em> schedulable by RM → there
            is <strong>no other <em>static</em> algorithm</strong> can
            schedule it</li>
            </ul>
            <section id="exact-response-time-analysis" class="level5"
            data-number="5.2.3.1.1">
            <h5 data-number="5.2.3.1.1"><span
            class="header-section-number">5.2.3.1.1</span> Exact
            (Response Time) Analysis</h5>
            <p>Now, let’s go back to one of our earlier examples (from
            the <a href="#cyclic-executives">cyclic executive</a>
            chapter):</p>
            <table>
            <thead>
            <tr>
            <th>task</th>
            <th>c</th>
            <th>T</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>1</sub></span></td>
            <td>1</td>
            <td>4</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>2</sub></span></td>
            <td>2</td>
            <td>6</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>3</sub></span></td>
            <td>3</td>
            <td>12</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>We added some period information to the tasks.</p>
            <p>We know that using the naive utilization test, <span
            class="math inline"><em>U</em> ≈ .0.833</span>. But, recall
            that the utilization bound test, for <span
            class="math inline"><em>n</em> = 3</span> tasks requires,
            <span class="math inline"><em>U</em> &lt; 0.78</span>. So,
            this task set must be <em>unschedulable</em>, right? Let’s
            draw it out on the timeline and see:</p>
            <p><img src="img/scheduling/rm_schedulable_example/rm.final.svg" width="400"></p>
            <p>Wait, this is <strong>schedulable</strong>? But it fails
            our test!</p>
            <p>This is why I referred to it as a <em>necessary but
            <strong>not</strong> sufficient</em> test. Hence, for the
            Liu and Layland utilization bound test,</p>
            <table>
            <thead>
            <tr>
            <th>result of test</th>
            <th>schedulable?</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>pass, <em>i.e.,</em> <span
            class="math inline"><em>U</em><sub><em>u</em><em>b</em></sub> &lt; 0.69</span></td>
            <td>yes</td>
            </tr>
            <tr>
            <td>fail, <span
            class="math inline"><em>U</em><sub><em>u</em><em>b</em></sub> &gt; 0.69</span></td>
            <td><strong>unsure</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>We we need a <em>better</em> test → <strong>Response Time
            Analysis</strong> (RTA):</p>
            <ul>
            <li>if <strong>all jobs</strong> of a task are able to
            <strong>complete before their respective deadlines</strong>
            → task set is schedulable</li>
            <li>caveat → we account for the
            <strong>interference</strong> (hence, delays) encountered by
            the jobs by <em>all</em> higher priority jobs</li>
            </ul>
            <p>Let’s look at it in more detail:</p>
            <ol type="1">
            <li><strong>worst-case</strong> response time of task, <span
            class="math inline"><em>τ</em><sub><em>i</em></sub></span>
            is</li>
            </ol>
            <p><span
            class="math display"><em>R</em><sub><em>i</em></sub> = <em>c</em><sub><em>i</em></sub> + <em>I</em><sub><em>i</em></sub></span></p>
            <p>where <span
            class="math inline"><em>I</em><sub><em>i</em></sub></span>
            is the <strong>interference</strong> faced by that job from
            <strong>all</strong> higher prioriy jobs until then.</p>
            <ol start="2" type="1">
            <li>For <em>each</em> higher priority job, <span
            class="math inline"><em>τ</em><sub><em>j</em></sub></span>,
            the number of <em>jobs</em> released during the time
            interval <span
            class="math inline"><em>R</em><sub><em>i</em></sub></span>
            is:</li>
            </ol>
            <p><span class="math display">$$\left\lceil \frac{R_i}{T_j}
            \right\rceil$$</span></p>
            <p>Since <em>each period</em> of task <span
            class="math inline"><em>τ</em><sub><em>j</em></sub></span>
            results in a new job being released. Since RM gives higher
            priority to shorter periods, those released jobs will
            execute ahead of the current teak, <span
            class="math inline"><em>τ</em><sub><em>i</em></sub></span>.</p>
            <ol start="3" type="1">
            <li>Since <span
            class="math inline">⌈<em>R</em><sub><em>i</em></sub>/<em>T</em><sub><em>j</em></sub>⌉</span>
            number of <span
            class="math inline"><em>τ</em><sub><em>j</em></sub></span>’s
            jobs execute before <span
            class="math inline"><em>τ</em><sub><em>i</em></sub></span>,
            the interference caused by all of them:</li>
            </ol>
            <p><span class="math display">$$I_j = \left\lceil
            \frac{R_i}{T_j} \right\rceil .\ c_j$$</span></p>
            <ol start="4" type="1">
            <li>the <strong>total</strong> interference then, is the sum
            of the individual interference by each of the higher
            priority jobs, <em>i.e.,</em></li>
            </ol>
            <p><span class="math display">$$I = \sum_{j\in
            hp(i)}\left\lceil \frac{R_i}{T_j} \right\rceil .\ c_j
            $$</span></p>
            <ol start="5" type="1">
            <li>Finally, the <strong>response time</strong> for task,
            <span
            class="math inline"><em>τ</em><sub><em>i</em></sub></span>
            must combine its own (worst-case) execution time with the
            total interference from <strong>all</strong> higher priority
            tasks,</li>
            </ol>
            <p><span
            class="math display"><em>R</em><sub><em>i</em></sub> = <em>c</em><sub><em>i</em></sub> + <em>I</em><sub><em>i</em></sub></span></p>
            <p><span class="math display">$$ R_i = c_i + \sum_{j\in
            hp(i)}\left\lceil \frac{R_i}{T_j} \right\rceil .\ c_j
            $$</span></p>
            <p>For each task, we carry out the above analysis → stop
            when consecutive iterations provide the
            <strong>same</strong> values.</p>
            <ol start="6" type="1">
            <li>At each stage, check if the response time for a task is
            less than or equal to its deadline</li>
            </ol>
            <p><span
            class="math display">∀<em>τ</em><sub><em>i</em></sub> : <em>R</em><sub><em>i</em></sub> &lt; <em>T</em><sub><em>i</em></sub></span></p>
            <p>If the above test passes for all tasks, then the task set
            is <strong>schedulable</strong> even if the earlier tests
            fail. Hence, this is both, <em>necessary
            <strong>and</strong> sufficient</em>.</p>
            <p><strong>Example</strong> [contd.]: Now, applying this to
            our errant example:</p>
            <ol type="1">
            <li><p>assign priorities: <span
            class="math inline"><em>τ</em><sub>1</sub> &gt; <em>τ</em><sub>2</sub> &gt; <em>τ</em><sub>3</sub></span></p></li>
            <li><p>response time calculations</p></li>
            </ol>
            <p>For each task, we calculate the response time using the
            above formula via iterative analysis.</p>
            <table>
            <colgroup>
            <col style="width: 11%" />
            <col style="width: 26%" />
            <col style="width: 30%" />
            <col style="width: 30%" />
            </colgroup>
            <thead>
            <tr>
            <th>task</th>
            <th>iteration</th>
            <th>calculations</th>
            <th><span
            class="math inline"><em>R</em><sub><em>i</em></sub> &lt; <em>T</em><sub><em>i</em></sub></span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>1</sub></span></td>
            <td>1</td>
            <td><span
            class="math inline"><em>R</em><sub>1</sub> = <em>c</em><sub>1</sub> = 1</span></td>
            <td>yes [ <span class="math inline">1 &lt; 4</span> ]</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>2</sub></span></td>
            <td>1</td>
            <td><span
            class="math inline"><em>R</em><sub>2</sub><sup>0</sup> = <em>c</em><sub>2</sub> = 2</span>
            <br> <span class="math inline">$R_2^1 = c_2 +
            \lceil\frac{R_2^0}{T_1}\rceil c_1$</span> <bR> <span
            class="math inline">$R_2^1 = 2 + \lceil\frac{2}{4}\rceil
            \cdot 1 = 3$</span></td>
            <td></td>
            </tr>
            <tr>
            <td></td>
            <td>2</td>
            <td><span class="math inline">$R_2^2 = 2 +
            \lceil\frac{3}{4}\rceil \cdot 1 = 3$</span> [stop]</td>
            <td>yes [ <span class="math inline">3 &lt; 6</span> ]</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>3</sub></span></td>
            <td>1</td>
            <td><span
            class="math inline"><em>R</em><sub>3</sub><sup>0</sup> = <em>c</em><sub>3</sub> = 3</span>
            <br> <span class="math inline">$R_3^1 = c_3 +
            \lceil\frac{R_3^0}{T_1}\rceil c_1 +
            \lceil\frac{R_3^0}{T_2}\rceil c_2$</span> <br> <span
            class="math inline">$R_3^1 = 3 + \lceil\frac{3}{4}\rceil
            \cdot 1 + \lceil\frac{3}{6}\rceil \cdot 2 = 6$</span></td>
            <td></td>
            </tr>
            <tr>
            <td></td>
            <td>2</td>
            <td><span class="math inline">$R_3^2 = 3 +
            \lceil\frac{6}{4}\rceil \cdot 1 + \lceil\frac{6}{6}\rceil
            \cdot 2 = 8$</span></td>
            <td></td>
            </tr>
            <tr>
            <td></td>
            <td>3</td>
            <td><span class="math inline">$R_3^3 = 3 +
            \lceil\frac{8}{4}\rceil \cdot 1 + \lceil\frac{8}{6}\rceil
            \cdot 2 = 8$</span> [stop]</td>
            <td>yes [ <span class="math inline">8 &lt; 12</span> ]</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Since the response time of <strong>all</strong> tasks
            meet their deadlines under RM scheduling, therefore the task
            set is <strong>schedulable</strong>.</p>
            <p>The response time analysis algorithm:</p>
            <p><img src="img/scheduling/response_time_algo.png" width="500"></p>
            <p><br></p>
            <p>There exist many of static assignment algorithms, for
            instance the <a
            href="http://www.cs.csi.cuny.edu/~yumei/csc744/Examples/realtimetasks.pdf">Deadline
            Monotonic Scheduler</a> where priorities assigned to
            processes are <strong>inversely proportional to the length
            of the deadline</strong>.</p>
            <p><strong>Resources</strong>:</p>
            <ol type="1">
            <li>to read more about the schedulability analysis details
            or other types of schedulers, see: <a
            href="https://www.eecs.umich.edu/courses/eecs571/lectures/lecture5-schedule2.pdf">Priority-Driven
            Scheduling</a>.</li>
            <li><a
            href="https://iris.unimore.it/bitstream/11380/1118762/4/POST_PRINT_JSA2016Priority.pdf">A
            Review of Priority Assignment in Real-Time Systems</a> by
            Davis et al.</li>
            </ol>
            </section>
            </section>
            <section id="earliest-deadline-first-scheduler-edf"
            class="level4" data-number="5.2.3.2">
            <h4 data-number="5.2.3.2"><span
            class="header-section-number">5.2.3.2</span>
            Earliest-Deadline First Scheduler (EDF)</h4>
            <p>The problem with static priority assignments, is that
            they are typically <em>non-optimal</em>. Wait, but we said
            earlier that RM <em>is</em> optimal? Well, among static
            priority assignment algorithms RM is optimal but, as we saw
            from the analyses and the graphs, upper bounds for tasksets
            often taper off at <span
            class="math inline"><em>U</em> = 0.69</span>. While we can
            create task sets with higher utilization (as the
            response-time analyis shows us), it takes a lot of manual
            effort to create such task sets. This means, that in the
            <em>common</em> case, we are <strong>wasting nearly
            <code>31 %</code> utilization</strong>!</p>
            <p>A dynamic assignment of priorities can help mitigate
            these problems and ensure that we make better use of the
            system resources. Many dynamic scheduling algorithms have
            been proposed, <em>e.g.</em>, <a
            href="https://d-nb.info/1240614632/34">FIFO</a> and <a
            href="https://www.javatpoint.com/least-slack-time-lst-scheduling-algorithm-in-real-time-systems">Least
            Slack Time</a> among others.</p>
            <p>In this section we will focus on <strong>the</strong>
            priority assignment mechanism for real-time systems which,
            actually, is one of the mainline schedulers in Linux, named
            <a
            href="https://docs.kernel.org/scheduler/sched-deadline.html"><code>SCHED_DEADLINE</code></a>.</p>
            <p>In a nutshell, as the name implies, EDF assigns priority
            based on the <strong>absolute deadline</strong> of the job.
            From <a
            href="https://en.wikipedia.org/wiki/Earliest_deadline_first_scheduling">Wikipedia</a>:
            &gt; Whenever a scheduling event occurs (task finishes, new
            task released, etc.) the queue will be searched for the
            process closest to its deadline. This process is the next to
            be scheduled for execution.</p>
            <p>While the exact priority of a job depends on its
            deadline, the latter are calculated statically. Hence, even
            though jobs of a task may have different priorities,
            <strong>each</strong> job has only one priority level. As
            jobs complete, the priorities of the remaining (or new,
            incoming) jobs change.</p>
            <p><strong>EDF Schedulability Test</strong>: is the
            <strong>same</strong> as the first test we discussed,
            <em>i.e.,</em></p>
            <p><span class="math display">$$ U = \sum_{i=1}^{n}
            \frac{c_i}{T_i} \le 1 $$</span></p>
            <p>As long as we’re following the task model described
            earlier, this is a <strong>very simple</strong> check to
            test for the validity of the task set for EDF.</p>
            <p>Let’s revisit our example from RM that was <em>not</em>
            schedulable, _viz.,</p>
            <table>
            <thead>
            <tr>
            <th>task</th>
            <th>c</th>
            <th>T</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>1</sub></span></td>
            <td>2</td>
            <td>4</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>2</sub></span></td>
            <td>2</td>
            <td>5</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>τ</em><sub>3</sub></span></td>
            <td>1</td>
            <td>10</td>
            </tr>
            </tbody>
            </table>
            <p>Before we proceed, we need to decide on some
            policies:</p>
            <ul>
            <li>two jobs have the <strong>same</strong> deadlines → pick
            the one that was released <em>first</em></li>
            <li>same deadline <strong>and</strong> released at the same
            time++ → pick one with the smaller index?</li>
            </ul>
            <p>These are some simple rules that help in simplifying the
            decision-making process. We can change them, as long as it
            helps us make forward progress and we remain
            <em>consistent</em>.</p>
            <p>++ in this scenario, ideally we should redesign the
            system to combine these two tasks – if they always run
            together, then why pay the extra costs of
            preemption/context-switch, <em>etc.</em>?</p>
            <p>For the above task set, we see that,</p>
            <p><span class="math display">$$U = \frac{2}{4} +
            \frac{2}{5} + \frac{1}{10} = 0.5 + 0.4 + 0.1 = 1 \le
            1$$</span></p>
            <p>Hence, this task set is <strong>schedulable</strong>
            using EDF (just barely though since its utilization is
            <code>1</code>!). At least theoretically! Remember that this
            task set <em>failed</em> for RM.</p>
            <p>The schedule diagram for this task set looks like,</p>
            <p><img src="img/scheduling/edf/pngs/edf.final.png" width="400"></p>
            <p>So far, it looks schedulable. It is left as an exercise
            to the reader to check until the hyperperiod
            (<code>20</code>).</p>
            <p><strong>Optimality</strong>: EDF is an <strong>optimal
            scheduling algorithm</strong> → if the task set is
            schedulable by some algorithm, it is also schedulable under
            EDF.</p>
            <p>EDF can definitely <em>squeeze</em> out the maximum from
            a system – as we just saw, it can even schedule task sets
            with the theoretical utilization vound (<code>1</code>)!</p>
            </section>
            <section id="rm-vs.-edf" class="level4"
            data-number="5.2.3.3">
            <h4 data-number="5.2.3.3"><span
            class="header-section-number">5.2.3.3</span> RM vs. EDF</h4>
            <p>So, let’s compare the two superstars of the real-time
            scheduling world, RM and EDF. The ✓ symbol indicates which
            one is better.</p>
            <table>
            <thead>
            <tr>
            <th>parameter</th>
            <th>RM</th>
            <th>EDF</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>optimality</td>
            <td>✓ (static)</td>
            <td>✓</td>
            </tr>
            <tr>
            <td>context switch overheads</td>
            <td>✓</td>
            <td>✗</td>
            </tr>
            <tr>
            <td>preemptions</td>
            <td>✗</td>
            <td>✓</td>
            </tr>
            <tr>
            <td>analysis overheads</td>
            <td>✗</td>
            <td>✓</td>
            </tr>
            <tr>
            <td>utilization</td>
            <td>✗</td>
            <td>✓</td>
            </tr>
            <tr>
            <td>implementation ease</td>
            <td>✓</td>
            <td>✗</td>
            </tr>
            <tr>
            <td>predictability</td>
            <td>✓</td>
            <td>✗</td>
            </tr>
            <tr>
            <td><strong>total</strong></td>
            <td><scb>4</scb></td>
            <td><scb>4</scb></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><strong>Other Issues</strong>: note that we have mostly
            considered a simple task model. But these may vary in real
            world systems, <em>e.g.,</em></p>
            <ul>
            <li>$ D &lt;T $, <span
            class="math inline"><em>D</em> &gt; <em>T</em></span> →
            essentially situations where deadlines and periods don’t
            align; there are other analyses methods that can be
            applied</li>
            <li><strong>multicore</strong> → so far we have only
            discussed single core systems; scheduling across multiple
            cores is an NP-complete problem but many heuristics have
            been developed; see <a
            href="https://dl.acm.org/doi/pdf/10.1145/1978802.1978814">A
            Survey of Hard Real-Time Scheduling for Multiprocessor
            Systems</a> for a summary of many results</li>
            <li><strong>priority inheritance protocols</strong> → for
            dealing with resource contentions; again a large body of
            work exists in this domain; see <a
            href="https://www.eecs.umich.edu/courses/eecs571/lectures/lecture6-schedule3.pdf">Lecture
            Note #6</a> for a good summary.</li>
            <li><strong>power consumption</strong> and scheduling →
            scheduling methods can help alleviate power consumption
            issues; see <a
            href="https://www.eecs.umich.edu/courses/eecs571/lectures/lecture12-rtdvs.pdf">Real-Time
            Dynamic Voltage Scaling for Low-Power Embedded Operating
            Systems</a> for one of the early results.</li>
            <li><strong><a
            href="https://link.springer.com/article/10.1007/s11241-012-9158-9">memory</a></strong>,
            <strong><a
            href="https://sibin.github.io/papers/2017_RTSS_SDNQoS_Rakesh.pdf">networks</a></strong>,
            <strong><a
            href="https://www.computer.org/csdl/journal/td/2017/05/07728147/13rRUwj7coW">networks-on-chip</a></strong>,
            <strong><a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9773317">5G</a></strong>,
            <em>etc.,</em> → real-time scheduling has been applied to
            wide variety of domains.</li>
            </ul>
            <p><br> <br> <br> <br> <br> <br> <br> <br> <br> <br></p>
            <!--rel="stylesheet" href="./custom.sibin.css"-->
            </section>
            </section>
            </section>
            </section>
            <section id="control-theory" class="level1" data-number="6">
            <h1 data-number="6"><span
            class="header-section-number">6</span> Control Theory</h1>
            <p>Consider a simple problem → how do you balance a
            ball?</p>
            <p><img src="img/controls/soccer_ball_balance.gif"></p>
            <p><br></p>
            <p>I guess that’s more complicated than what we wanted! So,
            let’s make it really simple and try in a <em>one dimensional
            plane</em>, as follows:</p>
            <p><img src="img/controls/ball/ball.unstable.gif" width="300"></p>
            <p>We want to balance the ball in the <em>middle</em> of the
            table. And the ball moves either left or right, based on how
            we <em>tilt</em> the table.</p>
            <p>As we see from this picture, a naive attempt at balancing
            the ball can quickly make it “<em>unstable</em>”. But, our
            objective, is to make sure that,</p>
            <ul>
            <li>the ball remains <strong>stable</strong> and</li>
            <li>it is in the <strong>middle</strong> of the table</li>
            </ul>
            <p>The options that are available to us are:</p>
            <ol type="1">
            <li>tilt the table down on the left (anti-clockwise)</li>
            <li>title the table down on the right (clockwise)</li>
            </ol>
            <p>We also have the ability to control the <em>speed</em> at
            which the table tilts to either side. We can actually
            combine these, as we shall see.</p>
            <p>Hence, the parameters for the problem are:</p>
            <table>
            <thead>
            <tr>
            <th>type</th>
            <th>options</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>inputs</td>
            <td>speed (clockwise, anticlockwise)</td>
            </tr>
            <tr>
            <td>output</td>
            <td>ball velocity, acceleration</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Some how, we need to <em>control</em> the outputs by
            modifying the inputs to the system. Enter <strong>control
            theory</strong>.</p>
            <section id="control-theory-introduction" class="level2"
            data-number="6.1">
            <h2 data-number="6.1"><span
            class="header-section-number">6.1</span> Control Theory |
            Introduction</h2>
            <p>Control theory is a <em>multidisciplinary</em> field at
            the intersection of applied mathematics and engineering.
            Engineering fields that heavily utilize control theory
            include mechanical, aerospace, electrical and chemical
            engineering. Control theory has been applied to the
            biological sciences, finance, you name it.</p>
            <p>Anything that you,</p>
            <ul>
            <li><strong>want to control</strong> and</li>
            <li>can <strong>develop a model</strong></li>
            </ul>
            <p>you can develop a “<em>controller</em>” for managing it,
            using the tools of control theory.</p>
            <p>In our everyday life, we interact with technologies that
            utilize control theory. They appear in applications like
            adaptive cruise control, thermostats, ovens and even lawn
            sprinkler systems. The field of control theory is huge and
            there’s a wide range of subdisciplines.</p>
            <p>The basic idea behind control theory is to <em>understand
            a process or a system</em> by developing a model for it that
            represents,</p>
            <blockquote>
            <p>the <strong>relationships between inputs and outputs for
            the system</strong></p>
            </blockquote>
            <p>We then use this model to <strong>adjust the
            inputs</strong> → to get <strong>desired outputs</strong>.
            The relationship between the inputs and outputs is usually
            obtained through empirical analysis, <em>viz.,</em>,</p>
            <ol type="1">
            <li>make changes to the input</li>
            <li>wait for the system to respond</li>
            <li>observe changes in the output.</li>
            </ol>
            <p>Even if the model is based on an equation from physics,
            the parameters within the model are still identified
            experimentally or through computer simulations.</p>
            <p>We repeat the experiments/simulations as needed to
            “understand” the system as well as we can, in ordero to
            develop the model. Once the model has been developed, we
            develop a <strong>control model</strong> that can used to
            tune the input → output relationship.</p>
            <p>In effect, we are <em>inverting</em> the original model
            (input → output) to develop,</p>
            <blockquote>
            <p>control model: input ← output</p>
            </blockquote>
            <p>To better understand this, consider the example of a
            light bulb and switch:</p>
            <p><img src="img/controls/lightbulb.png" width="300"></p>
            <p>Even if we didn’t know the relationship between the
            switch and bulb, we can conduct a few experiments to figure
            out the following:</p>
            <table>
            <thead>
            <tr>
            <th>switch state <br> (input)</th>
            <th>bulb state <br> (output)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>off</td>
            <td>off</td>
            </tr>
            <tr>
            <td>on</td>
            <td>on</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Now we have our “model” of input (switch state) → output
            (ligthbulb state). This model works as as no
            <em>external</em> disturbances occur (power failure or bulb
            burn out).</p>
            <p>But, this is <em>not</em> our control model. For that, we
            need to <strong>invert</strong> the model we’ve built so
            far.</p>
            <p>So, we start with the <strong>desired output
            state</strong>, <em>e.g.,</em> the “lightbulb must be on”.
            Then, we reason backwards to: “<em>what should the input be
            to achieve this desired state?</em>”. Should the switch be
            <code>on</code> or <code>off</code>?</p>
            <p>From our original model (and experiments), we have
            created the I/O relationship table above. Hence, it stands
            to reason that we can “invert” it as:</p>
            <table>
            <colgroup>
            <col style="width: 51%" />
            <col style="width: 48%" />
            </colgroup>
            <thead>
            <tr>
            <th>desired output <br> lightbulb state</th>
            <th>corresponding input <br> switch state</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>on</td>
            <td>on</td>
            </tr>
            <tr>
            <td>off</td>
            <td>off</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Now, let’s <strong>formalize</strong> things a
            little.</p>
            <p>Consider the following mathematical model that describes
            the behavior of a system:</p>
            <p><img src="img/controls/equations/svgs/equations.002.svg" width="300"></p>
            <p>The model says that if we change the input <code>u</code>
            the output <code>y</code> will change to be
            <strong>twice</strong> the value of the input
            <code>u</code>.</p>
            <p>Now, in control theory, we are concerned about how to
            <strong>get to a specific output</strong>. Hence, if we want
            to reach a specific value of <code>y</code>, say →
            <strong><span
            class="math inline"><em>y</em><sup>*</sup></span></strong>,
            we need to <em>manipulate the model</em> to now create a
            “control model”, <em>i.e.,</em></p>
            <p><span class="math display">$$u =
            \frac{y^*}{2}$$</span></p>
            <p>This model says for any value of the output <span
            class="math inline"><em>y</em><sup>*</sup></span> that we
            want, we can identify the input <code>u</code> → essentially
            dividing <span
            class="math inline"><em>y</em><sup>*</sup></span> by
            <code>2</code>. Notice that this equation is now <strong>in
            terms of <code>u</code></strong> → we have our
            <strong>control law</strong>! Restating the obvious, this is
            an “inverse” of the original model of the system.</p>
            <p>We have just developed our <strong>first
            controller</strong>! The desired value, <span
            class="math inline"><em>y</em><sup>*</sup></span> is
            referred to as the <strong>setpoint</strong>. We get to the
            setpoint by picking the right value for <code>u</code>.</p>
            <p>Developing a control law, in a sense, is inverting your
            model to rewrite the output in terms of the input so that
            you can get the output you want. More complex systems lead
            to more complicated control laws. Practitioners have
            developed methods of developing control laws for systems
            whose models cannot be cleanly inverted, <em>e.g.,</em> such
            as nonlinear systems or systems with dead times.</p>
            <p>For context, this is where we are in this course
            <em>map</em>:</p>
            <p><img src="img/stack_architecture/stack_overview.6.png" width="300"></p>
            <section id="open-loop-vs-closed-loop-control"
            class="level3" data-number="6.1.1">
            <h3 data-number="6.1.1"><span
            class="header-section-number">6.1.1</span> Open-Loop vs
            Closed-Loop Control</h3>
            <p>For the control law we just developed, if our model is
            accurate and there are no disturbances then,</p>
            <p><span
            class="math display"><em>y</em> = <em>y</em><sup>*</sup></span></p>
            <p>However, note that there is nothing ensuring that the
            value of <span
            class="math inline"><em>y</em> = <em>y</em><sup>*</sup></span>.
            We just assume (rather, <em>expect</em>) that it would be
            the case. This is known as an <strong>open loop
            controller</strong>. You desire a certain output and hope
            that the controller actually gets there.</p>
            <p>So, the open loop controller is depicted as:</p>
            <p><img src="img/controls/controls_openloop/controls.openloop.svg"></p>
            <p>What we really want, is to somehow <em>ensure</em> that
            the controllers gets to its setpoint. How do we do that?</p>
            <p>The problem is that while the input drives the output,
            there is no way to <em>guarantee</em> that the controller
            will get to the set point.</p>
            <p>What we really need, is a <strong>closed-loop
            controller</strong> → one that uses
            <strong>feedback</strong> to,</p>
            <ul>
            <li><strong>adjust <code>u</code></strong></li>
            <li>ensure that we get to <span
            class="math inline"><em>y</em><sup>*</sup></span> (or, at
            least as close to it as possible).</li>
            </ul>
            <p>The feedback typically comes from the output of the
            controller model that we created. So,</p>
            <p><img src="img/controls/controls_closedloop/controls.closedloop.final.svg"></p>
            <p>Note that the feedback can be positive or negative.</p>
            <p>[The above description is distillation of the <a
            href="https://www.reddit.com/r/ControlTheory/comments/lqjgb3/comment/gogu5pu/?utm_source=share&amp;utm_medium=web3x&amp;utm_name=web3xcss&amp;utm_term=1&amp;utm_content=share_button">excellent
            description found here</a>.]</p>
            </section>
            </section>
            <section id="feedback-control" class="level2"
            data-number="6.2">
            <h2 data-number="6.2"><span
            class="header-section-number">6.2</span> Feedback
            Control</h2>
            <p>[The following material is thanks to <a
            href="https://ece.ubc.ca/sathish-gopalakrishnan/">Prof. Sathish
            Gopalakrishan</a>, Univ. of British Columbia].</p>
            <p>Consider the following problem (that we increasingly
            encounter in the real world):</p>
            <blockquote>
            <p>how do you ensure that a car remains in the
            <em>center</em> of its lane?</p>
            </blockquote>
            <p>So, we have a car moving on the road, thus:</p>
            <p><img src="img/controls/feedback_lane/car_road.1.png" width="200"></p>
            <p><br></p>
            <p>the blue arrow shows the direction of motion of the car.
            Hence, for the car to remain in the <em>center</em> of the
            lane, we need to apply a <strong>correction</strong> to its
            direction of motion,</p>
            <p><img src="img/controls/feedback_lane/car_road.2.png" width="200"></p>
            <p><br></p>
            <p>There are some questions that come up:</p>
            <ul>
            <li><strong>how</strong> do we apply the corrections?</li>
            <li><strong>how much</strong> and</li>
            <li><strong>when</strong> do we <strong>stop</strong>?</li>
            </ul>
            <p>Enter <strong>feedback control</strong>:</p>
            <blockquote>
            <ul>
            <li><em>compare</em> system state to the desired state</li>
            <li>apply a <em>change</em> to the system inputs →
            counteract the deviations</li>
            <li>repeat until desired outcome → setpoint</li>
            </ul>
            </blockquote>
            <p><strong>Example</strong>: let’s see how feedback control
            can be applied to a temperature control of a room.</p>
            <p>Given a “desired” room temperature (as input to a
            thermostat), what do we need to consider while attempting to
            achieve this temperature?</p>
            <p><img src="img/controls/feedback_temp/temperature.1.png" width="200"></p>
            <p>The thermostat needs to control/provide inputs to a
            furnace/AC,</p>
            <p><img src="img/controls/feedback_temp/temperature.2.png" width="300"></p>
            <p>which then affects the temperature in the room:</p>
            <p><img src="img/controls/feedback_temp/temperature.3.png" width="400"></p>
            <p>Easy! Done…right?</p>
            <p>Except, the real world is far from ideal. We have to deal
            with disturbances…</p>
            <p><img src="img/controls/feedback_temp/vader_disturbance_force.jpg" width="300"></p>
            <p><br></p>
            <p>Well not that kind of disturbance, but pesky issues like
            heat loss, bad insulation and physical problems in
            general:</p>
            <p><img src="img/controls/feedback_temp/temperature.4.png" width="500"></p>
            <p>As we see from the picture, we may not get to the
            expected behavior due to external factors. So, as before,
            just the input will not suffice to reach the setpoint.</p>
            <p>So, we provide “feedback” to the controller:</p>
            <p><img src="img/controls/feedback_temp/temperature.5.png" width="500"></p>
            <p>Essentially the temperature reading of the room,
            <em>after</em> the thermostat and furnace/AC have completed
            their operations based on the original inputs (desired
            temperature).</p>
            <p>Let’s introduce some <em>generic</em> technical terms for
            each of these components:</p>
            <p><img src="img/controls/feedback_temp/temperature.6.png" width="500"></p>
            <p>The “controller” is based on the “control model” that we
            developed earlier. It sends commands (“<em>actuation
            signals</em>”) to an actuator and then affects the
            <em>process under control</em>. Finally, the <em>process
            variable</em> (the “output” from the earlier discussions) is
            what we want to drive towards the set point.</p>
            <p><strong>Note</strong>: in the case that feedback is not
            possible, there is work on → <a
            href="https://www.electronics-tutorials.ws/systems/open-loop-system.html">open-loop
            control</a> and <a
            href="https://web.stanford.edu/class/archive/ee/ee392m/ee392m.1034/Lecture5_Feedfrwrd.pdf">feedforward
            control</a>.</p>
            <p>Another example → cruise control.</p>
            <p><img src="img/controls/feedback_temp/cruise_control.png" width="500"></p>
            <p>Note how the feedback reaches the controller in this
            case.</p>
            <p>So, at a high-level, a <strong>closed-loop feedback
            control system</strong> looks like,</p>
            <p><img src="img/controls/closed_loop_feedback.1.png" width="400"></p>
            <p>Some of these inputs/edges have <em>specific</em>
            names:</p>
            <p><img src="img/controls/closed_loop_feedback.2.png" width="400"></p>
            <p><strong>Note:</strong> the main goal → <strong>error is
            minimized</strong> (ideally <code>0</code>).</p>
            <p>A more formal definition of the same quantities,</p>
            <p><img src="img/controls/closed_loop_feedback.3.png" width="400"></p>
            <table>
            <thead>
            <tr>
            <th>quantity</th>
            <th>definition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>r</em>(<em>t</em>)</span></td>
            <td><strong>reference</strong>/set point</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>e</em>(<em>t</em>)</span></td>
            <td><strong>error</strong></td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>u</em>(<em>t</em>)</span></td>
            <td><strong>control signal</strong>/“input”</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>y</em>(<em>t</em>)</span></td>
            <td>(expected/final) <strong>output</strong></td>
            </tr>
            <tr>
            <td><span class="math inline">$\overline{y(t)}$</span></td>
            <td>“feedback”/<strong>estimate</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Now let’s apply this feedback control model to the
            earlier problem of lane following.</p>
            </section>
            <section id="feedback-control-applied-to-lane-following"
            class="level2" data-number="6.3">
            <h2 data-number="6.3"><span
            class="header-section-number">6.3</span> Feedback Control
            Applied to Lane Following</h2>
            <p>Recall that we want to keep the car in the center of its
            lane:</p>
            <p><img src="img/controls/feedback_lane/car_road.2.png" width="100"></p>
            <p>But here’s a question → <em>how do you find the
            <strong>center</strong> of the lane?</em></p>
            <p>Consider a road with lane markings on either side,</p>
            <p><img src="img/controls/feedback_lane/lane.1.png" width="300"></p>
            <p>Now, let’s assume that some system (say, a camera), can
            track the yellow lines to an extent. We need to find the
            center of the lane, as marked in the figure:</p>
            <p><img src="img/controls/feedback_lane/lane.2.png" width="400"></p>
            <p>Based on the <em>two</em> points marked up ahead (that we
            can detect to be on the same plane), we can calculate,</p>
            <p><span class="math display">$$x_{center} =
            \frac{x_{left\_end}+x_{right\_end}}{2}$$</span></p>
            <p>Now, a car need not be in the actual center of the
            lane,</p>
            <p><img src="img/controls/feedback_lane/lane.3.png" width="400"></p>
            <p>Now, assuming that the camera is mounted at the center of
            the car,</p>
            <p><img src="img/controls/feedback_lane/lane.4.png" width="300"></p>
            <p>The car’s position can be calculated as:</p>
            <p><span class="math display">$$x_{car} =
            \frac{width}{2}$$</span></p>
            <p>From this we can calculate the <strong>cross-track
            error</strong> (CTE),</p>
            <p><span
            class="math display"><em>C</em><em>T</em><em>E</em> = <em>x</em><sub><em>c</em><em>a</em><em>r</em></sub> − <em>x</em><sub><em>c</em><em>e</em><em>n</em><em>t</em><em>e</em><em>r</em></sub></span></p>
            <p>What happens when,</p>
            <ul>
            <li><span
            class="math inline"><em>C</em><em>T</em><em>E</em> &gt; 0</span></li>
            <li><span
            class="math inline"><em>C</em><em>T</em><em>E</em> &lt; 0</span>?</li>
            </ul>
            <p>Now, back to our original problem → keeping the car in
            the center of the lane. We do this by → <strong>keep CTE as
            small as possible</strong> and applying
            <strong>corrections</strong>,</p>
            <p><img src="img/controls/feedback_lane/car_road.2.png" width="100"></p>
            <p>The $64,000 question is: <strong>how</strong>?</p>
            <p>Answer: <strong>feedback control</strong>!</p>
            <p>Recall the various components of the feedback
            control:</p>
            <p><img src="img/controls/closed_loop_feedback.2.png" width="200">
            <img src="img/controls/closed_loop_feedback.3.png" width="200"></p>
            <p><br></p>
            <p>Now, let’s map the lane following components on to
            this:</p>
            <p><img src="img/controls/feedback_lane/feedback.1.png" width="400"></p>
            <p>As we see from the figure, the <strong>lane following
            controller</strong> sends the control/actuation signal to
            the steering unit. Sensors (perhaps a camera equipped with
            vision algorithms in this case) provide
            <strong>feedback</strong> to the controller (<span
            class="math inline">$\overline y_t$</span>). Mapping this
            back to the lane variables and CTE,</p>
            <p><img src="img/controls/feedback_lane/feedback.2.png" width="400"></p>
            <p>This figure shows the important part → the <strong>CTE is
            the feedback</strong> for the lane following controller! The
            <strong>input</strong> then is the negative error,
            <em>i.e.,</em> the goal is to reduce the CTE. Also note that
            the <em>output</em> of the controller is the
            <strong>steering input</strong>.</p>
            <p>So, let’s focus in on how the controller operates,
            <em>i.e.,</em> this part:</p>
            <p><img src="img/controls/feedback_lane/feedback.3.png" width="400"></p>
            <p><br></p>
            <p>Problem statement:</p>
            <blockquote>
            <p>given the CTE, how do we compute the
            <strong>control</strong> signal so that the car <em>stays in
            the middle of the lane</em>?</p>
            </blockquote>
            <p>The final “corrections”, when applied, may look something
            like this:</p>
            <p><img src="img/controls/feedback_lane/car_road.3.png" width="100"></p>
            </section>
            <section id="pid-control" class="level2" data-number="6.4">
            <h2 data-number="6.4"><span
            class="header-section-number">6.4</span> PID Control</h2>
            <p>Let’s start with one goal → <strong>lateral position
            control</strong>:</p>
            <table>
            <tbody>
            <tr>
            <td>process variable</td>
            <td><span
            class="math inline"><strong>y(t)</strong></span></td>
            <td><span class="math inline"><em>y</em></span> position at
            time, <span class="math inline"><em>t</em></span></td>
            </tr>
            <tr>
            <td>goal</td>
            <td><span class="math inline"><em>y</em> = 0</span></td>
            <td>keep the car at position <code>0</code></td>
            </tr>
            <tr>
            <td>control signal</td>
            <td><span
            class="math inline"><em>u</em>(<em>t</em>)</span></td>
            <td>steering</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Let’s say we have the car’s <em>start</em> and
            <em>end</em> position,</p>
            <p><img src="img/controls/feedback_lane/lateral_axis.2.png" width="400"></p>
            <p>And we know the relationship between <span
            class="math inline"><em>u</em>(<em>t</em>)</span> and <span
            class="math inline"><em>y</em>(<em>t</em>)</span>:</p>
            <p><img src="img/controls/feedback_lane/lateral_axis.3.png" width="200"></p>
            <p>Basically we want <span
            class="math inline"><em>u</em>(<em>t</em>)</span> to be
            negative → so that <span
            class="math inline"><em>y</em></span> tends towards its
            eventual goal, <span
            class="math inline"><em>y</em> = 0</span>.</p>
            <p>So, what should our <strong>control input</strong>, <span
            class="math display"><em>e</em>(<em>t</em>) = ?</span></p>
            <p><br></p>
            <p>As we see below, we want the input to be a <em>decreasing
            value of the feedback</em>,</p>
            <p><span
            class="math display"><em>e</em>(<em>t</em>) = −<em>y</em>(<em>t</em>)</span></p>
            <p><br></p>
            <p><img src="img/controls/feedback_lane/lateral_axis.4.png" width="400"></p>
            <p>This is called <strong>proportional (P)
            control</strong>.</p>
            <section id="proportional-p-control" class="level3"
            data-number="6.4.1">
            <h3 data-number="6.4.1"><span
            class="header-section-number">6.4.1</span> Proportional (P)
            Control</h3>
            <p>The correction is <strong>proportional</strong> to the
            <strong>size of the error</strong>, <em>i.e.,</em></p>
            <p><img src="img/controls/feedback_lane/p_control.2.png" width="400"></p>
            <p>So, going back to our example of lateral control, let us
            try to apply the proportional control methodology to it:</p>
            <p><img src="img/controls/feedback_lane/p_lateral_axis.1.png" width="400"></p>
            <p>We have the following choices:</p>
            <ul>
            <li><span
            class="math inline"><em>K</em><sub><em>p</em></sub> &gt; 0</span></li>
            <li><span
            class="math inline"><em>K</em><sub><em>p</em></sub> &lt; 0</span></li>
            </ul>
            <p>We pick the <span
            class="math inline"><em>K</em><sub><em>p</em></sub> &gt; 0</span>
            since we want the following relationship to hold (following
            from <span
            class="math inline"><em>e</em>(<em>t</em>) = −<em>y</em>(<em>t</em>)</span>):</p>
            <p><span
            class="math display"><em>K</em><sub><em>p</em></sub><em>e</em>(<em>t</em>) = −<em>K</em><sub><em>p</em></sub><em>y</em>(<em>t</em>)</span></p>
            <p><img src="img/controls/feedback_lane/p_lateral_axis.2.png" width="400"></p>
            <p><br></p>
            <p>We see that this proportional controller helps us move
            the car towards our goal, <span
            class="math inline"><em>y</em> = 0</span>.</p>
            <p>Now, let’s consider a few situations:</p>
            <ol type="1">
            <li>what happens if <span
            class="math inline"><em>K</em><sub><em>p</em></sub></span> →
            <strong>too small</strong> (small <em>“gain”</em>)?</li>
            </ol>
            <p><img src="img/controls/feedback_lane/p_lateral_axis.3.png" width="400"></p>
            <p><br></p>
            <p>The response is <strong>too slow</strong>/gradual. We may
            <em>never</em> get to the goal in this case.</p>
            <ol start="2" type="1">
            <li>what happens if <span
            class="math inline"><em>K</em><sub><em>p</em></sub></span> →
            <strong>too large</strong> (large <em>“gain”</em>)?</li>
            </ol>
            <p><img src="img/controls/feedback_lane/p_lateral_axis.4.png" width="400"></p>
            <p><br></p>
            <p>The response is <strong>too sudden</strong>. The system
            may <strong>overshoot</strong> the goal!</p>
            <p>So, the next question that comes up → <em>can the car be
            stabilized at <span
            class="math inline"><em>y</em> = 0</span>?</em> This is
            <strong>unlikely</strong> using just the proportional
            control method since,</p>
            <table>
            <thead>
            <tr>
            <th>gain</th>
            <th>effect</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>small</td>
            <td><strong>stead-state error</strong></td>
            </tr>
            <tr>
            <td>large</td>
            <td><strong>oscillations</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>The question then becomes → <em>how can we <strong>reduce
            oscillation</strong></em>, <em>i.e.</em>, can we get to the
            following situation (smoother, <em>actual</em> approach to
            the goal)?</p>
            <p><img src="img/controls/feedback_lane/d_lateral_axis.1.png" width="400"></p>
            </section>
            <section id="derivative-d-control" class="level3"
            data-number="6.4.2">
            <h3 data-number="6.4.2"><span
            class="header-section-number">6.4.2</span> Derivative (D)
            Control</h3>
            <p>Derivative control <strong>improves the dynamic
            response</strong> of the system by,</p>
            <ul>
            <li>studying the <strong>rate of change</strong> of the
            error and</li>
            <li><strong>decreasing oscillation</strong></li>
            </ul>
            <p><br></p>
            <p><img src="img/controls/feedback_lane/d_control.2.png" width="400"></p>
            <p>So, what does this mean, in practice? What we’re
            measuring and trying to control, is the <strong>rate of
            change</strong>, <em>i.e.,</em> the change from,</p>
            <p><span
            class="math display"><em>y</em>(<em>t</em> − 1) → <em>y</em>(<em>t</em>)</span></p>
            <p><br></p>
            <p><img src="img/controls/feedback_lane/d_lateral_axis.2.png" width="400"></p>
            <p>Typically, proportional and derivative control are often
            <strong>used together</strong>, as a way to counteract each
            others’ influences. So, we get:</p>
            <p><img src="img/controls/equations/svgs/equations.004.svg" width="400"></p>
            <p>As with the proportional controller, we have two options
            for the derivative as well. Should,</p>
            <ul>
            <li><span class="math inline">$\frac{dy(t)}{dt} &lt;
            0$</span></li>
            <li><span class="math inline">$\frac{dy(t)}{dt} &gt;
            0$</span></li>
            </ul>
            <p>Note that the derivative controller’s job is to steer
            <strong>away</strong> from the reference line, to act as a
            counter to the proportional controller pushing in the
            opposite direction. Hence, we pick <span
            class="math inline">$\frac{dy(t)}{dt} &lt; 0$</span>.</p>
            <p>The derivative controller acts like a
            <strong>brake</strong> and counteracts the correctional
            force. It reduces overshoot by <strong>slowing the
            correctional factor</strong> → as the reference goal
            approaches.</p>
            <p><img src="img/controls/feedback_lane/pd_lateral_axis.1.png" width="400"></p>
            <p>We see numerous uses of the combination, known as the
            <strong>P-D</strong> controllers in every day life, from the
            smallest to the largest, even rocket ships!</p>
            <video controls width="500">
            <source src="img/controls/feedback_lane/SpaceX_landing.mp4">
            </video>
            <section id="tuning-p-d-controllers" class="level4"
            data-number="6.4.2.1">
            <h4 data-number="6.4.2.1"><span
            class="header-section-number">6.4.2.1</span> Tuning P-D
            Controllers</h4>
            <p>Consider the following values for the two coefficients,
            <span
            class="math inline"><em>K</em><sub><em>p</em></sub></span>
            and <span
            class="math inline"><em>K</em><sub><em>d</em></sub></span>:</p>
            <p><img src="img/controls/feedback_lane/pd_graph.1.png" width="150">
            <img src="img/controls/feedback_lane/pd_graph.2.png" width="150">
            <img src="img/controls/feedback_lane/pd_graph.3.png" width="150"></p>
            <p>As we see, tuning <span
            class="math inline"><em>K</em><sub><em>d</em></sub></span>
            has a significant impact! The oscillations pretty much go
            away and we <em>quickly</em> get to the reference line with
            very little oscillation. Of course, the car overshoots a
            little but the combination of P-D brings it back soon
            enough.</p>
            <p>An interesting question arises → <em>what if we increase
            the value of <span
            class="math inline"><em>K</em><sub><em>d</em></sub></span> –
            eventually making it <strong>very large</strong>?</em></p>
            <p><img src="img/controls/feedback_lane/pd_graph.4.png" width="150">
            <img src="img/controls/feedback_lane/pd_graph.5.png" width="150">
            <img src="img/controls/feedback_lane/pd_graph.6.png" width="150"></p>
            <p>While we see from the first graph (<span
            class="math inline"><em>K</em><sub><em>p</em></sub> = 0.2</span>,
            <span
            class="math inline"><em>K</em><sub><em>d</em></sub> = 4.0</span>)
            that the oscillations have gone away, increasing <span
            class="math inline"><em>K</em><sub><em>d</em></sub></span>
            further makes the situation worse – it drives the car
            <em>away</em> from the reference goal!</p>
            <p>How do we deal with this?</p>
            <p>Well, by <strong>tuning</strong> the paramters, of
            course! For instance, in the last case, if we make a change
            <em>K_p = 3.0</em>,</p>
            <p><img src="img/controls/feedback_lane/pd_graph.6.png" width="150">
            →
            <img src="img/controls/feedback_lane/pd_graph.7.png" width="150"></p>
            <p>We see a quick, “smooth” path to the reference!</p>
            <p>In fact, a lot of the design of control systems involves
            the tuning of such parameters, depending on the system, to
            get things “just right”.</p>
            </section>
            </section>
            <section id="integral-i-control" class="level3"
            data-number="6.4.3">
            <h3 data-number="6.4.3"><span
            class="header-section-number">6.4.3</span> Integral (I)
            Control</h3>
            <p>Are we done? Not quite. Let’s take a closer look at the
            results:</p>
            <p><img src="img/controls/feedback_lane/i.graph.2.png" width="400"></p>
            <p>As we see from this image, even though we reached the
            reference, the behavior is <strong>not smooth</strong>!
            There could be many reasons for this, such as <em>steering
            drift</em>, caused by the mechanical design of the
            system:</p>
            <p><img src="img/controls/feedback_lane/i_control.1.png" width="200">
            <img src="img/controls/feedback_lane/i_control.2.png" width="150"></p>
            <p><br></p>
            <p>There are a variety of <strong>unmodeled
            disturbances</strong> and <strong>systemic errors</strong>
            (<em>“bias”</em>):</p>
            <ul>
            <li>actuators and processes → not ideal</li>
            <li>friction, steering, drift, changing workloads,
            misalignments, <em>etc.</em></li>
            </ul>
            <p>Hence, the signal <strong>may never reach</strong> the
            set points! It will end up “settling” near the reference,
            which is not always ideal.</p>
            <p>To deal with this, we need <strong>integral</strong> (I)
            control.</p>
            <p>First, let’s define <strong>steady state error</strong>
            (SSE):</p>
            <blockquote>
            <p>difference between the reference and the steady-state
            process variable</p>
            </blockquote>
            <p>Hence, when time goes to <em>infinity</em>,</p>
            <p><span
            class="math display"><em>S</em><em>S</em><em>E</em> = lim<sub><em>x</em> → ∞</sub>[<em>r</em>(<em>t</em>) − <em>y</em>(<em>t</em>)]</span></p>
            <p>The correction must be <strong>proportional</strong> to
            both → <strong>error</strong> and <strong>duration</strong>
            of the error. Essentially it <strong>sums the error over
            time</strong>.</p>
            <p><img src="img/controls/feedback_lane/i_control.4.png" width="400"></p>
            <p><br></p>
            <p><strong>Note:</strong> unless the error is <em>zero</em>,
            this term will <strong>grow</strong>! In fact, the error
            keeps adding up, so the <strong>correction must also
            increase</strong> → this drives the steady state error to
            <strong>zero</strong>.</p>
            <p><img src="img/controls/feedback_lane/i.graph.4.png" width="400"></p>
            <p>In many instances, integral control is used <strong>in
            conjunction with</strong> the P and D controllers,</p>
            <p><img src="img/controls/equations/svgs/equations.006.svg" width="500"></p>
            <p><br></p>
            <p>This is known as: <strong>PID Control</strong>,</p>
            <p><img src="img/controls/feedback_lane/pid_control.2.png" width="400"></p>
            <p>Let’s look at some examples of tuning the various
            paramters of a PID controller, as applied to our
            problem:</p>
            <p><img src="img/controls/feedback_lane/pid_graph.1.png" width="200">
            <img src="img/controls/feedback_lane/pid_graph.2.png" width="200"></p>
            <p><br></p>
            <p>Let’s increase <span
            class="math inline"><em>K</em><sub><em>p</em></sub></span>
            now and see the effect:</p>
            <p><img src="img/controls/feedback_lane/pid_graph.3.png" width="200"></p>
            <p>We see that the system has stabilized well around the
            reference point and, if we zoom in, we will see fewer
            disturbances.</p>
            <p>Now, let’s keep increasing <span
            class="math inline"><em>K</em><sub><em>p</em></sub></span>,</p>
            <p><img src="img/controls/feedback_lane/pid_graph.4.png" width="200">
            <img src="img/controls/feedback_lane/pid_graph.5.png" width="200"></p>
            <p>Wait, the signal <em>oscillates</em>? The main reason is
            that the I term is not zero when crossing the reference,
            <span class="math inline"><em>y</em>(<em>t</em>) = 0</span>
            and it takes a little while to wash out the cumulative
            error.</p>
            <p>In summary:</p>
            <ul>
            <li><strong>P</strong> is required</li>
            <li>depending on the system, one or both of
            <strong>I</strong>/<strong>D</strong> is combined with
            <strong>P</strong>
            <ul>
            <li><strong>PI</strong>, <strong>PD</strong> or
            <strong>PID</strong></li>
            </ul></li>
            </ul>
            <p><strong>Tuning</strong> P, I, D gains. There is no
            “optimal” way to tune the PID gains</p>
            <ol type="1">
            <li>start with → <span
            class="math inline"><em>K</em><sub><em>p</em></sub> = 0</span>,
            <span
            class="math inline"><em>K</em><sub><em>d</em></sub> = 0</span>,
            <span
            class="math inline"><em>K</em><sub><em>i</em></sub> = 0</span></li>
            <li><strong>slowly</strong> increase <span
            class="math inline"><em>K</em><sub><em>p</em></sub></span>
            until → system oscillates around set point</li>
            <li>slowly increase <span
            class="math inline"><em>K</em><sub><em>d</em></sub></span>
            until → system settles around set point</li>
            <li>if steady-state error exists → slowly increase <span
            class="math inline"><em>K</em><sub><em>i</em></sub></span>
            until corrected without causing additional oscillations</li>
            </ol>
            </section>
            <section id="some-additional-feedback-control-applications"
            class="level3" data-number="6.4.4">
            <h3 data-number="6.4.4"><span
            class="header-section-number">6.4.4</span> Some additional
            feedback control applications:</h3>
            <ol type="1">
            <li>Segway balance control</li>
            </ol>
            <video controls width="500">
            <source src="img/controls/feedback_lane/segway.mp4">
            </video>
            <ol start="2" type="1">
            <li>Drone control</li>
            </ol>
            <video controls width="500">
            <source src="img/controls/feedback_lane/drone_PID.mp4">
            </video>
            <ol start="3" type="1">
            <li>Motor speed control</li>
            </ol>
            <video controls width="500">
            <source src="img/controls/feedback_lane/motor_speed_control.mp4">
            </video>
            <p><strong>References</strong>:</p>
            <ol type="1">
            <li>Control theory introductions: <a
            href="https://www.basicknowledge101.com/pdf/control/Control%20theory.pdf">1</a>,
            <a
            href="https://engineeringmedia.com/controlblog/what-is-control-engineering">2</a>,
            <a
            href="https://www.basicknowledge101.com/pdf/control/Control%20system.pdf">3</a></li>
            <li><a href="https://engineeringmedia.com/maps">Map of
            Control Theory</a></li>
            <li><a
            href="https://www.mathworks.com/discovery/pid-control.html">What
            is PID Control</a> by Mathworks</li>
            <li><a
            href="https://www.youtube.com/watch?v=oBc_BHxw78s&amp;list=PLUMWjy5jgHK1NC52DXXrriwihVrYZKqjk">Control
            Theory Lectures</a> by Brian Douglas.</li>
            <li><a
            href="https://www.eecs.umich.edu/courses/eecs571/reading/control-to-computer-zaher.pdf">Introduction
            to Control Theory and Application to Computing Systems</a>
            by Abdelzaher et al.</li>
            <li><a
            href="https://www.sciencedirect.com/topics/engineering/automotive-control-system">Automotive
            Control Systems</a></li>
            <li><a
            href="https://ctms.engin.umich.edu/CTMS/index.php?example=Introduction&amp;section=ControlPID">PID
            Controller Design</a></li>
            <li><a
            href="https://www.digikey.com/en/maker/projects/introduction-to-pid-controllers/763a6dca352b4f2ba00adde46445ddeb">Introduction
            to PID controllers</a></li>
            <li><a
            href="https://www.diva-portal.org/smash/get/diva2:1373784/FULLTEXT01.pdf.">Construction
            and theoretical study of a ball balancing platform</a> by
            Frank and TJERNSTRÖM.</li>
            <li><a
            href="https://acrome.net/post/understanding-pid-control-using-2-dof-ball-balancer-experiments">Understanding
            PID Control: 2-DOF Ball Balancer Experiments</a></li>
            <li><a
            href="https://web.stanford.edu/class/archive/ee/ee392m/ee392m.1034/">Control
            Engineering for Industry</a> from Stanford University.</li>
            <li><a href="http://robinhsieh.com/line-following-robot/">A
            Line Following Robot Using PID Controller</a></li>
            <li><a
            href="https://ctms.engin.umich.edu/CTMS/index.php?example=BallBeam&amp;section=SystemModeling">Ball
            and Beam: System Modeling</a></li>
            <li><a
            href="https://www.geeksforgeeks.org/open-loop-control-system/">Open
            Loop Control
            System</a><!--rel="stylesheet" href="./custom.sibin.css"--></li>
            </ol>
            </section>
            </section>
            </section>
            <section id="actuation" class="level1" data-number="7">
            <h1 data-number="7"><span
            class="header-section-number">7</span> Actuation</h1>
            <p>A controller will typically generate a <em>control
            signal</em> which, in many physical systems, is used to
            “<strong>actuate</strong>” a physical component –
            <em>i.e.,</em> make it move.</p>
            <p>An actuator, then, is a part of a device or machine that
            helps it to achieve physical movements by converting energy,
            such as electrical, air or hydraulic, into mechanical force.
            Simply put, it is the component in any machine that enables
            movement. They’re like muscles on a human body – converting
            energy to <strong>physical</strong> action. Actuators are
            present in almost every machine around us, from simple
            electronic access control systems, the vibrator on your
            mobile phone and household appliances to vehicles,
            industrial devices, and robots. Common examples of actuators
            include electric motors, stepper motors, jackscrews,
            electric muscular stimulators in robots, etc.</p>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/izeXcf5qu6s?si=yohPUDy1Uf10mges" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
            </iframe>
            <p><a
            href="https://www.progressiveautomations.com/pages/actuators">Defined
            simply</a>, an actuator is a device that converts energy,
            which may be electric, hydraulic, pneumatic, etc., to
            mechanical in such a way that it can be
            <strong>controlled</strong>. The quantity and the nature of
            input depend on:</p>
            <ul>
            <li>the kind of energy to be converted and</li>
            <li>function of the actuator.</li>
            </ul>
            <p><img src="img/actuation/electric_actuator_diagram.png" width=300></p>
            <p><br></p>
            <p><a
            href="https://www.norgren.com/en-us/support/blog/what-is-an-electric-actuator">Electric</a>
            and piezoelectric actuators, for instance, work on the input
            of electric current or voltage, for hydraulic actuators, its
            incompressible liquid, and for pneumatic actuators, the
            input is air. The <strong>output is always mechanical
            energy</strong>.</p>
            <p>They are responsible for ensuring a device such as a
            robotic arm is able to move when electric input is provided.
            A car uses actuators in the engine control system to
            regulate air flaps for torque and optimization of power,
            idle speed and fuel management for ideal combustion.</p>
            <p>An actuator requires,</p>
            <ul>
            <li>a control device (which provides control signal)
            and</li>
            <li>a source of energy.</li>
            </ul>
            <p>The displacement achieved is commonly linear or
            rotational, as exemplified by</p>
            <ul>
            <li>linear motors and</li>
            <li>rotary motors.</li>
            </ul>
            <p>Another broad classification of actuators separates them
            into two types:</p>
            <ol type="1">
            <li>continuous-drive actuators and</li>
            <li>incremental-drive actuators (<em>e.g.,</em>
            <strong>stepper motors</strong>).</li>
            </ol>
            <p><img src="img/actuation/Electric_motor.gif"></p>
            <p><strong>Brushed DC motors</strong> move
            <strong>continuously</strong> when DC voltage is applied to
            their terminals.</p>
            <p><strong>Stepper motors</strong> are a variant of motors,
            named <strong>brushless motors</strong>, that rotate in a
            series of small and discrete angular steps. Stepper motors
            can be set to any given step position <strong>without
            needing a position sensor for feedback</strong>. step
            position can be rapidly increased or decreased to create
            continuous rotation, or the motor can be ordered to actively
            hold its position at one given step. Motors vary in size,
            speed, step resolution and torque.</p>
            <p>The stepper motor is known for its property of converting
            a <strong>train of input pulses (typically square waves)
            into a precisely defined increment</strong> in the shaft’s
            rotational position. Each pulse rotates the shaft through a
            fixed angle.</p>
            <p><img src="img/actuation/200px-StepperMotor.gif"></p>
            <p>[From <a
            href="https://en.wikipedia.org/wiki/Stepper_motor">Wikipedia</a>:
            Animation of a simplified stepper motor turned on,
            attracting the nearest teeth of the gear-shaped iron rotor -
            with the teeth aligned to electromagnet 1, they will be
            slightly offset from right electromagnet (2) - Frame 2: The
            top electromagnet (1) is turned off, and the right
            electromagnet (2) is energized, pulling the teeth into
            alignment with it. This results in a rotation of 3.6° in
            this example. - Frame 3: The bottom electromagnet (3) is
            energized; another 3.6° rotation occurs. - Frame 4: The left
            electromagnet (4) is energized, rotating again by 3.6°.</p>
            <p>When the top electromagnet (1) is again enabled, the
            rotor will have rotated by one tooth position; since there
            are 25 teeth, it will take 100 steps to make a full rotation
            in this example.]</p>
            <p><strong><a
            href="https://control.com/technical-articles/understanding-the-basicsof-pulse-width-modulation-pwm/">Motor
            Control</a></strong>: motor speed and direction are dictated
            by the voltage applied – change or reverse the polarity of
            the voltage and the motor will respond in a similar fashion.
            Voltage can be changed by raising the series resistance
            within the electrical circuit, which in turn lowers the
            current through the motor. This change in voltage can be
            accomplished by series resistors, potentiometers or
            rheostats. While these devices may be effective for small
            changes in voltage, the power and torque of the motor are
            decreased as the current drops. In addition, significant
            resistance from these devices can <em>produce a lot of
            heat</em> which could damage other devices within the
            electrical system.</p>
            <p>A more efficient way to vary voltage is to use a
            <strong><a href="#pulse-width-modulation">PWM
            controller</a></strong>.</p>
            <section id="pulse-width-modulation" class="level2"
            data-number="7.1">
            <h2 data-number="7.1"><span
            class="header-section-number">7.1</span> Pulse Width
            Modulation</h2>
            <p>Digital Signals are represented as <code>0</code> and
            <code>1</code>. Analog signals, on the other hand, have a
            greater range of values, often continuous in nature (as we
            saw in the bit about <a
            href="#analog-to-digital-convertors-adcs">ADC</a>s). To
            control a physical/“analog” device using a microcontroller,
            we need to do the opposite → <strong>convert from digital to
            analog</strong> signal.</p>
            <p><img src="img/actuation/analog-vs-digital-signal.png" width="300"></p>
            <p>Some microcontrollers have an onboard <a
            href="https://www.whathifi.com/advice/dacs-what-is-a-dac-and-do-you-need-one">digital-to-analog
            converter</a> (DAC) to output a true analog signal in order
            to control analog devices and we can even use an external
            DAC. But a DAC is relatively expensive to produce in terms
            of cost and it also takes up a lot of silicon area. To
            overcome these issues and to easily achieve the
            functionality of a DAC in a much more cost-efficient way, we
            use <strong>pulse-width modulation</strong> (PWM).</p>
            <p>PWM is a method to control analog devices using digital
            signals. We output an “<strong>analog-like signal</strong>”
            from the microcontroller that can then control motors,
            lights and various actuators.</p>
            <p><strong>Note</strong>: the PWM is <strong>not</strong> a
            true analog signal, just a digital one modified to
            <strong>behave</strong> like one. It is essentially a
            <strong>rectangular wave</strong> with varying “duty cycle”
            and periods.</p>
            <p>In the following <a
            href="https://en.wikipedia.org/wiki/Pulse-width_modulation">example</a>,
            an idealized inductor is driven by a set of voltage pulses
            (in <font color="blue">blue</font>) that result in a
            sine-wave-like current (in
            <font color="red">red</font>).</p>
            <p><img src="img/actuation/PWM,_3-level.svg.png" width="300"></p>
            <p>PWM is useful for controlling the
            <strong>average</strong> power or amplitude delivered by an
            electrical signal. The average value of voltage (and
            current) fed to the load is controlled by switching the
            supply between 0 and 100% at a rate faster than it takes the
            load to change significantly. The longer the switch is on,
            the higher the total power supplied to the load.</p>
            <p><strong>Example</strong>: Consider the following analogy.
            Imagine you have a ceiling fan that has just an off-on
            switch, <em>i.e.,</em> it is either stationary or goes to
            <span class="math inline">100%</span>.</p>
            <p><img src="img/actuation/ceiling_fan_cartoon.gif"></p>
            <p>What if I say: <em>I want the fan to operate at <span
            class="math inline">50%</span>?</em> The only “control” you
            have is the on-off switch. Can you do it?</p>
            <p>Solution:</p>
            <ul>
            <li>turn <code>on</code> switch</li>
            <li>wait till fan reaches <span
            class="math inline">50%</span> (or close to it)</li>
            <li>turn it <code>off</code></li>
            <li>when it starts to slow down → turn it <code>on</code>
            again</li>
            <li>repeat <strong>fast enough</strong> and you get close to
            the <span class="math inline">50%</span></li>
            <li>the faster you do this → closer to the desired value
            (aka <strong>setpoint</strong>)</li>
            </ul>
            <p>Ideally I don’t recommend doing this…</p>
            <p><img src="img/actuation/ceiling_fan_cartoon-meme.gif" width="300"></p>
            <p><br></p>
            <p>So a PWM “wave” looks like:</p>
            <p><img src="img/actuation/PWM-signal-pulse-time-period.png" width="300"></p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <tbody>
            <tr>
            <td><code>on</code>-<code>off</code> switching</td>
            <td><strong>pulse</strong></td>
            </tr>
            <tr>
            <td>duration for which the pulse is held at a
            <strong>high</strong> state</td>
            <td><strong>pulse width</strong></td>
            </tr>
            <tr>
            <td><span class="math inline"><em>T</em></span></td>
            <td><strong>period</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>A PWM wave has two important properties that needs to be
            tuned:</p>
            <ol type="1">
            <li><a href="#duty-cycle">duty cycle</a></li>
            <li><a href="#pwm-period">period</a> → one complete cycle of
            the signal/pulse.</li>
            </ol>
            <section id="duty-cycle" class="level3" data-number="7.1.1">
            <h3 data-number="7.1.1"><span
            class="header-section-number">7.1.1</span> Duty Cycle</h3>
            <p>Recall that logic <strong>high</strong> → <code>on</code>
            (or <code>off</code> depending on the system, but pick one
            for consistency). To represent an <code>on</code> time, we
            use the concept of the <strong>duty cycle</strong>, defined
            as:</p>
            <blockquote>
            <p>duty cycle describes the proportion of ‘on’ time to the
            regular interval or ‘period’ of time.</p>
            </blockquote>
            <p>Duty cycles are represented as percentages (of time that
            the signal is <code>on</code>, relative to its period).</p>
            <p><img src="img/actuation/PWM-different-duty-cycles-average-voltage.jpg.webp" width="240">
            <img src="img/actuation/Duty_Cycle_Examples.png" width="200"></p>
            <p>The duty cycle can be calculated as:</p>
            <p><span class="math display">$$D = \frac{T_{on}}{T} *
            100$$</span></p>
            <p>where,</p>
            <table>
            <tbody>
            <tr>
            <td><strong>D</strong></td>
            <td>duty cycle (percentage)</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>T</em><sub><em>o</em><em>n</em></sub></span></td>
            <td>duration when signal is <code>on</code></td>
            </tr>
            <tr>
            <td><span class="math inline"><em>T</em></span></td>
            <td>total period</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Consider the periodic pulse wave, <span
            class="math inline"><em>f</em>(<em>t</em>)</span>, with a
            low value, <span
            class="math inline"><em>y</em><sub>min</sub></span>, high
            value, <span
            class="math inline"><em>y</em><sub>max</sub></span>, and
            constant duty cycle, <span
            class="math inline"><em>D</em></span>, as shown below:</p>
            <p><img src="img/actuation/Duty_cycle_general.svg.png" width="300"></p>
            <p><br></p>
            <p>The <strong>average</strong> value of a wave is,</p>
            <p><span class="math display">$$\bar{y} =
            \frac{1}{T}\int^T_0f(t)\,dt$$</span></p>
            <p>Since <span
            class="math inline"><em>f</em>(<em>t</em>)</span> is a pulse
            wave, its value is,</p>
            <table>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>y</em><sub><em>m</em><em>a</em><em>x</em></sub></span></td>
            <td><span
            class="math inline">0 &lt; <em>t</em> &lt; <em>D</em>.<em>T</em></span></td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>y</em><sub><em>m</em><em>i</em><em>n</em></sub></span></td>
            <td><span
            class="math inline"><em>D</em>.<em>T</em> &lt; <em>t</em> &lt; <em>T</em></span></td>
            </tr>
            </tbody>
            </table>
            <p>Now, we can expand the above expression as,</p>
            <p><span class="math display">$$
            \begin{align*}
              \bar{y} &amp;= \frac{1}{T} \left(\int_0^{DT}
            y_\text{max}\,dt + \int_{DT}^T y_\text{min}\,dt\right)\\
                      &amp;= \frac{1}{T} \left(D \cdot T \cdot
            y_\text{max} + T\left(1 - D\right) y_\text{min}\right)\\
                      &amp;= D\cdot y_\text{max} + \left(1 - D\right)
            y_\text{min}
            \end{align*}
            $$</span></p>
            <p>So we can now compute how long the signal should be at
            <span
            class="math inline"><em>y</em><sub><em>m</em><em>a</em><em>x</em></sub></span>
            and how much at <span
            class="math inline"><em>y</em><sub><em>m</em><em>i</em><em>n</em></sub></span>.</p>
            </section>
            <section id="pwm-period" class="level3" data-number="7.1.2">
            <h3 data-number="7.1.2"><span
            class="header-section-number">7.1.2</span> PWM Period</h3>
            <p>The period (or frequency, recall that <span
            class="math inline">$f=\frac{1}{T}$</span>) is another
            important parameter that defines a PWM signal. It
            essentially determines <em>the number of times a signal
            repeats per second</em>. The choice of <span
            class="math inline"><em>T</em></span> depends heavily on the
            application. For instance, when <a
            href="https://www.circuitbread.com/ee-faq/what-is-a-pwm-signal">controlling
            an LED</a>,</p>
            <blockquote>
            <p>the frequency of a PWM signal should be sufficiently high
            if we wish to see a proper dimming effect while controlling
            LEDs. A duty cycle of 20% at 1 Hz will be noticeable to the
            human eye that the LED is turning ON and OFF. However, if we
            increase the frequency to 100Hz, we’ll get to see the proper
            dimming of the LED.</p>
            </blockquote>
            </section>
            <section id="pwm-sampling-theorem" class="level3"
            data-number="7.1.3">
            <h3 data-number="7.1.3"><span
            class="header-section-number">7.1.3</span> PWM Sampling
            Theorem</h3>
            <p>Is directly related to the <a
            href="https://autonomy-course.github.io/textbook/autonomy-textbook.html#adc-sampling-rate">Nyquist-Shannon
            Sampling Theorem</a> discussed earlier. A simple
            summary,</p>
            <blockquote>
            <p>number of pulses in the waveform is equal to the number
            of Nyquist samples.</p>
            </blockquote>
            <p>Recall that the Nyquist rate is, <em>a value equal to
            <strong>twice</strong> the highest frequency</em>.</p>
            </section>
            <section id="example-servo-motor-control" class="level3"
            data-number="7.1.4">
            <h3 data-number="7.1.4"><span
            class="header-section-number">7.1.4</span> Example | Servo
            Motor Control</h3>
            <p>Servos (also RC servos) are small, cheap, mass-produced
            servomotors or other actuators used for radio control and
            small-scale robotics.</p>
            <p><img src="img/actuation/Micro_servo.png" width="300"></p>
            <p>They’re controlled by sending the servo a PWM
            (pulse-width modulation) signal, a series of repeating
            pulses of variable width where either the width of the pulse
            (most common modern hobby servos) or the duty cycle of a
            pulse train (less common today) determines the position to
            be achieved by the servo.</p>
            <p><img src="img/actuation/Servomotor_Timing_Diagram.svg.png" width="300"></p>
            </section>
            <section id="pwm-generation-in-microcontrollers"
            class="level3" data-number="7.1.5">
            <h3 data-number="7.1.5"><span
            class="header-section-number">7.1.5</span> PWM Generation in
            Microcontrollers</h3>
            <p>We can use the built-in PWM components in many
            microcontrollers or timer ICs. Using Arduino, generating a
            PWM is as simple as writing out <em>a few lines of
            code</em>!</p>
            <pre><code>analogWrite(pin, value)</code></pre>
            <p>Note that <strong>not all pins</strong> of an Arduino can
            generate a PWM signal. In the case of Arduino Uno, there are
            only 6 I/O pins (3,5,6,9,10,11) that support PWM generation
            and they are marked with a tilde (~) in front of their pin
            number on the board.</p>
            <p><img src="img/actuation/arduino_uno_pwm.png" width="300"></p>
            <p><br></p>
            <p>Examples of various duty cycles:</p>
            <pre><code>analogWrite(PWM_PIN, 64);   // 25% Duty Cycle or 25% of max speed
analogWrite(PWM_PIN, 127);  // 50% Duty Cycle or 50% of max speed
analogWrite(PWM_PIN, 191);  // 75% Duty Cycle or 75% of max speed
analogWrite(PWM_PIN, 255);  // 100% Duty Cycle or full speed</code></pre>
            <p><br></p>
            <p>The Raspberry Pi also has a variety of GPIO pins that can
            be used for generating PWM signals:</p>
            <p><img src="img/actuation/pi_pwm.png" width="300"></p>
            <p><br></p>
            <p>Consider this <a
            href="https://circuitdigest.com/microcontroller-projects/raspberry-pi-pwm-tutorial">code</a>
            for controlling the brightness of an LED:</p>
            <div class="sourceCode" id="cb20"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> RPi.GPIO <span class="im">as</span> IO       <span class="co">#calling header file which helps us use GPIO’s of PI</span></span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> time                 <span class="co">#calling time to provide delays in program</span></span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>IO.setwarnings(<span class="va">False</span>)       <span class="co">#do not show any warnings</span></span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>IO.setmode (IO.BCM)         <span class="co">#we are programming the GPIO by BCM pin numbers. (PIN35 as ‘GPIO19’)</span></span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>IO.setup(<span class="dv">19</span>,IO.OUT)         <span class="co"># initialize GPIO19 as an output.</span></span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>p <span class="op">=</span> IO.PWM(<span class="dv">19</span>,<span class="dv">100</span>)          <span class="co">#GPIO19 as PWM output, with 100Hz frequency</span></span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a>p.start(<span class="dv">0</span>)                  <span class="co">#generate PWM signal with 0% duty cycle</span></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="dv">1</span>:                    <span class="co">#execute loop forever</span></span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span> (<span class="dv">50</span>):    <span class="co">#execute loop for 50 times, x being incremented from 0 to 49.</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a>        p.ChangeDutyCycle(x) <span class="co">#change duty cycle for varying the brightness of LED.</span></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>        time.sleep(<span class="fl">0.1</span>)      <span class="co">#sleep for 100m second</span></span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> x <span class="kw">in</span> <span class="bu">range</span> (<span class="dv">50</span>):     <span class="co">#execute loop for 50 times, x being incremented from 0 to 49.</span></span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a>        p.ChangeDutyCycle(<span class="dv">50</span><span class="op">-</span>x)  <span class="co">#change duty cycle for changing the brightness of LED.</span></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>        time.sleep(<span class="fl">0.1</span>)          <span class="co">#sleep for 100m second</span></span></code></pre></div>
            <p><strong>References</strong>:</p>
            <p>Additional reading/examples/etc. if you want to learn
            more about PWMs, programming, <em>etc.</em>:</p>
            <ol type="1">
            <li><a
            href="https://www.circuitbread.com/ee-faq/what-is-a-pwm-signal">What
            is a PWM Signal?</a></li>
            <li><a
            href="https://www.circuitbread.com/tutorials/servo-motor-indirect-addressing-and-electronic-lock---part-10-microcontroller-basics-pic10f200">Servo
            Motors programing</a></li>
            <li><a
            href="https://www.whathifi.com/advice/dacs-what-is-a-dac-and-do-you-need-one">What
            is a DAC and why do you need on anyways?</a></li>
            <li><a
            href="https://pdhonline.com/courses/m543/m543content.pdf">An
            Engineer’s Primer on Actuators</a></li>
            <li><a
            href="https://www.youtube.com/watch?v=izeXcf5qu6s">What is a
            Linear Actuator?</a> [YouTube Video]</li>
            <li><a
            href="https://control.com/technical-articles/understanding-the-basicsof-pulse-width-modulation-pwm/">Understanding
            the Basics of PWM</a></li>
            <li><a
            href="https://randomnerdtutorials.com/raspberry-pi-pwm-python/">Raspberry
            Pi: PWM Outputs with Python (Fading LED)</a></li>
            <li><a
            href="https://circuitdigest.com/microcontroller-projects/raspberry-pi-pwm-tutorial">Raspberry
            Pi PWM tutorial</a></li>
            <li><a
            href="https://www.collinsaerospace.com/what-we-do/industries/business-aviation/power-controls-actuation/actuation">Actuation
            and
            Avionics</a><!--rel="stylesheet" href="./custom.sibin.css"--></li>
            </ol>
            </section>
            </section>
            </section>
            <section id="state-estimation" class="level1"
            data-number="8">
            <h1 data-number="8"><span
            class="header-section-number">8</span> State Estimation</h1>
            <p>Consider a robot in a simple grid starting at <span
            class="math inline">(0, 0)</span> with the intention of
            moving to <span class="math inline">(2, 2)</span>:</p>
            <p><img src="img/ekf/robot_grid.1.png" width="300">
            <img src="img/ekf/robot_grid.2.png" width="300"></p>
            <p>Now the robot can take multiple paths to get to its
            destination. The robot has multiple choices as shown in the
            following figure:</p>
            <p><img src="img/ekf/robot_grid.3.png" width="300"></p>
            <p>Let’s assume that it follows one of these paths and ends
            up at the following position:</p>
            <p><img src="img/ekf/robot_grid.4.png" width="200">
            <img src="img/ekf/robot_grid.5.png" width="200">
            <img src="img/ekf/robot_grid.6.png" width="200"></p>
            <p>Clearly this is not the intended goal so a few things
            need to happen:</p>
            <ol type="1">
            <li>first of all, the robot has to understand and estimate
            where it is <em>right now</em> → <em>i.e.,</em>
            <strong>estimate its current state</strong></li>
            <li>the robot has to then make a decision, based on its
            current state, where to head next</li>
            </ol>
            <p>How to do this?</p>
            <p>Answer: <strong><a href="#state-estimation">state
            estimation</a></strong>!</p>
            <p>But before we go there, we have to answer the following
            question → <em>how did we end up here in spite of onboard
            sensors?</em></p>
            <p>The sensor gave us a value, <span
            class="math inline"><em>x</em><sub><em>k</em></sub></span> →
            we can’t seem to trust it as is → because sensors are
            imperfect, mainly due to:</p>
            <ul>
            <li>physical limitations, measurement noise, poor
            calibrations, etc.</li>
            <li>errors can’t be zero
            <ul>
            <li><span
            class="math inline"><em>e</em><em>r</em><em>r</em><em>o</em><em>r</em> = <em>o</em><em>b</em><em>s</em><em>e</em><em>r</em><em>v</em><em>a</em><em>t</em><em>i</em><em>o</em><em>n</em> – <em>t</em><em>r</em><em>u</em><em>e</em>_<em>v</em><em>a</em><em>l</em><em>u</em><em>e</em></span></li>
            </ul></li>
            </ul>
            <p>In some sense, we need to “filter” out noisy data and
            only allow correct data to guide us. Hence, our robot can
            pick the right direction to end up at its <em>correct</em>
            destination.</p>
            <p><img src="img/ekf/robot_grid.7.png" width="200">
            <img src="img/ekf/robot_grid.8.png" width="200">
            <img src="img/ekf/robot_grid.9.png" width="200"></p>
            <section id="state-estimation-1" class="level2"
            data-number="8.1">
            <h2 data-number="8.1"><span
            class="header-section-number">8.1</span> State
            Estimation</h2>
            <p>State estimation is a fundamental problem in control
            theory, robotics and signal processing. It involves
            <strong>determining the state of a dynamic system from noisy
            or incomplete measurements</strong>. The Kalman filter,
            developed by Rudolf E. Kálmán in the early 1960s, is one of
            the most widely used and powerful algorithms for state
            estimation.</p>
            <p>In the context of dynamic systems,
            “<strong>state</strong>” is defined as,</p>
            <blockquote>
            <p>a set of variables that completely describe the system at
            a given time.</p>
            </blockquote>
            <p>For example:</p>
            <table>
            <thead>
            <tr>
            <th><strong>system</strong></th>
            <th><strong>state variables</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>moving vehicle</strong></td>
            <td>position, velocity, acceleration</td>
            </tr>
            <tr>
            <td><strong>pendulum</strong></td>
            <td>angle, angular velocity</td>
            </tr>
            <tr>
            <td><strong>financial system</strong></td>
            <td>asset prices, market indicators</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>State <strong>evolves over time</strong> according to the
            system dynamics, which can be described by a <strong>state
            transition model</strong>.</p>
            <section id="how-to-estimate-state" class="level3"
            data-number="8.1.1">
            <h3 data-number="8.1.1"><span
            class="header-section-number">8.1.1</span> How to Estimate
            State?</h3>
            <p>Let’s look at some data:</p>
            <p><img src="img/ekf/noisy_data.1.png" width="300"></p>
            <p><br></p>
            <p>How we take these values (<span
            class="math inline"><em>x</em><sub><em>k</em></sub></span>)
            and generate a state <em>estimate</em>, <span
            class="math inline">$\overline{x_k}$</span>?</p>
            <p>What if we have a few more values?</p>
            <p><img src="img/ekf/noisy_data.2.png" width="300">
            <img src="img/ekf/noisy_data.3.png" width="300"></p>
            <p><br></p>
            <p>Do we see a trend? What if we had many more values?</p>
            <p><img src="img/ekf/noisy_data.4.png" width="300"></p>
            <p><br></p>
            <p>What’s the best way to capture the behavior? We can see
            that it is “noisy” in that it doesn’t follow an “exact”
            trend.</p>
            <p>Remember that <span
            class="math inline">$\overline{x_k}$</span> is the estimate
            we want:</p>
            <p><img src="img/ekf/sensor_state.3.png" width="400"></p>
            <p>One way to compute <span
            class="math inline">$\overline{x_k}$</span> would be as an
            <strong>average</strong> of a <strong>running window of
            samples</strong>. Why “window” and not all the samples? Well
            we may only care about the most recent <code>n</code> values
            – anything older and it may not be directly applicable to
            our current situation.</p>
            <p>We can compute the average as follows:</p>
            <p><span class="math display">$$
            \overline{x_{k}}=\frac{x_{k-n-1}+\cdots+x_{k-1}+x_{k}}{n}
            $$</span></p>
            <p>where the only parameter is the window size, <span
            class="math inline"><em>n</em></span>.</p>
            <p>Consider the following data:</p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline"><em>k</em></span></th>
            <th><span
            class="math inline"><em>x</em><sub><em>k</em></sub></span></th>
            <th><span class="math inline">$\overline{x_{k}}$</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0</td>
            <td>0.08187</td>
            <td>0.08187</td>
            </tr>
            <tr>
            <td>1</td>
            <td>0.97601</td>
            <td>0.52894</td>
            </tr>
            <tr>
            <td>2</td>
            <td>1.18350</td>
            <td>0.74713</td>
            </tr>
            </tbody>
            </table>
            <p>Using the running window average method (for window size
            <span class="math inline"><em>n</em> = 3</span>), we
            get:</p>
            <p><img src="img/ekf/average_window.1.png" width="300"></p>
            <p><br></p>
            <p>As we add more values, we see that the window moves as
            well, aggregating groups of values:</p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline"><em>k</em></span></th>
            <th><span
            class="math inline"><em>x</em><sub><em>k</em></sub></span></th>
            <th><span class="math inline">$\overline{x_{k}}$</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0</td>
            <td>0.08187</td>
            <td>0.08187</td>
            </tr>
            <tr>
            <td>1</td>
            <td>0.97601</td>
            <td>0.52894</td>
            </tr>
            <tr>
            <td>2</td>
            <td>1.18350</td>
            <td>0.74713</td>
            </tr>
            <tr>
            <td>3</td>
            <td>0.99502</td>
            <td>1.05151</td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/ekf/average_window.2.png" width="300"></p>
            <p><br></p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline"><em>k</em></span></th>
            <th><span
            class="math inline"><em>x</em><sub><em>k</em></sub></span></th>
            <th><span class="math inline">$\overline{x_{k}}$</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0</td>
            <td>0.08187</td>
            <td>0.08187</td>
            </tr>
            <tr>
            <td>1</td>
            <td>0.97601</td>
            <td>0.52894</td>
            </tr>
            <tr>
            <td>2</td>
            <td>1.18350</td>
            <td>0.74713</td>
            </tr>
            <tr>
            <td>3</td>
            <td>0.99502</td>
            <td>1.05151</td>
            </tr>
            <tr>
            <td>4</td>
            <td>-0.31375</td>
            <td>0.62159</td>
            </tr>
            <tr>
            <td>5</td>
            <td>-0.25739</td>
            <td>0.14129</td>
            </tr>
            <tr>
            <td>6</td>
            <td>1.52112</td>
            <td>0.31666</td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/ekf/average_window.3.png" width="300"></p>
            <p><br></p>
            <p>We can see that the estimate is trying to match the
            changes in the original sensor readings.</p>
            <p>finally, we get:</p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline"><em>k</em></span></th>
            <th><span
            class="math inline"><em>x</em><sub><em>k</em></sub></span></th>
            <th><span class="math inline">$\overline{x_{k}}$</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>0</td>
            <td>0.08187</td>
            <td>0.08187</td>
            </tr>
            <tr>
            <td>1</td>
            <td>0.97601</td>
            <td>0.52894</td>
            </tr>
            <tr>
            <td>2</td>
            <td>1.18350</td>
            <td>0.74713</td>
            </tr>
            <tr>
            <td>3</td>
            <td>0.99502</td>
            <td>1.05151</td>
            </tr>
            <tr>
            <td>4</td>
            <td>-0.31375</td>
            <td>0.62159</td>
            </tr>
            <tr>
            <td>5</td>
            <td>-0.25739</td>
            <td>0.14129</td>
            </tr>
            <tr>
            <td>6</td>
            <td>1.52112</td>
            <td>0.31666</td>
            </tr>
            <tr>
            <td>7</td>
            <td>1.75454</td>
            <td>1.00609</td>
            </tr>
            <tr>
            <td>8</td>
            <td>1.82412</td>
            <td>1.69993</td>
            </tr>
            <tr>
            <td>9</td>
            <td>1.89229</td>
            <td>1.82365</td>
            </tr>
            <tr>
            <td>10</td>
            <td>1.10513</td>
            <td>1.60718</td>
            </tr>
            <tr>
            <td>11</td>
            <td>1.22321</td>
            <td>1.40688</td>
            </tr>
            <tr>
            <td>12</td>
            <td>2.20793</td>
            <td>1.51209</td>
            </tr>
            <tr>
            <td>13</td>
            <td>3.02390</td>
            <td>2.15168</td>
            </tr>
            <tr>
            <td>14</td>
            <td>2.45511</td>
            <td>2.56231</td>
            </tr>
            <tr>
            <td>15</td>
            <td>2.07442</td>
            <td>2.51781</td>
            </tr>
            <tr>
            <td>16</td>
            <td>1.49280</td>
            <td>2.00744</td>
            </tr>
            <tr>
            <td>17</td>
            <td>1.19093</td>
            <td>1.58605</td>
            </tr>
            <tr>
            <td>18</td>
            <td>2.32653</td>
            <td>1.67009</td>
            </tr>
            <tr>
            <td>19</td>
            <td>3.84177</td>
            <td>2.45308</td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/ekf/average_window.4.png" width="300"></p>
            <p><br></p>
            <p>This is a good start, but let’s consider a few
            changes.</p>
            <p>First: what happens if window size *increases**?</p>
            <p>We consider <span
            class="math inline"><em>n</em> = 4</span> or <span
            class="math inline"><em>n</em> = 9</span> even.</p>
            <p><img src="img/ekf/average_window.5.png" width="300"></p>
            <p><br></p>
            <p>But first, let’s discuss some properties that correlate
            with <em>larger</em> window size:</p>
            <table>
            <thead>
            <tr>
            <th>property</th>
            <th>effect (larger <span
            class="math inline"><em>n</em></span>)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>smoothening</td>
            <td>more/less?</td>
            </tr>
            <tr>
            <td>sensitivity (to changes)</td>
            <td>more/less?</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>These properties can significantly affect the
            <strong>response</strong> for the system → whether it is
            jerky or sudden vs. a better, albeit slower, response. Also,
            the sensitivity tells us that the system doesn’t respond
            easily to big changes in output, thus increasing
            inertia!</p>
            <p>Let’s look at a few examples:</p>
            <p><img src="img/ekf/average_window.6.png" width="150">
            <img src="img/ekf/average_window.7.png" width="150">
            <img src="img/ekf/average_window.8.png" width="150">
            <img src="img/ekf/average_window.9.png" width="150"></p>
            <p><br></p>
            <p>What about <strong>delays</strong>? Does the window size
            affect how delayed the estimate is?</p>
            <p><img src="img/ekf/average_window.10.png" width="150">
            <img src="img/ekf/average_window.11.png" width="150">
            <img src="img/ekf/average_window.12.png" width="150"></p>
            <p><br></p>
            <p>As we see, this is the case! With increased window sizes
            → delays increase.</p>
            <p><strong>Questions</strong>: Why does this happen?</p>
            <table>
            <thead>
            <tr>
            <th>property</th>
            <th>effect (larger <span
            class="math inline"><em>n</em></span>)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>smoothening</td>
            <td><strong>more</strong></td>
            </tr>
            <tr>
            <td>sensitivity (to changes)</td>
            <td><strong>less</strong></td>
            </tr>
            <tr>
            <td>delays</td>
            <td><strong>more</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            <section id="exponential-moving-average-ema" class="level3"
            data-number="8.1.2">
            <h3 data-number="8.1.2"><span
            class="header-section-number">8.1.2</span> Exponential
            Moving Average (EMA)</h3>
            <p>To give more weights to <em>recent</em> data, prevent
            delays and get a better control over the smoothing, we use
            EMA that usese <strong>exponentially decreasing weights over
            time</strong>,</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \overline{x_{0}}=x_{0} \\
            &amp; \overline{x_{k}}=\alpha x_{k}+(1-\alpha)
            \overline{x_{k-1}}, k&gt;0
            \end{aligned}
            $$</span></p>
            <p>where, <span class="math inline"><em>α</em></span> (<span
            class="math inline">0 &lt; <em>α</em> &lt; 1</span>) →
            <strong>smoothing factor</strong>.</p>
            <p>Example (<span
            class="math inline"><em>α</em> = 0.75</span>):</p>
            <table>
            <thead>
            <tr>
            <th style="text-align: center;"><span
            class="math inline"><em>k</em></span></th>
            <th style="text-align: center;"><span
            class="math inline"><em>x</em><sub><em>k</em></sub></span></th>
            <th style="text-align: center;"><span
            class="math inline">$\overline{x_{k}}$</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: center;">0</td>
            <td style="text-align: center;">2.0</td>
            <td style="text-align: center;">2.0000</td>
            </tr>
            <tr>
            <td style="text-align: center;">1</td>
            <td style="text-align: center;">3.0</td>
            <td style="text-align: center;">2.7000</td>
            </tr>
            <tr>
            <td style="text-align: center;">2</td>
            <td style="text-align: center;">2.0</td>
            <td style="text-align: center;">2.2100</td>
            </tr>
            <tr>
            <td style="text-align: center;">3</td>
            <td style="text-align: center;">4.0</td>
            <td style="text-align: center;">3.4630</td>
            </tr>
            <tr>
            <td style="text-align: center;">4</td>
            <td style="text-align: center;">3.0</td>
            <td style="text-align: center;">3.1389</td>
            </tr>
            <tr>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            </tr>
            </tbody>
            </table>
            <p>One of the main advantages of EMA is that we <strong>only
            need to store one value</strong>, <span
            class="math inline">$\overline{x_{k}}$</span>.</p>
            <p>But why “exponential”? If we expand the term for <span
            class="math inline">$\overline{x_{k}}$</span> we see,</p>
            <p><span class="math display">$$
            \begin{array}{rlrl}
            \overline{x_{k}} &amp; =\alpha x_{k}+(1-\alpha)
            \overline{x_{k-1}}\\
            &amp; =\alpha x_{k}+\alpha(1-\alpha) x_{k-1}+(1-\alpha)^{2}
            \overline{x_{k-2}} \\
            &amp; =\alpha x_{k}+\alpha(1-\alpha)
            x_{k-1}+\alpha(1-\alpha)^{2} x_{k-2}+(1-\alpha)^{3}
            \overline{x_{k-3}} &amp; \vdots \\
            &amp; =\cdots &amp; \\
            &amp; =\alpha\left[x_{k}+(1-\alpha) x_{k-1}+(1-\alpha)^{2}
            x_{k-2}+(1-\alpha)^{3} x_{k-3}+\cdots+(1-\alpha)^{k-1}
            x_{1}\right]+(1-\alpha)^{k} x_{0}
            \end{array}
            $$</span></p>
            <p>As we see, the effect of the smoothing factor, <span
            class="math inline"><em>α</em></span>, is applied
            exponentially with each sensor reading. For various values
            of <span class="math inline"><em>α</em></span>,</p>
            <p><span class="math display">$$
            \begin{array}{c}
            \alpha=0.3000 \\
            \alpha(1-\alpha)=0.2100 \\
            \alpha(1-\alpha)^{2}=0.1470 \\
            \alpha(1-\alpha)^{3}=0.1029 \\
            \vdots \\
            \end{array}
            $$</span></p>
            <p>EMA takes into acount <strong>all past data</strong> and
            encodes it into a <strong>single</strong> value, <span
            class="math inline">$\overline{x_{k}}$</span>.</p>
            <p>Consider the following example where <span
            class="math inline"><em>α</em> = 0.5</span>:</p>
            <table>
            <thead>
            <tr>
            <th style="text-align: right;"><span
            class="math inline"><sub><em>k</em></sub></span></th>
            <th style="text-align: left;"><span
            class="math inline"><em>χ</em><sub><em>k</em></sub></span></th>
            <th style="text-align: left;"><span
            class="math inline">$\overline{\chi_{k}}$</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: right;">0</td>
            <td style="text-align: left;">0.08187</td>
            <td style="text-align: left;">0.08187</td>
            </tr>
            <tr>
            <td style="text-align: right;">1</td>
            <td style="text-align: left;">0.97601</td>
            <td style="text-align: left;">0.52894</td>
            </tr>
            <tr>
            <td style="text-align: right;">2</td>
            <td style="text-align: left;">1.18350</td>
            <td style="text-align: left;">0.85622</td>
            </tr>
            <tr>
            <td style="text-align: right;">3</td>
            <td style="text-align: left;">0.99502</td>
            <td style="text-align: left;">0.92562</td>
            </tr>
            <tr>
            <td style="text-align: right;">4</td>
            <td style="text-align: left;">-0.31375</td>
            <td style="text-align: left;">0.30594</td>
            </tr>
            <tr>
            <td style="text-align: right;">5</td>
            <td style="text-align: left;">-0.25739</td>
            <td style="text-align: left;">0.02427</td>
            </tr>
            <tr>
            <td style="text-align: right;">6</td>
            <td style="text-align: left;">1.52112</td>
            <td style="text-align: left;">0.77269</td>
            </tr>
            <tr>
            <td style="text-align: right;">7</td>
            <td style="text-align: left;">1.75454</td>
            <td style="text-align: left;">1.26362</td>
            </tr>
            <tr>
            <td style="text-align: right;">8</td>
            <td style="text-align: left;">1.82412</td>
            <td style="text-align: left;">1.54387</td>
            </tr>
            <tr>
            <td style="text-align: right;">9</td>
            <td style="text-align: left;">1.89229</td>
            <td style="text-align: left;">1.71808</td>
            </tr>
            <tr>
            <td style="text-align: right;">10</td>
            <td style="text-align: left;">1.10513</td>
            <td style="text-align: left;">1.41161</td>
            </tr>
            <tr>
            <td style="text-align: right;">11</td>
            <td style="text-align: left;">1.22321</td>
            <td style="text-align: left;">1.31741</td>
            </tr>
            <tr>
            <td style="text-align: right;">12</td>
            <td style="text-align: left;">2.20793</td>
            <td style="text-align: left;">1.76267</td>
            </tr>
            <tr>
            <td style="text-align: right;">13</td>
            <td style="text-align: left;">3.02390</td>
            <td style="text-align: left;">2.39328</td>
            </tr>
            <tr>
            <td style="text-align: right;">14</td>
            <td style="text-align: left;">2.45511</td>
            <td style="text-align: left;">2.42420</td>
            </tr>
            <tr>
            <td style="text-align: right;">15</td>
            <td style="text-align: left;">2.07442</td>
            <td style="text-align: left;">2.24931</td>
            </tr>
            <tr>
            <td style="text-align: right;">16</td>
            <td style="text-align: left;">1.49280</td>
            <td style="text-align: left;">1.87105</td>
            </tr>
            <tr>
            <td style="text-align: right;">17</td>
            <td style="text-align: left;">1.19093</td>
            <td style="text-align: left;">1.53099</td>
            </tr>
            <tr>
            <td style="text-align: right;">18</td>
            <td style="text-align: left;">2.32653</td>
            <td style="text-align: left;">1.92876</td>
            </tr>
            <tr>
            <td style="text-align: right;">19</td>
            <td style="text-align: left;">3.84177</td>
            <td style="text-align: left;">2.88526</td>
            </tr>
            <tr>
            <td style="text-align: right;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>The graph looks like:</p>
            <p><img src="img/ekf/ema.1.png" width="300"></p>
            <p><br></p>
            <p>Let’s consider some of the values:</p>
            <table>
            <thead>
            <tr>
            <th style="text-align: right;"><span
            class="math inline"><sub><em>k</em></sub></span></th>
            <th style="text-align: left;"><span
            class="math inline"><em>χ</em><sub><em>k</em></sub></span></th>
            <th style="text-align: left;"><span
            class="math inline">$\overline{\chi_{k}}$</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: right;">0</td>
            <td style="text-align: left;">0.08187</td>
            <td style="text-align: left;">0.08187</td>
            </tr>
            <tr>
            <td style="text-align: right;">1</td>
            <td style="text-align: left;">0.97601</td>
            <td style="text-align: left;">0.52894</td>
            </tr>
            <tr>
            <td style="text-align: right;">2</td>
            <td style="text-align: left;">1.18350</td>
            <td style="text-align: left;">0.85622</td>
            </tr>
            <tr>
            <td style="text-align: right;">3</td>
            <td style="text-align: left;">0.99502</td>
            <td style="text-align: left;"><strong>0.92562</strong></td>
            </tr>
            <tr>
            <td style="text-align: right;">4</td>
            <td style="text-align: left;"><strong>-0.31375</strong></td>
            <td style="text-align: left;"><strong>0.30594</strong></td>
            </tr>
            <tr>
            <td style="text-align: right;">5</td>
            <td style="text-align: left;">-0.25739</td>
            <td style="text-align: left;">0.02427</td>
            </tr>
            <tr>
            <td style="text-align: right;">6</td>
            <td style="text-align: left;">1.52112</td>
            <td style="text-align: left;">0.77269</td>
            </tr>
            <tr>
            <td style="text-align: right;">…</td>
            <td style="text-align: left;">…</td>
            <td style="text-align: left;">…</td>
            </tr>
            </tbody>
            </table>
            <p><span class="math inline">$\overline{x_{4}}=0.5 x_{4}+0.5
            \overline{x_{3}}$</span></p>
            <p>We see that for this value of <span
            class="math inline"><em>α</em></span>, the estimate is
            “halfway” between the two sensor readings,</p>
            <p><img src="img/ekf/ema.3.png" width="300"></p>
            <p><br></p>
            <p>Now, if we change <span
            class="math inline"><em>α</em> = 0.7</span>, we get <span
            class="math inline">$\overline{x_{4}}=0.7 x_{4}+0.3
            \overline{x_{3}}$</span> and the graph now looks like,</p>
            <p><img src="img/ekf/ema.4.png" width="300"></p>
            <p><br></p>
            <p>As we see, there’s a <strong>heavier bias</strong>
            towards the <strong>more recent value</strong>.</p>
            <p>Now, of these graphs, which one is <span
            class="math inline"><em>α</em> = 0.05</span> and which one
            is <span class="math inline"><em>α</em> = 0.95</span>?</p>
            <p><img src="img/ekf/ema.5.png" width="300">
            <img src="img/ekf/ema.6.png" width="300"></p>
            <p><br></p>
            <p>We can see the significance of changing <span
            class="math inline"><em>α</em></span>,</p>
            <p><img src="img/ekf/ema.7.png" width="300">
            <img src="img/ekf/ema.8.png" width="300"></p>
            <p><br></p>
            <p>As we see from the following figures, increasing <span
            class="math inline"><em>α</em></span> (left to right)
            results in:</p>
            <ul>
            <li><strong>less delay</strong></li>
            <li><strong>less smoothening out</strong></li>
            </ul>
            <p><img src="img/ekf/ema.9.png" width="200">
            <img src="img/ekf/ema.10.png" width="200">
            <img src="img/ekf/ema.11.png" width="200"></p>
            <p><br></p>
            </section>
            <section id="state-space-representation" class="level3"
            data-number="8.1.3">
            <h3 data-number="8.1.3"><span
            class="header-section-number">8.1.3</span> State Space
            Representation</h3>
            <p>A more “formal” description of
            <strong>state</strong>:</p>
            <blockquote>
            <p>a <strong>quantitative</strong> characterization of a
            system that is <strong>not directly observable</strong></p>
            </blockquote>
            <p>Examples: temperature, position, velocity, weight,
            etc.</p>
            <p>There are <em>many</em> ways to represent state, even for
            the same quantity. For instance, consider how to estimate
            the position of a car in a 2D plane, with the intent of
            <strong>tracking</strong> it:</p>
            <p><img src="img/ekf/state.1.png" width="200"></p>
            <p><br></p>
            <p>So, how do we represent the “state” of this car?</p>
            <ol type="1">
            <li><span
            class="math inline"><strong>x</strong><sub><strong>k</strong></sub> = (<em>x</em>, <em>y</em>)</span></li>
            </ol>
            <p>A simple position on the coordinate system. Is this
            sufficient?</p>
            <p>While this captures a <strong>static</strong> state of
            the system, it doesn’t necessarily allow for tracking the
            car.</p>
            <ol start="2" type="1">
            <li><span
            class="math inline"><strong>x</strong><sub><strong>k</strong></sub> = (<em>x</em>, <em>ẋ</em>, <em>y</em>, <em>ẏ</em>)</span></li>
            </ol>
            <p>So, let’s also track the <strong>velocity</strong>.
            Clearly that will tell us how fast the car is moving and so
            we can “track” it correctly?</p>
            <p>Well, not quite. This is <strong>instantaneous
            velocity</strong> that doesn’t tell us if the car is
            accelerating or deccelerating!</p>
            <ol start="3" type="1">
            <li><span
            class="math inline">$\boldsymbol{x}_{\boldsymbol{k}}=(x,
            \dot{x}, \ddot{x}, y, \dot{y}, \ddot{y})$</span></li>
            </ol>
            <p>Ok, so now we have position, velocity
            <strong>and</strong> acceleration! Surely, we’re done?</p>
            <p>But do we know which <strong>direction</strong> the car
            is heading in?</p>
            <ol start="4" type="1">
            <li><span
            class="math inline">$\boldsymbol{x}_{\boldsymbol{k}}=(x,
            \dot{x}, \ddot{x}, y, \dot{y}, \ddot{y},
            \theta)$</span></li>
            </ol>
            <p>Now, we have a better sense of the “state” of the car, in
            order to track it.</p>
            <p><strong>State estimation</strong> → estimating state from
            sensor measurements.</p>
            <p>While the moving averages and EMA are good ways to deal
            with noisy measurements, estimating state is much harder.
            There is often <strong>uncertainty</strong> in the
            measurements and state estimation.</p>
            </section>
            <section id="probabilistic-state-estimation" class="level3"
            data-number="8.1.4">
            <h3 data-number="8.1.4"><span
            class="header-section-number">8.1.4</span> Probabilistic
            State Estimation</h3>
            <p>These methods allow us to deal with the uncertanties in
            sensor measurements as well as in state estimation by use of
            probility. The main idea → represent state in a
            <strong>probability distribution</strong></p>
            <ul>
            <li>Measurements are noisy</li>
            <li>Models ‘uncertainty’</li>
            </ul>
            <p>We start with the following <em>“belief”</em> → knowledge
            about the state or ‘estimate of the true state’</p>
            <p>A Probabilistic state estimation, then → computes
            <strong>new belief based on measurement data</strong></p>
            <p><img src="img/ekf/prob_state.1.jpeg" width="300"></p>
            <p>There are various methods for <strong>probabilistic state
            estimations</strong>, most notably,</p>
            <ol type="1">
            <li><a href="#bayes-filter">Bayes filter</a></li>
            <li><a href="#kalman-filter">Kalman filter</a></li>
            <li><a href="">Extended Kalman Filter</a></li>
            </ol>
            <section id="review-of-probability-theory" class="level4"
            data-number="8.1.4.1">
            <h4 data-number="8.1.4.1"><span
            class="header-section-number">8.1.4.1</span> Review of
            Probability Theory</h4>
            <p>Let’s take a quick detour to review some concepts in
            probability theory.</p>
            <p><strong>Random variable</strong>, <span
            class="math inline"><em>X</em></span></p>
            <ul>
            <li><p>A variable whose possible values are numerical
            outcomes of a random phenomenon</p>
            <ul>
            <li>A function X: <span
            class="math inline"><em>Ω</em> → ℝ</span>, where <span
            class="math inline"><em>Ω</em></span> is the set of possible
            outcomes</li>
            <li>Example: rolling a dice, <span
            class="math inline"><em>Ω</em> = {1, 2, 3, 4, 5, 6}</span></li>
            </ul></li>
            <li><p>It models state, measurement, controls, environments,
            etc.</p></li>
            <li><p><strong>Discrete random variable</strong> → X can
            take on a countable number of values</p></li>
            <li><p><strong>Continuous</strong> random variable → X can
            take on an infinite number of values</p></li>
            </ul>
            <p><strong>Probability distribution</strong>, <span
            class="math inline"><em>p</em>(<em>x</em>)</span></p>
            <ul>
            <li>Links each outcome with probability</li>
            </ul>
            <p>Probability distribution: <span
            class="math inline"><em>p</em>(⋅)</span> Probability: <span
            class="math inline">Pr (⋅)</span></p>
            <p>Two way do represent probability distributions:</p>
            <table>
            <colgroup>
            <col style="width: 47%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th>probability mass function (PMF)</th>
            <th>probability density function (PDF)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/ekf/pmf.png" width="200"></td>
            <td><img src="img/ekf/pdf.png" width="200"></td>
            </tr>
            </tbody>
            </table>
            <ul>
            <li><strong>Joint distribution</strong>
            <ul>
            <li><span
            class="math inline"><em>p</em>(<em>x</em>, <em>y</em>) = <em>p</em>(<em>X</em> = <em>x</em></span>
            and <span
            class="math inline"><em>Y</em> = <em>y</em>)</span></li>
            </ul></li>
            <li>If X and Y are <strong>independent</strong>
            <ul>
            <li><span
            class="math inline"><em>p</em>(<em>x</em>, <em>y</em>) = <em>p</em>(<em>x</em>)<em>p</em>(<em>y</em>)</span></li>
            </ul></li>
            </ul>
            <p><br></p>
            <p><strong>Conditional distribution</strong></p>
            <ul>
            <li><span
            class="math inline"><em>p</em>(<em>x</em> ∣ <em>y</em>)</span>
            : probability of <span class="math inline"><em>x</em></span>
            given <span class="math inline"><em>y</em></span></li>
            <li>If <span
            class="math inline"><em>p</em>(<em>y</em>) &gt; 0</span>,
            <ul>
            <li><span class="math inline">$p(x \mid y)=\frac{p(x,
            y)}{p(y)}$</span></li>
            <li><span
            class="math inline"><em>p</em>(<em>x</em>, <em>y</em>) = <em>p</em>(<em>x</em> ∣ <em>y</em>)<em>p</em>(<em>y</em>)</span></li>
            </ul></li>
            <li>If X and Y are independent, <span
            class="math inline"><em>p</em>(<em>x</em> ∣ <em>y</em>)=</span>
            ?, <span
            class="math inline"><em>p</em>(<em>y</em> ∣ <em>x</em>)=</span>
            ?
            <ul>
            <li>i.e., X (resp. Y ) tells nothing about Y (resp. X )</li>
            </ul></li>
            </ul>
            <p>Consider the example of rolling two dice ( <span
            class="math inline">X, Y</span> )</p>
            <table>
            <thead>
            <tr>
            <th style="text-align: center;"><span
            class="math inline">(1, 1)</span></th>
            <th style="text-align: center;"><span
            class="math inline">(1, 2)</span></th>
            <th style="text-align: center;"><span
            class="math inline">(1, 3)</span></th>
            <th style="text-align: center;"><span
            class="math inline">(1, 4)</span></th>
            <th style="text-align: center;"><span
            class="math inline">(1, 5)</span></th>
            <th style="text-align: center;"><span
            class="math inline">(1, 6)</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">(2, 1)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(2, 2)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(2, 3)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(2, 4)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(2, 5)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(2, 6)</span></td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">(3, 1)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(3, 2)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(3, 3)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(3, 4)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(3, 5)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(3, 6)</span></td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">(4, 1)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(4, 2)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(4, 3)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(4, 4)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(4, 5)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(4, 6)</span></td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">(5, 1)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(5, 2)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(5, 3)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(5, 4)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(5, 5)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(5, 6)</span></td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">(6, 1)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(6, 2)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(6, 3)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(6, 4)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(6, 5)</span></td>
            <td style="text-align: center;"><span
            class="math inline">(6, 6)</span></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>What is <span
            class="math inline"><em>P</em><em>r</em>(<em>𝑋</em> = 2|<em>𝑋</em> + <em>𝑌</em> ≤ 5)</span>?</p>
            <table>
            <tbody>
            <tr>
            <td style="text-align: center;"></td>
            <td style="text-align: center;">1</td>
            <td style="text-align: center;">2</td>
            <td style="text-align: center;">3</td>
            <td style="text-align: center;">4</td>
            <td style="text-align: center;">5</td>
            <td style="text-align: center;">6</td>
            <td style="text-align: center;"></td>
            </tr>
            <tr>
            <td style="text-align: center;">1</td>
            <td style="text-align: center;">2</td>
            <td style="text-align: center;">3</td>
            <td style="text-align: center;">4</td>
            <td style="text-align: center;">5</td>
            <td style="text-align: center;">6</td>
            <td style="text-align: center;">7</td>
            <td style="text-align: center;"></td>
            </tr>
            <tr>
            <td style="text-align: center;">2</td>
            <td style="text-align: center;">3</td>
            <td style="text-align: center;">4</td>
            <td style="text-align: center;">5</td>
            <td style="text-align: center;">6</td>
            <td style="text-align: center;">7</td>
            <td style="text-align: center;">8</td>
            <td style="text-align: center;"></td>
            </tr>
            <tr>
            <td style="text-align: center;">3</td>
            <td style="text-align: center;">4</td>
            <td style="text-align: center;">5</td>
            <td style="text-align: center;">6</td>
            <td style="text-align: center;">7</td>
            <td style="text-align: center;">8</td>
            <td style="text-align: center;">9</td>
            <td style="text-align: center;"></td>
            </tr>
            <tr>
            <td style="text-align: center;">4</td>
            <td style="text-align: center;">5</td>
            <td style="text-align: center;">6</td>
            <td style="text-align: center;">7</td>
            <td style="text-align: center;">8</td>
            <td style="text-align: center;">9</td>
            <td style="text-align: center;">10</td>
            <td style="text-align: center;"></td>
            </tr>
            <tr>
            <td style="text-align: center;">5</td>
            <td style="text-align: center;">6</td>
            <td style="text-align: center;">7</td>
            <td style="text-align: center;">8</td>
            <td style="text-align: center;">9</td>
            <td style="text-align: center;">10</td>
            <td style="text-align: center;">11</td>
            <td style="text-align: center;"></td>
            </tr>
            <tr>
            <td style="text-align: center;">6</td>
            <td style="text-align: center;">7</td>
            <td style="text-align: center;">8</td>
            <td style="text-align: center;">9</td>
            <td style="text-align: center;">10</td>
            <td style="text-align: center;">11</td>
            <td style="text-align: center;">12</td>
            <td style="text-align: center;"></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Theorem of total probability</strong></p>
            <p><span class="math display">$$
            \begin{array}{ll}
            \underline{\text { Discrete }} &amp; p(x)=\sum_{y} p(x,
            y)=\sum_{y} p(x \mid y) p(y) \\
            \underline{\text { Continuous }} &amp; p(x)=\int p(x, y) d
            y=\int p(x \mid y) p(y) d y
            \end{array}
            $$</span></p>
            <p>Consider the following problem:</p>
            <p><img src="img/ekf/cond_prob.1.jpg" width="400"></p>
            <p><br></p>
            <p>Let’s look at the row where “eye color” is
            <code>brown</code>:</p>
            <p><img src="img/ekf/cond_prob.2.jpg" width="400"></p>
            <p><br></p>
            <p>Filling in the right values:</p>
            <p><img src="img/ekf/cond_prob.3.jpg" width="400"></p>
            <p><br></p>
            <p><strong>Conditional independence</strong></p>
            <ul>
            <li><span class="math inline"><em>x</em></span> and <span
            class="math inline"><em>y</em></span> are independent if
            <span class="math inline"><em>z</em></span> is known</li>
            </ul>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; p(x, y \mid z)=p(x \mid z) p(y \mid z) \\
            &amp; p(x \mid z)=p(x \mid y, z) \\
            &amp; p(y \mid z)=p(y \mid x, z)
            \end{aligned}
            $$</span></p>
            <ul>
            <li>Conditional (resp. absolute) independence does not imply
            absolute (resp. conditional) independence</li>
            </ul>
            <p><br></p>
            <p><strong>Bayes Rule</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 45%" />
            <col style="width: 54%" />
            </colgroup>
            <tbody>
            <tr>
            <td>discrete</td>
            <td><span class="math inline">$p(x \mid y)=\frac{p(y \mid x)
            p(x)}{p(y)}=\frac{p(y \mid x) p(x)}{\sum_{x \prime} p\left(y
            \mid x^{\prime}\right)
            p\left(x^{\prime}\right)}$</span></td>
            </tr>
            <tr>
            <td>continuous</td>
            <td><span class="math inline">$\quad p(x \mid y)=\frac{p(y
            \mid x) p(x)}{p(y)}=\frac{p(y \mid x) p(x)}{\int p\left(y
            \mid x^{\prime}\right) p\left(x^{\prime}\right) d
            x^{\prime}}$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Prior and posterior probabilities</strong></p>
            <p><span class="math display">$$
            \text p(x \mid y)=\frac{p(y \mid x) p(x)}{p(y)}
            $$</span></p>
            <p>where,</p>
            <ul>
            <li><span
            class="math inline"><em>p</em>(<em>x</em> ∣ <em>y</em>)</span>
            → posterior</li>
            <li><span class="math inline"><em>p</em>(<em>x</em>)</span>
            → prior</li>
            </ul>
            <p>Hence,</p>
            <ul>
            <li>Inferring <span class="math inline"><em>x</em></span>
            from <span class="math inline"><em>y</em></span>
            <ul>
            <li><span class="math inline"><em>x</em></span> : state,
            <span class="math inline"><em>y</em></span> : data
            (measurement)</li>
            </ul></li>
            <li>Bayes rule helps compute a posterior using the inverse,
            <span
            class="math inline"><em>p</em>(<em>y</em> ∣ <em>x</em>)</span>,
            and prior, <span
            class="math inline"><em>p</em>(<em>x</em>)</span>
            <ul>
            <li>The probability of an event ( <span
            class="math inline"><em>x</em>=</span> state) given
            information ( <span class="math inline"><em>y</em>=</span>
            measurement)</li>
            <li>Easier to obtain <span
            class="math inline"><em>p</em>(<em>y</em> ∣ <em>x</em>)</span>
            <ul>
            <li>E.g., sensor accuracy: <span
            class="math inline"><em>p</em></span> (thermometer reading
            <span class="math inline"> = 72</span> | temperature <span
            class="math inline"> = 71</span> )?</li>
            </ul></li>
            <li>Example
            <ul>
            <li><span class="math inline"><em>x</em></span> : has
            disease, <span class="math inline"><em>y</em></span> : test
            is positive</li>
            <li>x: position, y: sensor reading</li>
            </ul></li>
            </ul></li>
            </ul>
            <p><br></p>
            <p><strong>Conditioning Bayes rule on an arbitrary random
            variable</strong></p>
            <p><span class="math display">$$
            p(x \mid y, z)=\frac{p(y \mid x, z) p(x \mid z)}{p(y \mid
            z)}
            $$</span></p>
            <p><br></p>
            <p><strong>Expectation of a random variable</strong>, <span
            class="math inline">E[<em>X</em>]</span> (or <span
            class="math inline"><em>μ</em></span>, mean)</p>
            <p><span class="math display">$$
            \begin{array}{ll}
            \underline{\text { Discrete }} &amp; \mathrm{E}[X]=\sum_{x}
            x p(x) \\
            \underline{\text { Continuous }} &amp; \mathrm{E}[X]=\int x
            p(x) d x
            \end{array}
            $$</span></p>
            <table>
            <thead>
            <tr>
            <th style="text-align: center;"></th>
            <th style="text-align: center;"><span
            class="math inline"><em>X</em> = 0</span></th>
            <th style="text-align: center;"><span
            class="math inline"><em>X</em> = 1</span></th>
            <th style="text-align: center;"><span
            class="math inline"><em>X</em> = 2</span></th>
            <th style="text-align: center;"><span
            class="math inline"><em>X</em> = 3</span></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: center;"><span
            class="math inline"><em>p</em>(<em>x</em>)</span></td>
            <td style="text-align: center;">0.10</td>
            <td style="text-align: center;">0.20</td>
            <td style="text-align: center;">0.60</td>
            <td style="text-align: center;">0.10</td>
            </tr>
            <tr>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><span
            class="math inline">E[<em>X</em>] = ∑<sub><em>x</em></sub><em>x</em><em>p</em>(<em>x</em>) = 0 ⋅ 0.10 + 1 ⋅ 0.20 + 2 ⋅ 0.60 + 3 ⋅ 0.10 = 1.70</span></p>
            <p><bR></p>
            <p><strong>Variance</strong>, <span
            class="math inline">Var (<em>X</em>)</span> or <span
            class="math inline"><em>σ</em><sup>2</sup></span></p>
            <ul>
            <li>How far values are spread out from the mean</li>
            <li>It can represent uncertainty or noise</li>
            </ul>
            <p><span class="math display">$$
            \begin{aligned}
            \operatorname{Var}[X]=\mathrm{E}\left[(X-\mu)^{2}\right]
            &amp; =\sum_{x}(x-\mu)^{2} p(x) &amp; &amp; \text { Discrete
            } \\
            &amp; =\int(x-\mu)^{2} p(x) d x &amp; &amp; \text {
            Continuous }
            \end{aligned}
            $$</span></p>
            <p>where,</p>
            <ul>
            <li><span class="math inline"><em>σ</em><sup>2</sup></span>
            : variance</li>
            <li><span class="math inline"><em>σ</em></span> : standard
            deviation</li>
            </ul>
            <p><br></p>
            <p><strong>Covariance</strong>, <span
            class="math inline">Cov (<em>X</em>, <em>Y</em>)</span> or
            <span
            class="math inline"><em>σ</em><sub><em>X</em><em>Y</em></sub><sup>2</sup></span></p>
            <ul>
            <li>Joint variability of <span
            class="math inline"><em>X</em></span> and <span
            class="math inline"><em>Y</em></span></li>
            </ul>
            <p><span class="math display">$$
            \begin{array}{ll}
            \operatorname{Cov}(X,
            Y)=\mathrm{E}[(X-\mathrm{E}[X])(Y-\mathrm{E}[Y])]=\mathrm{E}[\mathrm{XY}]-\mathrm{E}[\mathrm{X}]
            \mathrm{E}[Y] &amp; \\
            \operatorname{Cov}(X, Y)=\operatorname{Cov}(Y, X) &amp;
            \end{array}
            $$</span></p>
            <p>If <span class="math inline"><em>X</em></span> and <span
            class="math inline"><em>Y</em></span> are
            <strong>independent</strong>, <span
            class="math inline">Cov (<em>X</em>, <em>Y</em>) = 0</span></p>
            </section>
            </section>
            </section>
            <section id="bayes-filter" class="level2" data-number="8.2">
            <h2 data-number="8.2"><span
            class="header-section-number">8.2</span> Bayes Filter</h2>
            <p>Recall that sensors capture <strong>incomplete</strong>
            or <strong>noisy</strong> information. Consider the example
            of two LiDAR sensors that are trying to estimate the
            distance to a pedestrian. One of the LiDAR sensors, measure
            the distance to be at <span
            class="math inline">10<em>m</em></span> (with a probability
            of <span class="math inline">50%</span>) while the other
            estimates the person to be at <span
            class="math inline">10.8<em>m</em></span> (also with a
            probability of <span class="math inline">50%</span>).</p>
            <p><img src="img/ekf/bayes.lidar.4.png" width="400"></p>
            <p>In reality, as we see in the picture, the person is
            <strong>between <span
            class="math inline">10.3<em>m</em></span> and <span
            class="math inline">10.5<em>m</em></span></strong>. So, how
            do we deal with such situations? And what happens if the
            probabilities differ greatly? <strong>Which</strong> sensor
            would you trust and <strong>how much</strong>?</p>
            <section id="bayes-filter-prediction" class="level3"
            data-number="8.2.1">
            <h3 data-number="8.2.1"><span
            class="header-section-number">8.2.1</span> Bayes filter |
            <strong>Prediction</strong></h3>
            <p>Let’s revisit our previous example of a car:</p>
            <p><img src="img/ekf/state.1.png" width="200"></p>
            <p><br></p>
            <p>Let say the car is in <span
            class="math inline"><em>s</em><em>t</em><em>a</em><em>t</em><em>e</em><sub><em>k</em></sub></span>
            at a certain point in time and we want to make a prediction
            for its future state, <span
            class="math inline"><em>s</em><em>t</em><em>a</em><em>t</em><em>e</em><sub><em>k</em> + 1</sub></span>,</p>
            <p><img src="img/ekf/bayes_car.2.png" width="200"></p>
            <p><br></p>
            <p>To simplify matters, assume:</p>
            <ul>
            <li>assume car moving along <em>one</em> axis,
            <em>e.g.,</em> x-axis</li>
            <li>follow <strong>discrete</strong> position: <span
            class="math inline">0, 1, 2, .., 9</span></li>
            <li>this is a “circular” track: a <span
            class="math inline">+1</span> move from <span
            class="math inline">9 → 0</span></li>
            </ul>
            <p>So, we essentially have,</p>
            <p><img src="img/ekf/bayes_car.3.png" width="400"></p>
            <p><br></p>
            <p>Now, the car starts moving. Let’s assume the existence of
            <strong>another sensor</strong> that tells us about the
            car’s movement, <em>i.e.,</em></p>
            <table>
            <thead>
            <tr>
            <th>sensor output</th>
            <th>meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>0</code></td>
            <td><strong>stay</strong></td>
            </tr>
            <tr>
            <td><code>+1</code>, <code>+2</code>, …</td>
            <td>move forward by that amount</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>We also assume this sensor is <strong>always
            correct</strong> (for now).</p>
            <p>Based on the location of the car, we can plot our
            (current) “belief” as:</p>
            <p><img src="img/ekf/bayes_car.4.png" width="300"></p>
            <p>Now, let the car move forward, say by <code>1</code>
            step.</p>
            <p><img src="img/ekf/bayes_car.5.png" width="400"></p>
            <p>How does this update the belief?</p>
            <p><img src="img/ekf/bayes_car.6.png" width="400"></p>
            <p>This is a simple case where the car has
            <em>obviously</em> moved forward by <code>1</code> step and
            we can verify it. so the belief is updated accordingly.</p>
            <p>Now, let’s assume that the motion sensor is more
            realistic and has <strong>noise</strong> with the following
            probabilities:</p>
            <table>
            <thead>
            <tr>
            <th>probability</th>
            <th>meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">80%</span></td>
            <td>correct reading</td>
            </tr>
            <tr>
            <td><span class="math inline">10%</span></td>
            <td>overestimate</td>
            </tr>
            <tr>
            <td><span class="math inline">10%</span></td>
            <td>underestimate</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/ekf/bayes_car.7.png" width="400"></p>
            <p>If the sensor says the <strong>number</strong> of moves
            is <code>+3</code>,</p>
            <ul>
            <li>Car is <span class="math inline">80%</span> likely to
            have moved → <code>+3</code></li>
            <li>Car is <span class="math inline">10%</span> likely to
            have moved → <code>+2</code></li>
            <li>Car is <span class="math inline">10%</span> likely to
            have moved → <code>+4</code></li>
            </ul>
            <p>Let’s look at our belief as a result of this update:</p>
            <p><img src="img/ekf/bayes_car.8.png" width="400"></p>
            <p>The <strong>updated belief</strong> looks like:</p>
            <p><img src="img/ekf/bayes_car.9.png" width="400"></p>
            <p>The question is: → <strong>how</strong>?</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \operatorname{Pr}\left(x_{1}=5\right)= \\
            &amp; \operatorname{Pr}\left(x_{0}=1\right)
            \operatorname{Pr}(\text { under })+ \\
            &amp; \operatorname{Pr}\left(x_{0}=2\right)
            \operatorname{Pr}(\text { correct }) \\
            &amp; =0.5 \times 0.1+0.5 \times 0.8=0.45
            \end{aligned}
            $$</span></p>
            <p>This comes from the <strong><a
            href="#review-of-probability-theory">law of total
            probability</a></strong>:</p>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub><em>k</em></sub>) = ∑<sub><em>i</em></sub><em>p</em>(<em>x</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>k</em> − 1</sub> = <em>i</em>)<em>p</em>(<em>x</em><sub><em>k</em> − 1</sub> = <em>i</em>)</span></p>
            <p>So, after <code>3</code> moves, the
            <strong>predicted</strong> probability estimate is:</p>
            <p><img src="img/ekf/bayes_car.10.png" width="400"></p>
            <p><br></p>
            <p>Let’s keep going on…</p>
            <p><img src="img/ekf/bayes_car.13.png" width="400"></p>
            <p><br></p>
            <p>Now, what happens after → <strong><code>20</code>
            predictions</strong> ?</p>
            <p><img src="img/ekf/bayes_car.15.png" width="400"></p>
            <p><br></p>
            <p>Wait, what is happening? Why did the probabilities
            collapse into a lower spread?</p>
            <p>Well, <strong>sensors are imperfect</strong>! Hence,
            <strong>information is lost</strong>. As a result, we may
            end up with the sitauation where the actual state
            <strong>differs</strong> from the predicted state.</p>
            <p><img src="img/ekf/bayes_car.16.png" width="400"></p>
            <p><br></p>
            </section>
            <section id="bayes-filter-measurement-update" class="level3"
            data-number="8.2.2">
            <h3 data-number="8.2.2"><span
            class="header-section-number">8.2.2</span> Bayes filter |
            <strong>Measurement Update</strong></h3>
            <p>We use a <strong>measurement</strong> to “fix” the
            problem of divergence between the actual state and predicted
            state. So, in essence, we get some <strong>feedback</strong>
            from the real world → using <strong>sensors</strong> (same
            or different type). <a href="#control-theory">Sounds
            familiar</a>?</p>
            <p><img src="img/ekf/bayes_car.17.png" width="400"></p>
            <p><br></p>
            <p><strong>Prior</strong>: probability prior to
            incorporating measurement → <strong>equally likely</strong>
            to be in any position, <em>i.e.,</em> <span
            class="math inline">$\frac{1}{10}$</span> for every
            position. Let’s put aside prediction, for now.</p>
            <p><img src="img/ekf/bayes_measure.1.png" width="400"></p>
            <p><br></p>
            <p>Let the <strong>first</strong> sensor reading → <span
            class="math inline"><em>z</em><sub>0</sub> = 2</span>. But
            the sensor is <strong>noisy</strong>. Let’s also assume it
            has a probability of <span
            class="math inline">90%</span>.</p>
            <blockquote>
            <p>Note: in the “predict” stage, we were using “noisy”
            sensors as well, but those were measuring
            <strong>different</strong> quantities. In that case, the
            sensor could be from the car engine or the wheels (or
            odometers) that say how much we “<strong>intended</strong>”
            to move, _e.g., “we generated enough torque in the engine to
            move forward by <code>3</code> slots. <br> In contrast, the
            sensors in this section <strong>measure</strong> →”how much
            was <strong>actually</strong> moved?“. This could be via
            other types of sensors, <em>e.g.,</em> GPS. <br>
            <strong>Both</strong> of these sensors can, and will, of
            course have noise and probabilities associated with them.
            The question then, is →”how to <strong>reconcile</strong>
            the two?”</p>
            </blockquote>
            <p>So now, what is the <strong>new belief</strong>? From
            this setup, intuitively, it seems → <span
            class="math inline"><em>x</em><sub>0</sub> = 2 is 9 times
            likely than <em>x</em><sub>0</sub> ≠ 2</span>.</p>
            <p>Time to use <strong><a
            href="#review-of-probability-theory">Bayes
            Rule</a></strong>:</p>
            <p><img src="img/ekf/equations/pngs/equations-0.png" width="500"></p>
            <p><br></p>
            <p>where, <span
            class="math inline"><em>p</em>(<em>z</em><sub><em>k</em></sub>) = ∑<sub><em>x</em><sub><em>i</em></sub></sub><em>p</em>(<em>z</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>i</em></sub>)<em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>
            (as we’ve seen before, from the law of total
            probability).</p>
            <p>So now, we get the <strong>likelihood</strong>, <span
            class="math inline"><em>p</em>(<em>z</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>k</em></sub>)</span>,</p>
            <p><img src="img/ekf/bayes_measure.2.png" width="300"></p>
            <p><br></p>
            <p><strong>Question</strong>: what if the probability was
            <span class="math inline">80%</span>?</p>
            <p>Carrying on, the numerator on the right hand side,
            <em>i.e.,</em> <span
            class="math inline"><em>p</em>(<em>z</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>k</em></sub>) × <em>p</em>(<em>x</em><sub><em>k</em></sub>)</span>:</p>
            <p><img src="img/ekf/bayes_measure.3.png" width="500"></p>
            <p><br></p>
            <p>The result of which is:</p>
            <p><img src="img/ekf/bayes_measure.4.png" width="300"></p>
            <p><br></p>
            <p>which puts our car at the <strong>right
            spot</strong>:</p>
            <p><img src="img/ekf/bayes_car.3.png" width="400"></p>
            <p><br></p>
            <p>We’re still not done. We have to compute the denominator,
            <span
            class="math inline"><em>p</em>(<em>z</em><sub><em>k</em></sub>) = ∑<sub><em>x</em><sub><em>i</em></sub></sub><em>p</em>(<em>z</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>i</em></sub>)<em>p</em>(<em>x</em><sub><em>i</em></sub>)</span>,
            which essentially <strong>normalizes</strong> the
            belief:</p>
            <p><img src="img/ekf/bayes_measure.5.png" width="300"></p>
            <p><br></p>
            <p>So our final value, the new belief, aka
            <strong>posterior</strong>, <span
            class="math inline"><em>p</em>(<em>x</em><sub><em>k</em></sub> ∣ <em>z</em><sub><em>k</em></sub>)</span>
            → probability <strong>after incorporating
            measurement</strong>.</p>
            <p>Let’s look at the <strong>formal calculations</strong>
            for each:</p>
            <p>If we want to calculate,</p>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub>0</sub> = 2 ∣ <em>z</em><sub>0</sub> = 2) = ▫</span></p>
            <p>Recall,</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; p\left(z_{k}=a \mid x_{k}=a\right)=90 \% \\
            &amp; p\left(z_{k}=a \mid x_{k}=b\right)=10 \%
            \end{aligned}
            $$</span></p>
            <p><em>i.e.,</em> sensor is correct with probability of
            <span class="math inline">90%</span> → the belief that the
            measurement <span
            class="math inline"><em>z</em><sub><em>k</em></sub> = <em>a</em></span>
            matches reality <span
            class="math inline"><em>x</em><sub><em>k</em></sub> = <em>a</em></span>
            is <span class="math inline">90%</span> otherwise it is
            <span class="math inline">10%</span>. For instance,</p>
            <table>
            <thead>
            <tr>
            <th>measurement (<span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>)</th>
            <th>reality (<span
            class="math inline"><em>x</em><sub><em>k</em></sub></span>)</th>
            <th>belief (prob %)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>z</em><sub><em>k</em></sub> = 2</span></td>
            <td><span
            class="math inline"><em>x</em><sub><em>k</em></sub> = 2</span></td>
            <td><span class="math inline">90%</span></td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>z</em><sub><em>k</em></sub> = 2</span></td>
            <td><span
            class="math inline"><em>x</em><sub><em>k</em></sub> = 5</span></td>
            <td><span class="math inline">10%</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><span class="math display">$$
            p\left(x_{0}=2 \mid z_{0}=2\right)=\frac{p\left(z_{0}=2 \mid
            x_{0}=2\right) \times
            p\left(x_{0}=2\right)}{p\left(z_{0}=2\right)}=\frac{\textbf{0.9}
            \times
            0.1}{p\left(z_{0}=2\right)}=\frac{0.09}{p\left(z_{0}=2\right)}
            $$</span></p>
            <p><br></p>
            <p>In the same vein,</p>
            <p><span class="math display">$$
            p\left(x_{0}=1 \mid z_{0}=2\right)=\frac{p\left(z_{0}=2 \mid
            x_{0}=1\right) \times
            p\left(x_{0}=1\right)}{p\left(z_{0}=2\right)}=\frac{\textbf{0.1}
            \times
            0.1}{p\left(z_{0}=2\right)}=\frac{0.01}{p\left(z_{0}=2\right)}
            $$</span></p>
            <p><br></p>
            <p>Use this formula, we can find <span
            class="math inline"><em>p</em>(<em>x</em><sub>0</sub> = <em>x</em> ∣ <em>z</em><sub>0</sub> = 2)</span>
            → for all <code>x</code>.</p>
            <p>But we still need to compute → <span
            class="math inline"><em>p</em>(<em>z</em><sub>0</sub> = 2)</span>.</p>
            <p>Recalling the law of total probability, so we can
            compute:</p>
            <p><span
            class="math display"><em>p</em>(<em>z</em><sub>0</sub> = 2) = ∑<sub><em>x</em></sub><em>p</em>(<em>z</em><sub>0</sub> = 2 ∣ <em>x</em><sub>0</sub> = <em>x</em>)<em>p</em>(<em>x</em><sub>0</sub> = <em>x</em>)</span></p>
            <p>From the above table,</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; p\left(z_{k}=a \mid x_{k}=a\right)=90 \% \\
            &amp; p\left(z_{k}=a \mid x_{k}=b\right)=10 \%
            \end{aligned}
            $$</span></p>
            <p>Plugging in the values,</p>
            <p><span class="math display">$$
            \begin{aligned}
            p\left(z_{0}=2\right) &amp; =\sum_{x} p\left(z_{0}=2 \mid
            x_{0}=x\right) p\left(x_{0}=x\right) \\
            &amp; =0.09+9 \times 0.01=\textbf{0.18}
            \end{aligned}
            $$</span></p>
            <p>Hence, the final, <strong>normalized</strong> probability
            distribution looks like:</p>
            <p><img src="img/ekf/bayes_measure.6.png" width="300"></p>
            <p><br></p>
            <p>Now, the sensor gives again <span
            class="math inline"><em>z</em><sub>1</sub> = 2</span> → same
            likelihood (<span class="math inline">90%</span> correct),
            <em>i.e.,</em></p>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub>1</sub> ∣ <em>z</em><sub>1</sub> = 2)</span></p>
            <p>Then, what’s our new belief (using the previous belief as
            the <strong>prior</strong>), <span
            class="math inline">Pr (<em>x</em><sub>1</sub> = 2 ∣ <em>z</em><sub>1</sub>) = 90.00%</span>?</p>
            <p>For the next iteration, we get:</p>
            <p><span
            class="math display">Pr (<em>x</em><sub>2</sub> = 2 ∣ <em>z</em><sub>2</sub>) = <strong>98.78</strong>%</span></p>
            <p><img src="img/ekf/bayes_measure.8.png" width="300"></p>
            <p><br></p>
            <p>We can keep calculating this for various values of
            <code>k</code>:</p>
            <p><img src="img/ekf/bayes_measure.9.png" width="500"></p>
            <p><br></p>
            <p>What is happening here? Why is only one bar increasing?
            Nothing ele changes?</p>
            <p><strong>Answer</strong>: the car is stationary at
            location <code>2</code>!</p>
            <p><img src="img/ekf/bayes_car.3.png" width="400"></p>
            <p><br></p>
            <p>If we <strong>drop the sensor accuracy</strong> to say,
            <span class="math inline">60%</span>, we see:</p>
            <p><img src="img/ekf/bayes_measure.10.png" width="500"></p>
            <p><br></p>
            <p>The confidence in the measurement increases, albeit at a
            much slower rate.</p>
            <p>Now, if we drop it further, to say, <span
            class="math inline">50%</span>,</p>
            <p><img src="img/ekf/bayes_measure.12.png" width="500"></p>
            <p><br></p>
            <p>Why is nothing changing? Because the sensor is
            <strong>not providing any additional information</strong>!
            The <span class="math inline">50%</span> rate keeps it at
            the same belief level as everything else.</p>
            </section>
            <section
            id="bayes-filter-combining-prediction-and-measurement"
            class="level3" data-number="8.2.3">
            <h3 data-number="8.2.3"><span
            class="header-section-number">8.2.3</span> Bayes filter |
            <strong>Combining</strong> Prediction and Measurement</h3>
            <ul>
            <li>prediction <strong>loses</strong> information</li>
            <li>measurement <strong>improves</strong> knowledge (i.e.,
            decreases uncertainty)</li>
            </ul>
            <p>Let’s <strong>combine</strong> prediction and
            measurement!</p>
            <p>We start with the same initial position and sensor
            accuracies:</p>
            <ul>
            <li>initial position = <code>2</code></li>
            <li>motion model:</li>
            </ul>
            <table>
            <thead>
            <tr>
            <th>sensor output</th>
            <th>meaning</th>
            <th>accuracy</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>0</code></td>
            <td><strong>stay</strong></td>
            <td><span class="math inline">10%</span></td>
            </tr>
            <tr>
            <td><code>+1</code></td>
            <td>move forward by <code>1</code></td>
            <td><span class="math inline">80%</span></td>
            </tr>
            <tr>
            <td><code>+2</code></td>
            <td>move forward by <code>2</code></td>
            <td><span class="math inline">10%</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>This <strong>motion model</strong> can be represented
            as:</p>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>k</em> − 1</sub> = <em>i</em>)</span></p>
            <p><br></p>
            <p><em>i.e.,</em> what is the probability of <span
            class="math inline"><em>x</em><sub><em>k</em></sub></span>,
            given <span
            class="math inline"><em>x</em><sub><em>k</em> − 1</sub> = <em>i</em></span>,
            represented visually as,</p>
            <p><img src="img/ekf/bayes_combined.1.png" width="400"></p>
            <p><br></p>
            <p>And given the <strong>posterior</strong> at time <span
            class="math inline"><em>k</em> − 1</span>,
            <em>i.e.,</em></p>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub><em>k</em> − 1</sub> = <em>i</em> ∣ <em>z</em><sub><em>k</em> − 1</sub>)</span></p>
            <p>The <strong>combined</strong> model, <em>i.e.,</em> the
            <strong>new prior</strong>, is:</p>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub><em>k</em></sub>) = ∑<sub><em>i</em></sub><em>p</em>(<em>x</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>k</em> − 1</sub> = <em>i</em>)<em>p</em>(<em>x</em><sub><em>k</em> − 1</sub> = <em>i</em> ∣ <em>z</em><sub><em>k</em> − 1</sub>)</span></p>
            <p>Let’s start with the <strong>initial belief</strong>,
            <em>viz.,</em></p>
            <p><img src="img/ekf/bayes_combined.2.png" width="200"></p>
            <p><br></p>
            <p>At time, <span class="math inline"><em>t</em> = 0</span>,
            the predictions and updates:</p>
            <p><img src="img/ekf/bayes_combined.6.png" width="500"></p>
            <p><br></p>
            <p>Note that the end result is the <strong>new
            posterior</strong> → that we will use in the future cycles
            as,</p>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub><em>k</em></sub>) = ∑<sub><em>i</em></sub><em>p</em>(<em>x</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>k</em> − 1</sub> = <em>i</em>)<em>p</em>(<em>x</em><sub><em>k</em> − 1</sub> = <em>i</em> ∣ <em>z</em><sub><em>k</em> − 1</sub>)</span>.</p>
            <p>Hence we see:</p>
            <p><img src="img/ekf/bayes_combined.8.png" width="500"></p>
            <p><br></p>
            <p>The confidence in the prediction and belief in the system
            both increase, as we see in the next cycle:</p>
            <p><img src="img/ekf/bayes_combined.9.png" width="500"></p>
            <p><br></p>
            <p><strong>Note:</strong> when the measurement matches the
            prediction, the uncertainty decreases.</p>
            <p>Now what happens if → the car <strong>did not
            move</strong>? We see:</p>
            <p><img src="img/ekf/bayes_combined.10.png" width="500"></p>
            <p><br></p>
            <p>The true position remains same (<code>5</code>), but the
            prediction is still based on the original motion model:</p>
            <table>
            <thead>
            <tr>
            <th>sensor output</th>
            <th>meaning</th>
            <th>accuracy</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><code>0</code></td>
            <td><strong>stay</strong></td>
            <td><span class="math inline">10%</span></td>
            </tr>
            <tr>
            <td><code>+1</code></td>
            <td>move forward by <code>1</code></td>
            <td><span class="math inline">80%</span></td>
            </tr>
            <tr>
            <td><code>+2</code></td>
            <td>move forward by <code>2</code></td>
            <td><span class="math inline">10%</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>The <strong>uncertainty increases</strong>! But still the
            estimate <span class="math inline">≈</span> true state.</p>
            <p>We keep going…</p>
            <p><img src="img/ekf/bayes_combined.11.png" width="500"></p>
            <p><br></p>
            <p>Let’s see what happens if we don’t follow the motion
            model, <em>i.e.,</em> the car <strong>moved by
            <code>2</code></strong> (instead of <code>1</code>):</p>
            <p><img src="img/ekf/bayes_combined.12.png" width="500"></p>
            <p><br></p>
            <p>The <strong>true position</strong> → <code>8</code> but
            the prediction is still based on the original motion model
            hence, the confidence drops.</p>
            <p>Getting back to the original motion model, one last
            time,</p>
            <p><img src="img/ekf/bayes_combined.13.png" width="500"></p>
            <p><br></p>
            <p>We get back to the high confidence levels, <strong>really
            quickly</strong>!</p>
            <p>To <strong>summarize</strong>, the steps in the Bayes
            Filter are:</p>
            <p><img src="img/ekf/bayes/bayes_filter.final.png" width="300"></p>
            <p><br></p>
            <ol type="1">
            <li><strong>predict</strong>:
            <ul>
            <li>calculate the prior, <span
            class="math inline"><em>p</em>(<em>x</em><sub><em>k</em></sub>)</span>,
            from the previous posterior, <span
            class="math inline"><em>p</em>(<em>x</em><sub><em>k</em> − 1</sub> ∣ <em>z</em><sub><em>k</em> − 1</sub>)</span></li>
            <li>by incorporating the motion (process) model,</li>
            </ul></li>
            </ol>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub><em>k</em></sub>) = ∑<sub><em>i</em></sub><em>p</em>(<em>x</em><sub><em>k</em></sub> ∣ <em>x</em><sub><em>k</em> − 1</sub> = <em>i</em>)<em>p</em>(<em>x</em><sub><em>k</em> − 1</sub> = <em>i</em> ∣ <em>z</em><sub><em>k</em> − 1</sub>)</span></p>
            <ol start="2" type="1">
            <li><strong>update</strong>:
            <ul>
            <li>given a measurement, <span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>,
            compute the likelihood</li>
            <li>from the likelihood and prior, apply Bayes Rule to
            update the belief:</li>
            </ul></li>
            </ol>
            <p><span class="math display">$$
            p\left(x_{k} \mid z_{k}\right)=\frac{p\left(z_{k} \mid
            x_{k}\right) \times
            p\left(x_{k}\right)}{p\left(z_{k}\right)}
            $$</span></p>
            <p>where, <span
            class="math inline"><em>p</em>(<em>z</em><sub><em>k</em></sub>) = ∑<sub><em>x</em></sub><em>p</em>(<em>z</em><sub><em>k</em></sub> ∣ <em>x</em>)<em>p</em>(<em>x</em>)</span>.</p>
            <p><strong>Limitations</strong>:</p>
            <p>While the Bayesian filter works for many instances, the
            main problem is that it is <strong>discrete</strong> in
            nature. Consider the example for robots that need a <span
            class="math inline">1<em>c</em><em>m</em></span> resolution
            in a <span class="math inline">100<em>m</em></span> space →
            <span class="math inline">10, 000</span> positions needed to
            model the environment!</p>
            <p><strong>Scaling</strong> is much harder, especially in
            the multidimensional systems. For instance, how do we model
            the original state of the car that we discussed earlier:</p>
            <p><span class="math display">$$
            \boldsymbol{x}_{\boldsymbol{k}}=(x, \dot{x}, \ddot{x}, y,
            \dot{y}, \ddot{y}, \theta)
            $$</span></p>
            <p>We need to <strong>update</strong> the Bayes model itself
            to handle such situations.</p>
            <p>Enter the <strong><a href="#kalman-filter">Kalman
            Filter</a></strong>.</p>
            </section>
            </section>
            <section id="kalman-filter" class="level2"
            data-number="8.3">
            <h2 data-number="8.3"><span
            class="header-section-number">8.3</span> Kalman Filter</h2>
            <p>To deal with some of the issues with the basic Bayes
            filter, we introduce the <strong>Kalman Filter</strong> that
            express state and uncertainty using
            <strong>Gaussians</strong> (aka the “<a
            href="https://en.wikipedia.org/wiki/Normal_distribution">normal</a>”
            distribution).</p>
            <p><img src="img/ekf/kalman/discrete_bayes.png" width="200">
            <img src="img/ekf/kalman/kalman_gaussian.png" width="200"></p>
            <p><br></p>
            <section id="gaussian-distribution" class="level3"
            data-number="8.3.1">
            <h3 data-number="8.3.1"><span
            class="header-section-number">8.3.1</span> Gaussian
            Distribution</h3>
            <p>Now a quick detour on Gaussians. A normal distribution or
            Gaussian distribution is a type of <strong>continuous
            probability distribution</strong> for a <strong>real-valued
            random variable</strong> and is represented as:</p>
            <p><img src="img/ekf/equations/pngs/equations-1.png" width="500"></p>
            <!--
            $$
            \mathcal{N}\left(\mu, \sigma^{2}\right)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} 
            $$
            -->
            <p><br></p>
            <p>A very common Gaussian can be visualized as:</p>
            <p><img src="img/ekf/kalman/normal_distribution_wiki.svg" width="400"></p>
            <p><br></p>
            <p>Note that <strong>variance</strong> (<span
            class="math inline"><em>σ</em><sup>2</sup></span>) captures
            the <strong>uncertainty</strong> in the system and is the
            <strong>square</strong> of the standard deviation (<span
            class="math inline"><em>σ</em></span>).</p>
            <p><img src="img/ekf/kalman/variance_uncertainty_examples.png" width="400"></p>
            <p><br></p>
            <p>More specific examples:</p>
            <p><img src="img/ekf/kalman/uncertainty.1.png" width="200">
            <img src="img/ekf/kalman/uncertainty.2.png" width="200">
            <img src="img/ekf/kalman/uncertainty.3.png" width="200"></p>
            <p><br></p>
            <p>Some properties of Gaussians:</p>
            <ol type="1">
            <li><strong>area</strong> under the curve = <code>1</code>
            (since it is the sum of all the probabilities)</li>
            </ol>
            <p><span
            class="math display"><em>a</em><em>r</em><em>e</em><em>a</em> = ∫<sub>−∞</sub><sup>+∞</sup>𝒩(<em>μ</em>, <em>σ</em><sup>2</sup>)</span></p>
            <p>Note that the curve approaches <strong>infinity</strong>
            (<span class="math inline">∞</span>) on <strong>either
            side</strong> → the probability of certain events is
            <strong>never <code>0</code></strong>, no matter how
            small.</p>
            <p>Hence,</p>
            <p><span class="math display">$$
            area = \int_{-\infty}^{+\infty} \frac{1}{\sqrt{2 \pi
            \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} =
            \textbf{1}
            $$</span></p>
            <ol start="2" type="1">
            <li>probability that a value, <span
            class="math inline"><em>x</em></span> → is in a
            <strong>range</strong>, <span
            class="math inline">(<em>a</em>, <em>b</em>)</span>:</li>
            </ol>
            <p><span class="math display">$$
            \operatorname{Pr}(a \leq x \leq b)=\int_{a}^{b}
            \frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2
            \sigma^{2}}} d x
            $$</span></p>
            <ol start="3" type="1">
            <li>since it is often useful to find the probability
            <strong>within one standard deviation</strong> (on either
            side),</li>
            </ol>
            <p><span class="math display">$$
            \operatorname{Pr}(\mu-\sigma \leq x \leq
            \mu+\sigma)=\int_{\mu-\sigma}^{\mu+\sigma} \frac{1}{\sqrt{2
            \pi \sigma^{2}}} e^{-\frac{(x-\mu)^{2}}{2 \sigma^{2}}} d x
            \approx \textbf{68 \%}
            $$</span></p>
            <ol start="4" type="1">
            <li><strong>sum</strong> and <strong>product</strong> of two
            Gaussian distributions is fairly easy to calculate</li>
            </ol>
            <p>Given, two Gaussians,</p>
            <p><span
            class="math inline"><em>X</em><sub>1</sub> ∼ 𝒩(<em>μ</em><sub>1</sub>, <em>σ</em><sub>1</sub><sup>2</sup>)</span></p>
            <p><span
            class="math inline"><em>X</em><sub>2</sub> ∼ 𝒩(<em>μ</em><sub>2</sub>, <em>σ</em><sub>2</sub><sup>2</sup>)</span></p>
            <table>
            <colgroup>
            <col style="width: 31%" />
            <col style="width: 26%" />
            <col style="width: 42%" />
            </colgroup>
            <thead>
            <tr>
            <th>result</th>
            <th>sum</th>
            <th>product</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>new</strong> Gaussian, <span
            class="math inline"><em>Z</em> ∼ 𝒩(<em>μ</em>, <em>σ</em><sup>2</sup>)</span></td>
            <td><span
            class="math inline"><em>Z</em> = <em>X</em><sub>1</sub> + <em>X</em><sub>2</sub></span></td>
            <td><span
            class="math inline"><em>Z</em> = <em>X</em><sub>1</sub><em>X</em><sub>2</sub></span></td>
            </tr>
            <tr>
            <td>new <strong>mean</strong>, <span
            class="math inline"><em>μ</em></span></td>
            <td><span
            class="math inline"><em>μ</em><sub>1</sub> + <em>μ</em><sub>2</sub></span></td>
            <td><span class="math inline">$\mu=\frac{\sigma_{2}^{2}
            \mu_{1}+\sigma_{1}^{2}
            \mu_{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}$</span></td>
            </tr>
            <tr>
            <td>new <strong>variance</strong>, <span
            class="math inline"><em>σ</em><sup>2</sup></span></td>
            <td><span
            class="math inline"><em>σ</em><sub>1</sub><sup>2</sup> + <em>σ</em><sub>2</sub><sup>2</sup></span></td>
            <td><span
            class="math inline">$\sigma^{2}=\frac{\sigma_{1}^{2}
            \sigma_{2}^{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <ol start="5" type="1">
            <li><strong>closure</strong> under <strong>linear
            transformations</strong></li>
            </ol>
            <p>From <a
            href="https://soulhackerslabs.com/sensor-fusion-with-the-extended-kalman-filter-in-ros-2-d33dbab1829d">Probabilistic
            Robotics</a>:</p>
            <p>The advantage of using Gaussians (or Normals) lies in
            their mathematical properties, which simplify the Kalman
            Filter equations. A key property is their closure under
            linear transformations: when a Gaussian belief undergoes a
            linear transformation, the <strong>result remains a Gaussian
            random variable</strong>! This property ensures that the
            equations of the Kalman Filter remain,</p>
            <ul>
            <li>elegant and</li>
            <li>manageable</li>
            </ul>
            </section>
            <section id="state-in-kalman-filters" class="level3"
            data-number="8.3.2">
            <h3 data-number="8.3.2"><span
            class="header-section-number">8.3.2</span>
            <strong>State</strong> in Kalman Filters</h3>
            <p>So, we can define state in this model as:</p>
            <p><img src="img/ekf/kalman/kalman.state.png" width="400"></p>
            <p><br></p>
            <p>The <strong>prior</strong> ↔︎ <strong>measurement</strong>
            ↔︎ <strong>update</strong> process looks like:</p>
            <p><img src="img/ekf/kalman/kalman.car.state.png" width="300"></p>
            <p><br></p>
            <p>As we see from this figure, the prediction and
            measurements are not just single points but a
            <strong>distribution</strong>.</p>
            <p>The various transitions look like:</p>
            <p><img src="img/ekf/kalman/kalman.final.png" width="300"></p>
            <p><br></p>
            <p>Essentially the same as the Bayes Filter → the difference
            being that each of the edges is now a probabilistic Gaussian
            value.</p>
            </section>
            <section id="kalman-filter-prediction" class="level3"
            data-number="8.3.3">
            <h3 data-number="8.3.3"><span
            class="header-section-number">8.3.3</span> Kalman Filter |
            <strong>Prediction</strong></h3>
            <p>First of all, we note that the <strong>process
            model</strong>, <em>i.e.,</em> the model of how the system
            evolves over time (essentially the physics) is also a
            Gaussian! For instance, in the equations of motion, →
            <em>velocity</em> follows a Gaussian distribution.</p>
            <p>Suppose we want to track the motion of this car:</p>
            <p><img src="img/ekf/kalman/kalman.predict.1.png" width="300"></p>
            <p><br></p>
            <p>From Newton’s laws of motion we can
            <strong>predict</strong> the next state using the following
            <strong>process model</strong>:</p>
            <p><span class="math display">$$
            \overline{x_{k+1}}=x_{k} +v_{k} \Delta t
            $$</span></p>
            <p>where,</p>
            <table>
            <thead>
            <tr>
            <th>variable</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>x</em><sub><em>k</em></sub></span></td>
            <td>current state</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>x</em><sub><em>k</em> + 1</sub></span></td>
            <td><strong>predicted</strong> next state</td>
            </tr>
            <tr>
            <td><strong><span
            class="math inline"><em>v</em><sub><em>k</em></sub></span></strong></td>
            <td>velocity</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>Δ</em><em>t</em></span></td>
            <td>time difference</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Since velocity follows a Gaussian distribution, let’s
            assume,</p>
            <p><span
            class="math display"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 1<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span></p>
            <p>Now, if the current state, <span
            class="math inline"><em>x</em><sub><em>k</em></sub></span>
            is a Gaussian, as we’ve established before,</p>
            <p><img src="img/ekf/kalman/kalman.predict.3.png" width="300"></p>
            <p><br></p>
            <p>Now clearly, <span
            class="math inline"><em>x</em><sub><em>k</em> + 1</sub></span>
            has to be a Gaussian as well → since at least one of the
            terms on the right is a Gaussian, <em>viz.,</em> the current
            state <strong>and</strong> velocity.</p>
            <p>So, the question is → what is the value for,</p>
            <p><img src="img/ekf/kalman/kalman.predict.4.png" width="300"></p>
            <p><br></p>
            <p><span class="math display">$$
            \overline{x_{k+1}} \sim \mathcal{N}\left(? \mathrm{~m}, ?
            \mathrm{~m}^{2}\right)
            $$</span></p>
            <p>Recall, that the sum of two Gaussians is,</p>
            <p><span class="math display">$$
            \begin{aligned}
            \mu &amp; =\mu_{1}+\mu_{2} \\
            \sigma^{2} &amp; =\sigma_{1}^{2}+\sigma_{2}^{2}
            \end{aligned}
            $$</span></p>
            <p>Assume that <span
            class="math inline"><em>Δ</em><em>t</em> = 1<em>s</em></span>.
            Plugging in thes values,</p>
            <p><span class="math display">$$
            \overline{x_{k+1}} \sim \mathcal{N}\left(6.5 \mathrm{~m}, 2
            \mathrm{~m}^{2}\right)
            $$</span></p>
            <p>In this example, the second parameter in the velocity
            term is the <strong>process noise</strong> in the physics
            model:</p>
            <p><img src="img/ekf/kalman/kalman.predict.6.png" width="300"></p>
            <p><br></p>
            <p>*Important Notes<strong>: Kalman Filter is
            </strong>unimodel<strong>, <em>i.e.,</em> there is a
            </strong>single peak** each time.</p>
            <p><img src="img/ekf/kalman/kalman_unimodal.png" width="300"></p>
            <p><br></p>
            <p>An obstacle is <strong>not</strong> both, <span
            class="math inline">10<em>m</em></span> away with <span
            class="math inline">90%</span> and <span
            class="math inline">8<em>m</em></span> away with <span
            class="math inline">70%</span> probability. It is:</p>
            <ul>
            <li><strong><span class="math inline">9.7<em>m</em></span>
            away with <span class="math inline">98%</span></strong>
            or</li>
            <li><strong>nothing</strong></li>
            </ul>
            <p>Let’s look at some examples for prediction. Consider the
            following velocity models:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <tbody>
            <tr>
            <td><strong>A</strong></td>
            <td><span
            class="math display"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 0<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span></td>
            </tr>
            <tr>
            <td><strong>B</strong></td>
            <td><span
            class="math display"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 1<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span></td>
            </tr>
            <tr>
            <td><strong>C</strong></td>
            <td><span
            class="math display"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 2<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>And we have the following *predicted** states:</p>
            <table>
            <colgroup>
            <col style="width: 55%" />
            <col style="width: 44%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction</th>
            <th>velocity model</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.7.png" width="300"></td>
            <td><br> <strong>A</strong>, <strong>B</strong> or
            <strong>C</strong> ?</td>
            </tr>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.8.png" width="300"></td>
            <td><strong>A</strong>, <strong>B</strong> or
            <strong>C</strong> ?</td>
            </tr>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.9.png" width="300"></td>
            <td><strong>A</strong>, <strong>B</strong> or
            <strong>C</strong> ?</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>So, which one of the above is &amp;rarr,
            <strong>A</strong>, <strong>B</strong> and
            <strong>C</strong>? Let us plug in the values:</p>
            <table>
            <colgroup>
            <col style="width: 55%" />
            <col style="width: 44%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction</th>
            <th>velocity model</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.7.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 1<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>B</strong>]</td>
            </tr>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.8.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 0<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>A</strong>]</td>
            </tr>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.9.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 2<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>C</strong>]</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>So, what does the predicted state look like?</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 26%" />
            <col style="width: 40%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction</th>
            <th>velocity model</th>
            <th>predicted state</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.7.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 1<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>B</strong>]</td>
            <td><span class="math inline">$\overline{x_{k+1}} \sim
            \mathcal{N}\left(6.5 \mathrm{~m}, ?
            \mathrm{~m}^{2}\right)$</span></td>
            </tr>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.8.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 0<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>A</strong>]</td>
            <td><span class="math inline">$\overline{x_{k+1}} \sim
            \mathcal{N}\left(6.5 \mathrm{~m}, ?
            \mathrm{~m}^{2}\right)$</span></td>
            </tr>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.9.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 2<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>C</strong>]</td>
            <td><span class="math inline">$\overline{x_{k+1}} \sim
            \mathcal{N}\left(6.5 \mathrm{~m}, ?
            \mathrm{~m}^{2}\right)$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Coming back to the sum of two Gaussians, <span
            class="math inline"><em>σ</em><sup>2</sup> = <em>σ</em><sub>1</sub><sup>2</sup> + <em>σ</em><sub>2</sub><sup>2</sup></span>,
            and the current state, <span
            class="math inline">$\overline{x_{k}} \sim
            \mathcal{N}\left(3.5 \mathrm{~m}, 1
            \mathrm{~m}^{2}\right)$</span>,</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 26%" />
            <col style="width: 40%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction</th>
            <th>velocity model</th>
            <th>predicted state</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.7.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 1<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>B</strong>]</td>
            <td><span class="math inline">$\overline{x_{k+1}} \sim
            \mathcal{N}\left(6.5 \mathrm{~m}, 2
            \mathrm{~m}^{2}\right)$</span></td>
            </tr>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.8.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 0<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>A</strong>]</td>
            <td><span class="math inline">$\overline{x_{k+1}} \sim
            \mathcal{N}\left(6.5 \mathrm{~m}, 1
            \mathrm{~m}^{2}\right)$</span></td>
            </tr>
            <tr>
            <td><img src="img/ekf/kalman/kalman.predict.9.png" width="300"></td>
            <td><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(3 m/s, 2<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            [<strong>C</strong>]</td>
            <td><span class="math inline">$\overline{x_{k+1}} \sim
            \mathcal{N}\left(6.5 \mathrm{~m}, 5
            \mathrm{~m}^{2}\right)$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>As we see from these examples, depending on the
            <strong>noise</strong> in the intial Gaussian, the
            <strong>uncertainty increases</strong> for the predicted
            state, even when the velocity model had <code>0</code>
            noise!</p>
            <p>The problem is that we don’t know → how
            <strong>close</strong> is our prediction to reality. Hence,
            we need an <strong>update</strong> step → measurements!</p>
            </section>
            <section id="kalman-filter-update" class="level3"
            data-number="8.3.4">
            <h3 data-number="8.3.4"><span
            class="header-section-number">8.3.4</span> Kalman Filter |
            <strong>Update</strong></h3>
            <p>In the measurement (or sensor) model, the
            <strong>measurement uncertainty</strong> (aka “likelihood”)
            is also represented as a Gaussian.</p>
            <p><img src="img/ekf/equations/pngs/equations-2.png" width="500"></p>
            <p><br></p>
            <p>For instance, current temperature is <span
            class="math inline">72<sup>∘</sup>F ± 1<sup>∘</sup>F</span>.</p>
            <p>Now, recall that,</p>
            <p><span
            class="math display"><em>p</em><em>o</em><em>s</em><em>t</em><em>e</em><em>r</em><em>i</em><em>o</em><em>r</em> = <em>p</em><em>r</em><em>i</em><em>o</em><em>r</em> × <em>l</em><em>i</em><em>k</em><em>e</em><em>l</em><em>i</em><em>h</em><em>o</em><em>o</em><em>d</em></span></p>
            <p><span class="math display">$$
            posterior = \overline x_k \times z_k
            $$</span></p>
            <p>What does this <strong>actually mean</strong>?</p>
            <p>Let’s consider a few examples.</p>
            <ol type="1">
            <li>Example 1</li>
            </ol>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction (<span class="math inline">$\overline
            x_{k}$</span>)</th>
            <th>measurement (<span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>)</th>
            <th>posterior (<span class="math inline">$\overline x_{k}
            \times z_k$</span>)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">$\overline{x_{k}} \sim
            \mathcal{N}\left(5,1^{2}\right)$</span></td>
            <td><span
            class="math inline"><em>z</em><sub><em>k</em></sub> ∼ 𝒩(5, 1<sup>2</sup>)</span></td>
            <td><strong>?</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Let’s plot these two Gaussians,</p>
            <p><img src="img/ekf/kalman/kalman.update.1.png" width="300"></p>
            <p><br></p>
            <p>As we see from the figure, the two align perfectly →
            since they’re the <strong>same</strong> distribution. What
            would <span
            class="math inline">$\overline{\mathrm{x}}_{\mathrm{k}}
            \times \mathrm{z}_{\mathrm{k}}$</span> look like?</p>
            <p>Recall the <strong>product</strong> of two Gaussians,</p>
            <p><span class="math display">$$
            \begin{aligned}
            \mu &amp; =\frac{\sigma_{2}^{2} \mu_{1}+\sigma_{1}^{2}
            \mu_{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}} \\
            \sigma^{2} &amp; =\frac{\sigma_{1}^{2}
            \sigma_{2}^{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}
            \end{aligned}
            $$</span></p>
            <p>Hence, the posterior would be,</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction (<span class="math inline">$\overline
            x_{k}$</span>)</th>
            <th>measurement (<span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>)</th>
            <th>posterior (<span class="math inline">$\overline x_{k}
            \times z_k$</span>)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">$\overline{x_{k}} \sim
            \mathcal{N}\left(5,1^{2}\right)$</span></td>
            <td><span
            class="math inline"><em>z</em><sub><em>k</em></sub> ∼ 𝒩(5, 1<sup>2</sup>)</span></td>
            <td><span
            class="math inline">$\overline{\mathrm{x}}_{\mathrm{k}}
            \times \mathrm{z}_{\mathrm{k}} \sim
            \mathcal{N}(5,0.5)$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Graphically this looks like,</p>
            <p><img src="img/ekf/kalman/kalman.update.4.png" width="300"></p>
            <p><br></p>
            <p>Hence, we see the following:</p>
            <ul>
            <li>an <strong>increase</strong> in the probability around
            <code>5</code> (<strong>taller</strong> Gaussian)</li>
            <li>a <strong>decrease</strong> in the uncertainty
            (<strong>narrower</strong> Gaussian)</li>
            </ul>
            <p><br></p>
            <ol start="2" type="1">
            <li>Example 2: this time the initial Guassians differ
            slightly,</li>
            </ol>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction (<span class="math inline">$\overline
            x_{k}$</span>)</th>
            <th>measurement (<span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>)</th>
            <th>posterior (<span class="math inline">$\overline x_{k}
            \times z_k$</span>)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">$\overline{x_{k}} \sim
            \mathcal{N}\left(5,1^{2}\right)$</span></td>
            <td><span
            class="math inline"><em>z</em><sub><em>k</em></sub> ∼ 𝒩(5.4, 1<sup>2</sup>)</span></td>
            <td><span
            class="math inline">$\overline{\mathrm{x}}_{\mathrm{k}}
            \times \mathrm{z}_{\mathrm{k}} \sim
            \mathcal{N}(5.2,0.5)$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>We see that the result is in the <strong>midddle</strong>
            of the two original Gaussians.</p>
            <p><img src="img/ekf/kalman/kalman.update.5.png" width="300"></p>
            <p><br></p>
            <ol start="3" type="1">
            <li>Example 3: let’s see what happens if the Gaussian’s
            differ significantly, <em>i.e.,</em></li>
            </ol>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction (<span class="math inline">$\overline
            x_{k}$</span>)</th>
            <th>measurement (<span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>)</th>
            <th>posterior (<span class="math inline">$\overline x_{k}
            \times z_k$</span>)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">$\overline{x_{k}} \sim
            \mathcal{N}\left(4,1^{2}\right)$</span></td>
            <td><span
            class="math inline"><em>z</em><sub><em>k</em></sub> ∼ 𝒩(6, 0.5<sup>2</sup>)</span></td>
            <td><span
            class="math inline">$\overline{\mathrm{x}}_{\mathrm{k}}
            \times \mathrm{z}_{\mathrm{k}} \sim
            \mathcal{N}(?,?)$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/ekf/kalman/kalman.update.6.png" width="300"></p>
            <p><br></p>
            <p>After the update, the resultant distribution looks
            like,</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th>prediction (<span class="math inline">$\overline
            x_{k}$</span>)</th>
            <th>measurement (<span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>)</th>
            <th>posterior (<span class="math inline">$\overline x_{k}
            \times z_k$</span>)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline">$\overline{x_{k}} \sim
            \mathcal{N}\left(4,1^{2}\right)$</span></td>
            <td><span
            class="math inline"><em>z</em><sub><em>k</em></sub> ∼ 𝒩(6, 0.5<sup>2</sup>)</span></td>
            <td><span
            class="math inline">$\overline{\mathrm{x}}_{\mathrm{k}}
            \times \mathrm{z}_{\mathrm{k}} \sim
            \mathcal{N}(5.6,0.2)$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/ekf/kalman/kalman.update.7.png" width="300"></p>
            <p><br></p>
            <p>What this tells us is → we give <strong>more weight to
            more ‘certain’ information</strong>, <em>i.e.,</em> the
            measurement in this case that has a higher+narrower
            Gaussian. Which is as it should be.</p>
            <p>Hence, the <strong>posterior</strong> is,</p>
            <ul>
            <li>between prediction (<span class="math inline">$\overline
            x_{k}$</span>) and measurement (<span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>)</li>
            <li>closer to **more certain side* (based on the
            variances)</li>
            <li>so, a ‘<strong>weighted average</strong>’</li>
            </ul>
            <p><img src="img/ekf/kalman/kalman.update.8.png" width="300"></p>
            <p><br></p>
            <p>Let us look at a few more <strong>detailed
            examples</strong>.</p>
            <ol type="1">
            <li>Example I</li>
            </ol>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 57%" />
            </colgroup>
            <thead>
            <tr>
            <th>process model</th>
            <th>measurement noise/sensor error</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline">$\overline{x_{k+1}}=x_{k}+v_{k} \Delta
            t$</span></td>
            <td><span
            class="math inline"><em>σ</em><sup>2</sup> = 0.5<sup>2</sup></span></td>
            </tr>
            <tr>
            <td>$ v_{k} (2 / , 1^{2} ^{2} / ^{2})$</td>
            <td></td>
            </tr>
            <tr>
            <td>$ t=1 $</td>
            <td></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>The initial state looks like,</p>
            <p><img src="img/ekf/kalman/kalman.posterior_example.1.png" width="500"></p>
            <p><br></p>
            <p>If we plot this system, the actual position vs
            measurements (first graph) and afer applying the Kalman
            Filter.</p>
            <p><img src="img/ekf/kalman/kalman.posterior_example.2.png" width="300">
            <img src="img/ekf/kalman/kalman.posterior_example.3.png" width="300"></p>
            <p><br></p>
            <p>We see the following properties:</p>
            <ol type="1">
            <li>posterior → <strong>always between</strong> prior and
            measurement</li>
            <li>posterior is <strong>closer to</strong> measurement →
            sensor noise is small</li>
            </ol>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: center;"><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(2 m/s, 1<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            <br> prior (prediction)</th>
            <th style="text-align: center;"><span
            class="math inline"><em>σ</em><sup>2</sup> = 0.5<sup>2</sup></span>
            <br> measurement</th>
            <th style="text-align: center;">posterior (with
            measurement)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(2, 101)</span></td>
            <td style="text-align: center;">1.555</td>
            <td style="text-align: center;">$(1.556,0.249) $</td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(3.556, 1.249)</span></td>
            <td style="text-align: center;">2.267</td>
            <td style="text-align: center;">$(2.482,0.208) $</td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(4.482, 1.208)</span></td>
            <td style="text-align: center;">1.233</td>
            <td style="text-align: center;">$(1.790,0.207) $</td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(3.790, 1.207)</span></td>
            <td style="text-align: center;">3.534</td>
            <td style="text-align: center;">$(3.578,0.207) $</td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(5.578, 1.207)</span></td>
            <td style="text-align: center;">4.644</td>
            <td style="text-align: center;"><span
            class="math inline">𝒩(4.804, 0.207)</span></td>
            </tr>
            <tr>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            </tr>
            </tbody>
            </table>
            <p>Note that the uncertainty (<span
            class="math inline"><em>σ</em><sup>2</sup></span>) values
            for the prior (<span
            class="math inline"><em>v</em><sub><em>k</em></sub></span>)
            are all <strong>larger</strong> than that for the
            measurement (<code>0.5</code>). Hence the prior is
            <strong>almost useless</strong> for the posterior. Whereas,
            the posterior is <strong>very close</strong> to the
            measurement! This shows us what a difference a good
            measurement makes.</p>
            <ol start="2" type="1">
            <li>Example II: let’s update the model so that the
            measurement error is <span
            class="math inline"><em>σ</em><sup>2</sup> = 1.5<sup>2</sup></span></li>
            </ol>
            <p>How do you think this will change the posterior?</p>
            <p>Updated model:</p>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 57%" />
            </colgroup>
            <thead>
            <tr>
            <th>process model</th>
            <th>measurement noise/sensor error</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline">$\overline{x_{k+1}}=x_{k}+v_{k} \Delta
            t$</span></td>
            <td><strong><span
            class="math inline"><em>σ</em><sup>2</sup> = 1.5<sup>2</sup></span></strong>
            (<strong>more inacccurary</strong>!)</td>
            </tr>
            <tr>
            <td>$ v_{k} (2 / , 1^{2} ^{2} / ^{2})$</td>
            <td></td>
            </tr>
            <tr>
            <td>$ t=1 $</td>
            <td></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>We see <strong>more noise</strong> in the
            measurements:</p>
            <p><img src="img/ekf/kalman/kalman.posterior_example.4.png" width="300"></p>
            <p><br></p>
            <p>Let’s look at the results of applying the Kalman Filter
            (side-by-side with the previous example):</p>
            <p><img src="img/ekf/kalman/kalman.posterior_example.5.png" width="300">
            <img src="img/ekf/kalman/kalman.posterior_example.6.png" width="300"></p>
            <p><br></p>
            <p>Let’s look at the detailed data:</p>
            <table>
            <colgroup>
            <col style="width: 33%" />
            <col style="width: 33%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: center;"><span
            class="math inline"><em>v</em><sub><em>k</em></sub> ∼ 𝒩(2 m/s, 1<sup>2</sup> m<sup>2</sup>/s<sup>2</sup>)</span>
            <br> prior (prediction)</th>
            <th style="text-align: center;"><span
            class="math inline"><em>σ</em><sup>2</sup> = 1.5<sup>2</sup></span>
            <br> measurement</th>
            <th style="text-align: center;">posterior (with
            measurement)</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(2, 101)</span></td>
            <td style="text-align: center;">1.499</td>
            <td style="text-align: center;">$(1.510,2.201) $</td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(3.510, 3.201)</span></td>
            <td style="text-align: center;">3.907</td>
            <td style="text-align: center;">$(3.743,1.321) $</td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(5.743, 2.3218)</span></td>
            <td style="text-align: center;">0.391</td>
            <td style="text-align: center;">$(3.025,1.143) $</td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(5.025, 2.143)</span></td>
            <td style="text-align: center;">2.289</td>
            <td style="text-align: center;">$(3.690,1.097) $</td>
            </tr>
            <tr>
            <td style="text-align: center;"><span
            class="math inline">𝒩(5.690, 2.097)</span></td>
            <td style="text-align: center;">3.735</td>
            <td style="text-align: center;"><span
            class="math inline">𝒩(4.747, 1.086)</span></td>
            </tr>
            <tr>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            <td style="text-align: center;"></td>
            </tr>
            </tbody>
            </table>
            <p>We see that the <span
            class="math inline"><em>σ</em><sup>2</sup></span> for the
            prior is <strong>similar to or better</strong> than the
            measurement uncertainty (<span
            class="math inline"><em>σ</em><sup>2</sup> = 1.5<sup>2</sup></span>)!</p>
            </section>
            <section id="kalman-filter-kalman-gain" class="level3"
            data-number="8.3.5">
            <h3 data-number="8.3.5"><span
            class="header-section-number">8.3.5</span> Kalman Filter |
            <strong>Kalman Gain</strong></h3>
            <p>The <strong>update</strong> step for the Kalman Filter
            now (recall that the posterior is obtained by “mixing”)
            looks like,</p>
            <p><img src="img/ekf/kalman/kalman.gain.1.png" width="300">
            <img src="img/ekf/kalman/kalman.gain.2.png" width="300"></p>
            <p><br></p>
            <p>So, consider these simplifications,</p>
            <p><span class="math display">$$
            \begin{aligned}
            \mu &amp; =\frac{\sigma_{z}^{2} \bar{\mu}+\bar{\sigma}^{2}
            \mu_{z}}{\bar{\sigma}^{2}+\sigma_{z}^{2}} \\
            &amp; =K_{1} \mu_{z}+K_{2} \bar{\mu} \\
            &amp; =K \mu_{z}+(1-K) \bar{\mu}
            \end{aligned}
            $$</span></p>
            <p>where <span
            class="math inline"><em>K</em><sub>2</sub> = 1 − <em>K</em></span>
            is a simplification that works as follows → the final result
            tells us whether the posterior is <strong>closer to the
            prior or the measurement</strong>, <em>i.e.,</em> where does
            it like on this line?</p>
            <p><img src="img/ekf/kalman/kalman.gain.3.png" width="300"></p>
            <p><br></p>
            <p>Hence, the <strong>Kalman Gain</strong>,</p>
            <p><span class="math display">$$
            K = \frac{\bar{\sigma}^{2}}{\bar{\sigma}^{2} +
            \sigma_{z}^{2}}
            $$</span></p>
            <p>The Kalman gains tells us,</p>
            <ul>
            <li><strong>how much</strong> measurement is
            <strong>trusted</strong></li>
            <li><strong>mixing ratio</strong> between measurement and
            prior</li>
            </ul>
            <p><strong>Example</strong>: measurement is <strong>three
            times</strong> more accurate than prior. What is the Kalman
            Gain?</p>
            <p><span class="math display">$$
            \begin{gathered}
            \bar{\sigma}^{2}=3 \sigma_{z}^{2} \\
            K=\frac{3 \sigma_{z}^{2}}{3
            \sigma_{Z}^{2}+\sigma_{Z}^{2}}=\frac{3}{4}
            \end{gathered}
            $$</span></p>
            <p>Hence, the posterior is:</p>
            <p><img src="img/ekf/kalman/kalman.gain.4.png" width="300"></p>
            <p><br></p>
            <p>Now we can write the posterior as:</p>
            <table>
            <tbody>
            <tr>
            <td><strong>residual</strong></td>
            <td><span
            class="math inline"><em>y</em> = <em>μ</em><sub><em>z</em></sub> − <em>μ̄</em></span></td>
            </tr>
            <tr>
            <td>posterior <strong>mean</strong></td>
            <td><span
            class="math inline"><em>μ</em> = <em>μ̄</em> + <em>K</em><em>y</em></span></td>
            </tr>
            <tr>
            <td>posterior <strong>noise</strong></td>
            <td><span class="math inline">$\sigma^2 = (1-K)
            \bar{\sigma^2}$</span></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>(since <span class="math inline">$\sigma^{2}
            =\frac{\sigma_{1}^{2}
            \sigma_{2}^{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}$</span>).</p>
            <p><strong>Note:</strong> <strong>higher K</strong> →
            <strong>more certainty</strong>!</p>
            </section>
            <section id="kalman-filter-summary" class="level3"
            data-number="8.3.6">
            <h3 data-number="8.3.6"><span
            class="header-section-number">8.3.6</span> Kalman Filter |
            <strong>Summary</strong></h3>
            <p><img src="img/ekf/kalman/kalman.summary.png" width="500"></p>
            <p><br></p>
            <ol type="1">
            <li><strong>Step 0</strong>: <strong>initialize</strong>
            <span
            class="math inline"><em>x</em><sub>0</sub> ∼ 𝒩(<em>μ</em><sub>0</sub>, <em>σ</em><sub>0</sub><sup>2</sup>)</span>,
            the initial belief
            <ul>
            <li>to reasonably random values or initial measurement</li>
            </ul></li>
            <li><strong>Step 1</strong>: <strong>predict</strong>
            <ul>
            <li>calculate the prior, <span
            class="math inline">$\overline{x_{k}} \sim
            \mathcal{N}\left(\overline{\mu_{k}},{\overline{\sigma_{k}}}^{2}\right)$</span>,
            from the previous posterior, <span
            class="math inline"><em>x</em><sub><em>k</em> − 1</sub> ∼ 𝒩(<em>μ</em><sub><em>k</em> − 1</sub>, <em>σ</em><sub><em>k</em> − 1</sub><sup>2</sup>)</span></li>
            <li>by incorporating the process model, <span
            class="math inline"><em>f</em><sub><em>x</em></sub> ∼ 𝒩(<em>μ</em><sub><em>f</em></sub>, <em>σ</em><sub><em>f</em></sub><sup>2</sup>)</span>
            <span class="math display">$$
            \begin{gathered}
            \overline{\mu_{k}}=\mu_{k-1}+\mu_{f} \\
            {\overline{\sigma_{k}}}^{2}=\sigma_{k-1}^{2}+\sigma_{f}^{2}
            \end{gathered}
            $$</span></li>
            </ul></li>
            <li><strong>Step 2</strong>: <strong>update</strong>
            <ul>
            <li>given a measurement, <span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>,
            compute the residual and the Kalman gain</li>
            <li>set the posterior, <span
            class="math inline"><em>x</em><sub><em>k</em></sub></span>,
            between the prior, <span
            class="math inline"><em>x̄</em><sub><em>k</em></sub></span>,
            and the measurement, <span
            class="math inline"><em>z</em><sub><em>k</em></sub></span>,
            based on the residual and the Kalman gain</li>
            </ul></li>
            </ol>
            <p><span class="math display">$$
            \begin{aligned}
            y_{k} &amp; =\mu_{z, k}-\overline{\mu_{k}} \\
            K_{k} &amp;
            =\frac{{\overline{\sigma_{k}}}^{2}}{{\overline{\sigma_{k}}}^{2}+\sigma_{z,
            k}^{2}} \\
            \mu_{k} &amp; =\overline{\mu_{k}}+ K_{k}.y_{k} \\
            \sigma_{k}^{2} &amp; =\left(1-K_{k}\right)
            \bar{\sigma}_{k}^{2}
            \end{aligned}
            $$</span></p>
            </section>
            </section>
            <section id="multivariate-kalman-filter" class="level2"
            data-number="8.4">
            <h2 data-number="8.4"><span
            class="header-section-number">8.4</span> Multivariate Kalman
            Filter</h2>
            <p>In reality state is <em>not</em> single dimensional → it
            is <strong>multi-dimensional</strong>. In fact, it is a
            multi-dimensional <strong>vector</strong>, for instance,</p>
            <ul>
            <li>Example: [position, velocity] <span
            class="math inline"><sup>⊤</sup></span></li>
            <li>Represented by a <strong>multivariate
            gaussian</strong>:</li>
            </ul>
            <p><img src="img/ekf/multi_kalman/n_dimensional.png" width="300"></p>
            <p>Let’s look at [position, velocity] <span
            class="math inline"><sup>⊤</sup></span>,</p>
            <p><img src="img/ekf/multi_kalman/variance_velocity_position.png" width="300"></p>
            <p>How do we represent this? They’re two <em>different</em>
            Gaussians, with different properties. We use an
            <strong>ellipse</strong> (from <a
            href="https://engineeringmedia.com/controlblog/the-kalman-filter">Engineering
            Media</a>),</p>
            <p><img src="img/ekf/multi_kalman/variances_ellipse.png" width="300"></p>
            <p>We represent these unequal variances as an ellipse, where
            the major and minor axes are set to the variance for that
            dimension. The above image is graphically showing that we
            have <em>more uncertainty in position than we do in
            velocity</em>.</p>
            <p>This is great for combining the means but what about the
            <strong>variance</strong>? We need to understand how the two
            variables, velocity and position, <em>relate</em> to each
            other, since the state of one variable depends on the other.
            For instance, the faster that an object is traveling, the
            <strong>further the position will be from the actual
            measurements</strong>. Hence, capturing the velocity will
            allow us to better estimate the position. On the other hand,
            a better estimation of the position tells us whether our
            velocity measurements were accurate.</p>
            <p>So what we need to really capture is →
            <strong>covariance</strong> between the two variables.</p>
            <p><img src="img/ekf/multi_kalman/covariance_velocity_position.png" width="300"></p>
            <p>Covariance is captured using the <strong>covariance
            matrix</strong> → a square matrix with the number of rows
            and columns equal to the number of state variables. So, our
            position and velocity system would have a <span
            class="math inline">2 × 2</span> covariance matrix, <span
            class="math inline"><em>P</em></span> where,</p>
            <table>
            <thead>
            <tr>
            <th>terms</th>
            <th>meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>diagonal</td>
            <td>variances with <strong>itself</strong></td>
            </tr>
            <tr>
            <td>off-diagonal</td>
            <td><strong>covariances</strong> of each state w.r.t.
            <strong>other</strong> states</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>The <strong>state vector</strong> for our system (also a
            matrix):</p>
            <p><span class="math display">$$
            \hat{x} =
            \begin{bmatrix}
            position\\
            velocity
            \end{bmatrix}
            $$</span></p>
            <p>So, our position and velocity co-variance can be
            represented as:</p>
            <p><span class="math display">$$
            \Sigma =
            \begin{bmatrix}
            \text{how pos varies with pos} &amp; \text{how vel varies
            with pos}\\
            \text{how pos varies with vel} &amp; \text{how vel varies
            with vel}
            \end{bmatrix}
            $$</span></p>
            <p>Using the actual terms for variance,</p>
            <p><span class="math display">$$
            \Sigma =
            \begin{bmatrix}
            \sigma^2_{11} &amp; \sigma^2_{12} \\
            \sigma^2_{21} &amp; \sigma^2_{22}
            \end{bmatrix}
            $$</span></p>
            <p>So now, we can represent our multivariate Gaussian
            as:</p>
            <p><span class="math display">$$
            \mathcal{N}(\boldsymbol{\mu},
            \boldsymbol{\Sigma})=\frac{1}{(2 \pi)^{n /
            2}|\boldsymbol{\Sigma}|^{1 / 2}}
            e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}
            \boldsymbol{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}
            $$</span></p>
            <p>where,</p>
            <table>
            <colgroup>
            <col style="width: 27%" />
            <col style="width: 31%" />
            <col style="width: 41%" />
            </colgroup>
            <thead>
            <tr>
            <th>variable</th>
            <th>definition</th>
            <th>meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline"><strong>x</strong></span></td>
            <td><span
            class="math inline">(<em>x</em><sub>1</sub>, <em>x</em><sub>2</sub>, …, <em>x</em><sub><em>n</em></sub>)</span></td>
            <td>state variable</td>
            </tr>
            <tr>
            <td><span class="math inline"><strong>μ</strong></span></td>
            <td><span
            class="math inline">(<em>μ</em><sub>1</sub>, <em>μ</em><sub>2</sub>, …, <em>μ</em><sub><em>n</em></sub>)</span></td>
            <td>mean vector</td>
            </tr>
            <tr>
            <td><span class="math inline"><em>Σ</em></span></td>
            <td><span
            class="math inline"><em>Σ</em><sub>i, j</sub> = Cov (x<sub>i</sub>, x<sub>j</sub>)</span></td>
            <td>covariance matrix</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Let’s look at some examples of multivariate Gaussians and
            corresponding values:</p>
            <table>
            <colgroup>
            <col style="width: 46%" />
            <col style="width: 53%" />
            </colgroup>
            <thead>
            <tr>
            <th>example</th>
            <th style="text-align: left;">comments</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><img src="img/ekf/multi_kalman/multi_g.1.png" width="300"></td>
            <td style="text-align: left;"><span
            class="math inline"><em>x</em><sub>1</sub></span> and <span
            class="math inline"><em>x</em><sub>2</sub></span> are
            <strong>not</strong> correlated</td>
            </tr>
            <tr>
            <td><img src="img/ekf/multi_kalman/multi_g.2.png" width="300"></td>
            <td style="text-align: left;"><span
            class="math inline"><em>x</em><sub>1</sub></span> and <span
            class="math inline"><em>x</em><sub>2</sub></span> are
            <strong>not</strong> correlated</td>
            </tr>
            <tr>
            <td><img src="img/ekf/multi_kalman/multi_g.3.png" width="300"></td>
            <td style="text-align: left;"><span
            class="math inline"><em>x</em><sub>1</sub></span> and <span
            class="math inline"><em>x</em><sub>2</sub></span> are
            <strong>not</strong> correlated</td>
            </tr>
            <tr>
            <td><img src="img/ekf/multi_kalman/multi_g.4.png" width="300"></td>
            <td style="text-align: left;"><span
            class="math inline"><em>x</em><sub>1</sub></span> and <span
            class="math inline"><em>x</em><sub>2</sub></span>
            <strong>are</strong> correlated → <span
            class="math inline"><em>x</em><sub>1</sub>(<em>x</em><sub>2</sub>)</span>
            gives us information about what <span
            class="math inline"><em>x</em><sub>2</sub>(<em>x</em><sub>1</sub>)</span>
            could be</td>
            </tr>
            </tbody>
            </table>
            <section id="process-model-and-noise" class="level3"
            data-number="8.4.1">
            <h3 data-number="8.4.1"><span
            class="header-section-number">8.4.1</span> Process Model and
            Noise</h3>
            <p>Now, we need to figure out how to model the system with
            these updates to the system model, covariance, multivariate
            gaussians and what not. Let’s look at the position and
            velocity as examples.</p>
            <p>Recall the state is,</p>
            <p><span class="math display">$$
            \hat{x} =
            \begin{bmatrix}
            position\\
            velocity
            \end{bmatrix}
            =
            \begin{bmatrix}
            x_{k}\\
            \dot{x}_{k}
            \end{bmatrix}
            $$</span></p>
            <blockquote>
            <p>Why is velocity represented as: <span
            class="math inline"><em>ẋ</em><sub><em>k</em></sub></span>?</p>
            </blockquote>
            <p>Now, in the original model, our physical equation
            was:</p>
            <ul>
            <li><span
            class="math inline">$\overline{x_{k+1}}=x_{k}+\dot{x}_{k}
            \Delta t$</span></li>
            <li><span class="math inline"><em>ẋ</em></span> remains
            <strong>constant</strong> (<em>i.e.,</em> <span
            class="math inline">$\overline{\dot{x}_{k+1}}=\dot{x}_{k}$</span>
            )</li>
            </ul>
            <p>But, in this multivariate Gaussian version of things,
            velocity and position are correlated. So,</p>
            <p><span class="math display">$$
            \overline{\mathbf{x}_{k+1}}=\left[\begin{array}{cc}
            \square &amp; \square \\
            \square &amp; \square \\
            \end{array}\right] \mathbf{x}_{k}
            $$</span></p>
            <p>Because the transition from <span
            class="math inline"><strong>x</strong><sub><strong>k</strong></sub></span>
            → <span
            class="math inline">$\overline{\mathbf{x}_{k+1}}$</span>
            requires a <strong>vector</strong>!</p>
            <p><strong>Note:</strong> <strong>bold</strong> variables
            indicate vectors, non bold variables indicate scalars.</p>
            <p>Plugging in the remanining parts from the motion equation
            (<span
            class="math inline">$\overline{x_{k+1}}=x_{k}+\dot{x}_{k}
            \Delta t$</span>) we get,</p>
            <p><span class="math display">$$
            \overline{\mathbf{x}_{k+1}}=\left[\begin{array}{cc}
            1 &amp; \Delta t \\
            0 &amp; 1
            \end{array}\right] \mathbf{x}_{k}
            $$</span></p>
            <p>The matrix,</p>
            <p><span class="math display">$$
            \mathbf{F}=\left[\begin{array}{cc}
            1 &amp; \Delta t \\
            0 &amp; 1
            \end{array}\right]
            $$</span></p>
            <p>is known as the <strong>state transition
            matrix</strong>.</p>
            <p>So we write the equation as:</p>
            <p><span class="math display">$$
            \overline{\mathbf{x}_{k+1}} = \mathbf{F} \mathbf{x}_{k}
            $$</span></p>
            <p>Let’s revisit out multivariate Gaussian definition,</p>
            <p><span class="math display">$$
            \mathcal{N}(\boldsymbol{\mu}, \Sigma)=\frac{1}{(2 \pi)^{n /
            2}|\Sigma|^{1 / 2}}
            e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}
            \Sigma^{-1}(\mathbf{x}-\boldsymbol{\mu})}
            $$</span></p>
            <p>We particularly care about <span
            class="math inline">$\textcolor{orange}{\Sigma}$</span>,</p>
            <p><span class="math display">$$
            \mathcal{N}(\boldsymbol{\mu},
            \textcolor{orange}{\Sigma})=\frac{1}{(2 \pi)^{n /
            2}|\textcolor{orange}{\Sigma}|^{1 / 2}}
            e^{-\frac{1}{2}(\mathbf{x}-\boldsymbol{\mu})^{T}
            \textcolor{orange}{\Sigma}^{-1}(\mathbf{x}-\boldsymbol{\mu})}
            $$</span></p>
            <p><span
            class="math inline">$\textcolor{orange}{\Sigma}$</span> →
            <strong>error covariance</strong> (or state covariance):</p>
            <ul>
            <li>represents the state uncertainty</li>
            <li>typically denoted by → <span
            class="math inline"><strong>P</strong></span></li>
            </ul>
            <p>So,</p>
            <p><span class="math display">$$
            \mathbf{P} =
            \begin{bmatrix}
            \sigma^2_{11} &amp; \sigma^2_{12} \\
            \sigma^2_{21} &amp; \sigma^2_{22}
            \end{bmatrix}
            $$</span></p>
            <p>We also need to worry about →
            <strong>noise/uncertainty</strong> during state transitions
            because, well, we live in the real world. Hence, there is a
            matrix <span class="math inline"><strong>Q</strong></span>
            used to represent this,</p>
            <ul>
            <li>uncertainty in state transition (<em>e.g.,</em>
            friction, winds, etc.)</li>
            <li>white noise (<em>i.e.,</em> zero mean)</li>
            </ul>
            <p>So, looking at the state transition, <span
            class="math inline"><em>k</em> → <em>k</em> + 1</span>, can
            we write?</p>
            <p><span class="math display">$$
            \overline{\mathbf{P}_{k+1}}=\mathbf{P}_{k}+\mathbf{Q}
            $$</span></p>
            <p>This is incorrect! The right way to do this is as
            follows:</p>
            <p><span class="math display">$$
            \overline{\mathbf{P}_{k+1}}=\mathbf{F P}_{k}
            \mathbf{F}^{\mathrm{T}}+\mathbf{Q}
            $$</span></p>
            <p>Where, <span
            class="math inline"><strong>F</strong><sup>T</sup></span> is
            the <strong><a
            href="https://en.wikipedia.org/wiki/Transpose">transpose</a></strong>
            of matrix <span
            class="math inline"><strong>F</strong></span>.</p>
            <details>
            <summary>
            Proof and Further Details
            </summary>
            <p>In the given equations, the symbol <span
            class="math inline"><em>E</em>[.]</span> represents the
            expectation operator, also known as the expected value.</p>
            <ul>
            <li>for <em>scalar</em> random variables, the expectation is
            the <strong>arithmetic average value</strong> you’d expect
            to observe if you repeated the experiment infinitely many
            times</li>
            <li>for random <em>vectors</em>, the expectation denotes the
            <strong>vector of expectations of each
            component</strong>:</li>
            </ul>
            <p>Given a random vector:</p>
            <p><span class="math display">$$
            \mathbf{x} = \begin{bmatrix}
            x_1 \\
            x_2 \\
            \vdots \\
            x_n
            \end{bmatrix},
            $$</span></p>
            <p>the expectation is defined component-wise as:</p>
            <p><span class="math display">$$
            \mathrm{E}[\mathbf{x}] = \begin{bmatrix}
            \mathrm{E}[x_1] \\
            \mathrm{E}[x_2] \\
            \vdots \\
            \mathrm{E}[x_n]
            \end{bmatrix}.
            $$</span></p>
            <p>Additionally, the expression,</p>
            <p><span
            class="math display">E[(<strong>x</strong> − <strong>μ</strong>)(<strong>x</strong> − <strong>μ</strong>)<sup><em>T</em></sup>]</span></p>
            <p>represents the covariance matrix of the random vector
            <span class="math inline"><em>x</em></span>, typically
            denoted by: <span
            class="math display">Var (<strong>x</strong>)  or  <strong>Σ</strong></span></p>
            <p>Now, we have:</p>
            <p><span class="math display">$$
            \begin{aligned}
            \operatorname{Var}(\mathbf{A x}) &amp;
            =\mathrm{E}\left[(\mathbf{A}(\mathbf{x}-\boldsymbol{\mu}))(\mathrm{A}(\mathbf{x}-\boldsymbol{\mu}))^{\mathrm{T}}\right]
            \\
            &amp;
            =\mathrm{E}\left[\left(\mathbf{A}(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{T}\right)
            \mathbf{A}^{\mathrm{T}}\right] \\
            &amp;
            \left.=\operatorname{AE}\left[(\mathbf{x}-\boldsymbol{\mu})(\mathbf{x}-\boldsymbol{\mu})^{T}\right)\right]
            \mathbf{A}^{\mathrm{T}} \\
            &amp; =\operatorname{AVar}(\mathbf{x})
            \mathbf{A}^{\mathrm{T}}
            \end{aligned}
            $$</span></p>
            <p>Going back to our state transition model, recall,</p>
            <p><span class="math display">$$
            \overline{\mathbf{x}_{k+1}}=\mathbf{F} \mathbf{x}_{k}
            $$</span></p>
            <p>We have seen, from above,</p>
            <p><span
            class="math display">Var (<strong>A</strong><strong>x</strong>) = <strong>A</strong><strong>V</strong><strong>a</strong><strong>r</strong>(<strong>x</strong>)<strong>A</strong><sup>T</sup></span></p>
            <p>Replacing with the right vectors/paramters,</p>
            <p><span
            class="math display">Var (<strong>F</strong><strong>x</strong><sub><em>k</em></sub>) = <strong>F</strong>Var (<strong>x</strong><sub><em>k</em></sub>)<strong>F</strong><sup>T</sup> = <strong>F</strong><strong>P</strong><sub><em>k</em></sub><strong>F</strong><sup><strong>T</strong></sup></span></p>
            <p>since, <span
            class="math inline"><strong>P</strong><sub><em>k</em></sub></span>
            is the (state/error) <strong>covariance</strong> matrix.</p>
            </details>
            <p>Now, at the system start, we have:</p>
            <p><span class="math display">$$
            \begin{aligned}
            \mathbf{F} &amp; =\left[\begin{array}{cc}
            1 &amp; \Delta t \\
            0 &amp; 1
            \end{array}\right] \quad
            \text{and} \quad
            \mathbf{P}_{\mathbf{k}} &amp; =\left[\begin{array}{cc}
            \sigma_{x_{k}}^{2} &amp; 0 \\
            0 &amp; \sigma_{x_{k}}^{2}
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            <p><em>i.e.,</em> <strong>no correlation</strong> between
            position and velocity.</p>
            <p>But, after <em>one</em> step (<em>i.e.,</em>
            <strong>prediction</strong>),</p>
            <p><span class="math display">$$
            \begin{aligned}
            \overline{\mathbf{P}_{k+1}} &amp; =\mathbf{F P}_{k}
            \mathbf{F}^{\mathrm{T}} \\
            &amp; =\left[\begin{array}{cc}
            1 &amp; \Delta t \\
            0 &amp; 1
            \end{array}\right]\left[\begin{array}{cc}
            \sigma_{x_{k}}^{2} &amp; 0 \\
            0 &amp; \sigma_{x_{k}}^{2}
            \end{array}\right]\left[\begin{array}{cc}
            1 &amp; 0 \\
            \Delta t &amp; 1
            \end{array}\right] \\
            &amp; =\left[\begin{array}{cc}
            \sigma_{x_{k}}^{2}+\sigma_{x_{k}}^{2} \Delta t^{2} &amp;
            \textcolor{orange}{\sigma_{x_{k}}^{2} \Delta t} \\
            \textcolor{orange}{\sigma_{x_{k}}^{2} \Delta t} &amp;
            \sigma_{x_{k}}^{2}
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            <p>Position and velocity → <strong>correlated</strong>!</p>
            <p>Let’s look at an example:</p>
            <p><span class="math display">$$
            \begin{array}{llll}
            \mathbf{x}_{0}=\left[\begin{array}{l}
            x_{0} \\
            \dot{x}_{0}
            \end{array}\right]=\left[\begin{array}{c}
            0 \\
            10
            \end{array}\right] &amp; \mathbf{F}=\left[\begin{array}{cc}
            1 &amp; \Delta t \\
            0 &amp; 1
            \end{array}\right] &amp; \Delta t=1 \mathrm{~s} &amp;
            \mathbf{P}_{\mathbf{0}}=\left[\begin{array}{ll}
            1 &amp; 0 \\
            0 &amp; 3
            \end{array}\right] \\
            \begin{array}{l}
            \text { Initial position }=0 \mathrm{~m}
            \end{array} &amp; \overline{x_{k+1}}=x_{k}+\dot{x}_{k}
            \Delta t &amp; &amp; \text { Initial state covariance } \\
            \text { Initial velocity }=10 \mathrm{~m} / \mathrm{s} &amp;
            \overline{\dot{x}_{k+1}}=\dot{x}_{k} &amp;
            \end{array}
            $$</span></p>
            <details>
            <summary>
            Example Details
            </summary>
            <p>Recall that we write our model as,</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \overline{\mathbf{x}_{k+1}}=\mathbf{F} \mathbf{x}_{k}
            \\
            &amp; \overline{\mathbf{P}_{k+1}}=\mathbf{F} \mathbf{P}_{k}
            \mathbf{F}^{\mathrm{T}}+\boldsymbol{Q}
            \end{aligned}
            $$</span></p>
            <p>Let’s forget about <span
            class="math inline"><strong>Q</strong></span> for now,</p>
            <p>So, our starting state can be represented as (we see that
            velocty and position are <strong>not
            correlated</strong>):</p>
            <p><img src="img/ekf/multi_kalman/process_model.1.png" width ="300"></p>
            <p>Now, after one iteration/prediction,</p>
            <p><span class="math display">$$
            \begin{aligned}
            \overline{\mathbf{x}_{1}}=\mathbf{F} \mathbf{x}_{0} &amp;
            =\left[\begin{array}{cc}
            1 &amp; \Delta t \\
            0 &amp; 1
            \end{array}\right]\left[\begin{array}{c}
            0 \\
            10
            \end{array}\right]=\left[\begin{array}{l}
            10 \\
            10
            \end{array}\right] \\
            \overline{\mathbf{P}_{1}}=\mathbf{F P}_{0}
            \mathbf{F}^{\mathrm{T}} &amp; =\left[\begin{array}{cc}
            1 &amp; \Delta t \\
            0 &amp; 1
            \end{array}\right]\left[\begin{array}{cc}
            1 &amp; 0 \\
            0 &amp; 3
            \end{array}\right]\left[\begin{array}{cc}
            1 &amp; 0 \\
            \Delta t &amp; 1
            \end{array}\right] \\
            &amp; =\left[\begin{array}{ll}
            1 &amp; 3 \\
            0 &amp; 3
            \end{array}\right]\left[\begin{array}{cc}
            1 &amp; 0 \\
            \Delta t &amp; 1
            \end{array}\right]=\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            <p>If we plot this, we see that <strong>velocity and
            position are now correlated</strong>:</p>
            <p><img src="img/ekf/multi_kalman/process_model.2.png" width ="300"></p>
            <p>A couple more iterations,</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \overline{\mathbf{x}_{2}}=\left[\begin{array}{l}
            20 \\
            10
            \end{array}\right] \\
            &amp; \overline{\mathbf{P}_{2}}=\left[\begin{array}{cc}
            13 &amp; 6 \\
            6 &amp; 3
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            <p><img src="img/ekf/multi_kalman/process_model.3.png" width ="300"></p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \overline{\mathbf{x}_{3}}=\left[\begin{array}{l}
            30 \\
            10
            \end{array}\right] \\
            &amp; \overline{\mathbf{P}_{3}}=\left[\begin{array}{cc}
            28 &amp; 9 \\
            9 &amp; 3
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            </details>
            <p><br></p>
            <p><img src="img/ekf/multi_kalman/process_model.4.png" width ="300"></p>
            <p><br></p>
            <p>Hence, after multiple iterations,</p>
            <ul>
            <li>we started with <span
            class="math inline">$\mathbf{x}_{0}=\left[\begin{array}{c}0
            \mathrm{~m} \\ 10 \mathrm{~m} /
            \mathrm{s}\end{array}\right]$</span></li>
            <li>position moves by <span
            class="math inline">10<em>m</em></span> in each step due to
            the velocity estimate <span
            class="math inline"> = 10 m/s</span></li>
            <li>velocity estimate <strong>does not change</strong>
            because we <strong>predict</strong> it based on <span
            class="math inline">${\overline{\dot{x}_{k+1}}}=\dot{x}_{k}$</span></li>
            <li>position <strong>uncertainty increases</strong></li>
            <li>position and velocity become more
            <strong>correlated</strong> because <span
            class="math inline">$\overline{x_{k+1}}=x_{k}+\dot{x}_{k}
            \Delta t$</span></li>
            </ul>
            </section>
            <section id="control-input" class="level3"
            data-number="8.4.2">
            <h3 data-number="8.4.2"><span
            class="header-section-number">8.4.2</span> Control
            Input</h3>
            <p>The state of the system is not only dependant on the
            sensor and model. Recall that <strong>control input</strong>
            is meant to drive the system to a particular state. So, we
            must include it (if it is present) as,</p>
            <p>&lt;!–&gt; <span class="math display">$$
            \overline{\mathbf{x}_{k+1}}=\mathbf{F}
            \mathbf{x}_{k}+\mathbf{\textcolor{orange}{B u}}
            $$</span> &lt;–&gt;</p>
            <p><img src="img/ekf/equations/pngs/equations-3.png" width="500"></p>
            <p>where,</p>
            <table>
            <colgroup>
            <col style="width: 16%" />
            <col style="width: 50%" />
            <col style="width: 33%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">term</th>
            <th style="text-align: left;">definition</th>
            <th style="text-align: left;">details</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><span
            class="math inline"><strong>u</strong></span></td>
            <td style="text-align: left;">control input</td>
            <td style="text-align: left;">example: input to motor
            <code>ESC</code></td>
            </tr>
            <tr>
            <td style="text-align: left;"><span
            class="math inline"><strong>B</strong></span></td>
            <td style="text-align: left;">control input model</td>
            <td style="text-align: left;">models the contribution of
            control input to the state transition</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>Note: <span
            class="math inline"><strong>B</strong> = <strong>0</strong></span>
            if there is no control input.</p>
            </section>
            <section id="summary-of-prediction-model" class="level3"
            data-number="8.4.3">
            <h3 data-number="8.4.3"><span
            class="header-section-number">8.4.3</span> Summary of
            Prediction Model</h3>
            <p>To summarize,</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \overline{\mathbf{x}_{k+1}}=\mathbf{F}
            \mathbf{x}_{k}+\mathbf{B} \mathbf{u} \\
            &amp; \overline{\mathbf{P}_{k+1}}=\mathbf{F P}_{k}
            \mathbf{F}^{\mathrm{T}}+\mathbf{Q}
            \end{aligned}
            $$</span></p>
            <ul>
            <li>from <span
            class="math inline"><strong>x</strong><sub><em>k</em></sub></span>,
            we predict the next state <span
            class="math inline">$\overline{\mathbf{x}_{k+1}}$</span>
            (i.e., prior)</li>
            <li><span class="math inline"><strong>F</strong></span> and
            <span class="math inline"><strong>Q</strong></span> are
            typically <strong>constant</strong></li>
            <li><span
            class="math inline"><strong>B</strong><strong>u</strong></span>
            can be set to <code>0</code> → if there is no explicit
            control input to the system</li>
            <li><span
            class="math inline"><strong>B</strong><strong>u</strong></span>
            can be used to provide additional information → better
            prediction</li>
            </ul>
            </section>
            <section id="measurement-update" class="level3"
            data-number="8.4.4">
            <h3 data-number="8.4.4"><span
            class="header-section-number">8.4.4</span> Measurement
            Update</h3>
            <p>As with the simpler Kalman filter, we need to “fix” our
            prediction using a measurement from the sensor(s) which is
            essentially a <strong>Gaussian multiplication</strong> step
            between → the prior and the measurement,</p>
            <p><span class="math display">$$
            \text{prior:} \quad
            \mathbf{\mu_1} = \begin{bmatrix}
            3.0\\
            5.0
            \end{bmatrix}
            \quad
            \mathbf{\Sigma_1} = \begin{bmatrix}
            4 &amp; 3\\
            3 &amp; 3
            \end{bmatrix}
            $$</span></p>
            <p><span class="math display">$$
            \text{measurement:} \quad
            \mathbf{\mu_2} = \begin{bmatrix}
            3.3\\
            4.8
            \end{bmatrix}
            \quad
            \mathbf{\Sigma_2} = \begin{bmatrix}
            3 &amp; -1\\
            -1 &amp; 1
            \end{bmatrix}
            $$</span></p>
            <p>Visually, we can represent these as:</p>
            <p><img src="img/ekf/multi_kalman/measure.1.png" width="300"></p>
            <p><br></p>
            <p>The multivariate Gaussian multiplication equations
            are:</p>
            <p><span
            class="math display"><strong>μ</strong> = <strong>Σ</strong><sub><strong>2</strong></sub>(<strong>Σ</strong><sub><strong>1</strong></sub> + <strong>Σ</strong><sub>2</sub>)<sup>−<strong>1</strong></sup><strong>μ</strong><sub><strong>1</strong></sub> + <strong>Σ</strong><sub><strong>1</strong></sub>(<strong>Σ</strong><sub><strong>1</strong></sub> + <strong>Σ</strong><sub><strong>2</strong></sub>)<sup><strong>−</strong><strong>1</strong></sup><strong>μ</strong><sub><strong>2</strong></sub></span></p>
            <p><span
            class="math display"><strong>Σ</strong> = <strong>Σ</strong><sub><strong>1</strong></sub>(<strong>Σ</strong><sub><strong>1</strong></sub> + <strong>Σ</strong><sub><strong>2</strong></sub>)<sup>−<strong>1</strong></sup><strong>Σ</strong><sub><strong>2</strong></sub></span></p>
            <p>Recall the univariate Gaussian equivalents:</p>
            <p><span class="math display">$$
            \mu=\frac{\sigma_{2}^{2} \mu_{1}+\sigma_{1}^{2}
            \mu_{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}} \quad
            \sigma^{2}=\frac{\sigma_{1}^{2}
            \sigma_{2}^{2}}{\sigma_{1}^{2}+\sigma_{2}^{2}}
            $$</span></p>
            <p>The net result is:</p>
            <p><img src="img/ekf/multi_kalman/measure.2.png" width="300"></p>
            <p><br></p>
            <p>A problem with Gaussian multiplication is that the
            dimensionalities of the state and the measurement
            <strong>need not match</strong>! For instance,</p>
            <ul>
            <li>state → <span
            class="math inline">[<em>p</em><em>o</em><em>s</em><em>i</em><em>t</em><em>i</em><em>o</em><em>n</em>, <em>v</em><em>e</em><em>l</em><em>o</em><em>c</em><em>i</em><em>t</em><em>y</em>]</span></li>
            <li>measurement → <span
            class="math inline">[<em>p</em><em>o</em><em>s</em><em>i</em><em>t</em><em>i</em><em>o</em><em>n</em>]</span></li>
            </ul>
            <p>Hence, we need a <strong>measurement function</strong> →
            converts a state into a measurement space,
            <em>e.g.,</em></p>
            <ul>
            <li>different dimensions and/or</li>
            <li>different units (voltage → distance, celsius →
            fahrenheit)</li>
            </ul>
            <p>So, incorporating the measurement function, <span
            class="math inline"><strong>H</strong></span>, we get</p>
            <p><img src="img/ekf/equations/pngs/equations-4.png" width="500"></p>
            <p><br></p>
            <p><strong>Note:</strong> the “<strong>residual</strong>” is
            the difference between measurement and prediction.</p>
            <p>If we have,</p>
            <p><span class="math display">$$
            \text{State:} \quad \mathbf{x}=\left[\begin{array}{l}x \\
            \dot{x} \\ y \\ \dot{y}\end{array}\right]
            \quad
            \text{and \quad Measurement:} \quad
            \mathbf{z}=\left[\begin{array}{l}x \\ y\end{array}\right]
            $$</span></p>
            <p>Then the <strong>measurement function</strong>, is</p>
            <p><span class="math display">$$
            \mathbf{H}=\left[\begin{array}{llll}1 &amp; 0 &amp; 0 &amp;
            0 \\ 0 &amp; 0 &amp; 1 &amp; 0\end{array}\right]
            $$</span></p>
            <p>Recall that measurement → also follows a Gaussian and has
            some noise. Hence it is represented as a
            <strong>multivariate</strong> Gaussian,</p>
            <p><span class="math display">$$
            \mathcal{N}\left(\mathbf{z}_{\mathbf{k}},
            \textcolor{orange}{\mathbf{R}}\right)
            $$</span></p>
            <p>where, the <strong>measurement noise</strong> is given
            by,</p>
            <p><span class="math display">$$
            \mathbf{R}=\left[\begin{array}{ccc}
            \square &amp; \ldots &amp; \square \\
            \vdots &amp; \ddots &amp; \vdots \\
            \square &amp; \cdots &amp; \square
            \end{array}\right]
            $$</span></p>
            <p><span class="math inline"><strong>R</strong></span> →
            models the <strong>accuracy</strong> of the sensor,</p>
            <ul>
            <li><span class="math inline"><em>m</em> × <em>m</em></span>
            covariance matrix (<span
            class="math inline"><em>m</em></span> → # of sensors)</li>
            <li>also models correlations between sensors</li>
            <li>typically <strong>constant</strong></li>
            </ul>
            </section>
            <section id="state-and-uncertainty-update" class="level3"
            data-number="8.4.5">
            <h3 data-number="8.4.5"><span
            class="header-section-number">8.4.5</span> State and
            Uncertainty Update</h3>
            <p>If this is the state of our system,</p>
            <p><img src="img/ekf/multi_kalman/k_gain.1.png" width="300"></p>
            <p><br></p>
            <p>then let’s define <strong>system uncertainty</strong>
            (<span
            class="math inline"><strong>S</strong><sub><strong>k</strong></sub></span>)
            as,</p>
            <ul>
            <li>the sum of (predicted) state noise and sensor noise</li>
            <li>equivalent to <span
            class="math inline"><em>σ̄</em><sup>2</sup> + <em>σ</em><sub><em>Z</em></sub><sup>2</sup></span>
            in the univariate case</li>
            </ul>
            <p><span class="math display">$$
            \mathbf{S}_{\mathbf{k}}=\mathbf{H}
            \overline{\mathbf{P}_{\mathbf{k}}}
            \mathbf{H}^{\mathrm{T}}+\mathbf{R}
            $$</span></p>
            <p>Bringing all of the prior terms together, we can now
            define → the <strong>Kalman Gain</strong> (<span
            class="math inline"><strong>K</strong><sub><strong>k</strong></sub></span>)
            which is answering the question:</p>
            <blockquote>
            <p>“how much we can trust the predicted state vs the
            measurement?”</p>
            </blockquote>
            <p><span class="math display">$$
            \mathbf{K}_{\mathbf{k}}=\overline{\mathbf{P}_{\mathbf{k}}}
            \mathbf{H}^{\mathrm{T}} \mathbf{S}_{\mathbf{k}}^{-1}
            $$</span></p>
            <p>So, to update the state and uncertainty,</p>
            <p><img src="img/ekf/multi_kalman/update.png" width="300"></p>
            <p><br></p>
            <ol type="1">
            <li><strong>state update</strong>:</li>
            </ol>
            <p><span class="math display">$$
            \mathbf{x}_{\mathbf{k}}=\overline{\mathbf{x}}_{k}+\mathbf{K}_{\mathbf{k}}
            \mathbf{y}_{\mathbf{k}}
            $$</span></p>
            <ol start="2" type="1">
            <li><strong>state uncertainty update</strong>:</li>
            </ol>
            <p><span class="math display">$$
            P_{k}=\left(I-K_{k} H\right) \overline{P_{k}}
            $$</span></p>
            <p><strong>Note</strong>: if <span
            class="math inline"><strong>K</strong><sub><strong>k</strong></sub></span>
            is large → then we’re <strong>closer to the
            measurement</strong>.</p>
            <p>Consider the following example:</p>
            <p><span class="math display">$$
            \overline{\mathbf{x}_{1}}=\left[\begin{array}{l}
            10 \\
            10
            \end{array}\right] \quad
            \overline{\mathbf{P}_{1}}=\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right]
            $$</span></p>
            <p>represented as:</p>
            <p><img src="img/ekf/multi_kalman/update_example.1.png" width="300"></p>
            <p>Now consider the following sensor measurement and noise
            model:</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \mathbf{z}_{\mathbf{1}}=[12], \quad
            \textcolor{orange}{\mathbf{R}=[1]} \quad \mathbf{H}=[1 0]\\
            &amp;
            \mathbf{y}_{\mathbf{1}}=\mathbf{z}_{\mathbf{1}}-\mathbf{H}
            \overline{\mathbf{x}_{\mathbf{1}}}=\left[\begin{array}{ll}
            12
            \end{array}\right]-\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right]\left[\begin{array}{l}
            10 \\
            10
            \end{array}\right]=[2] \\
            &amp; \mathbf{S}_{\mathbf{1}}=\mathbf{H}
            \overline{\mathbf{P}_{\mathbf{1}}}
            \mathbf{H}^{\mathrm{T}}+\mathbf{R}=\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right]\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right]\left[\begin{array}{l}
            1 \\
            0
            \end{array}\right]+[1]=[5] \\
            &amp;
            \mathbf{K}_{\mathbf{1}}=\overline{\mathbf{P}_{\mathbf{1}}}
            \mathbf{H}^{\mathrm{T}}
            \mathbf{S}_{\mathbf{1}}^{-1}=\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right]\left[\begin{array}{l}
            1 \\
            0
            \end{array}\right][1 / 5]=\left[\begin{array}{l}
            4 / 5 \\
            3 / 5
            \end{array}\right] \\
            &amp;
            \mathbf{x}_{\mathbf{1}}=\overline{\mathbf{x}_{1}}+\mathbf{K}_{\mathbf{1}}
            \mathbf{y}_{\mathbf{1}}=\left[\begin{array}{l}
            10 \\
            10
            \end{array}\right]+\left[\begin{array}{l}
            4 / 5 \\
            3 / 5
            \end{array}\right]
            2=\textcolor{orange}{\left[\begin{array}{l}
            11.6 \\
            11.2
            \end{array}\right]} \\
            &amp;
            \mathbf{P}_{\mathbf{1}}=\left(\mathbf{I}-\mathbf{K}_{\mathbf{1}}
            \mathbf{H}\right)
            \overline{\mathbf{P}_{1}}=\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right]-\left[\begin{array}{l}
            4 / 5 \\
            3 / 5
            \end{array}\right]\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right]\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right] \\
            &amp; =\left[\begin{array}{ll}
            0.8 &amp; 0.6 \\
            0.6 &amp; 1.2
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            <p>The result looks like,</p>
            <p><img src="img/ekf/multi_kalman/update_example.2.png" width="300"></p>
            <p><br></p>
            <p>Let’s see what happens if we change <span
            class="math inline"><strong>R</strong></span>.</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \mathbf{z}_{\mathbf{1}}=[12], \quad
            \textcolor{orange}{\mathbf{R}=[4]} \begin{array}{c}
            \text {\textcolor{orange}{Degraded sensor accuracy}}
            \end{array} \quad \mathbf{H}=\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right] \\
            &amp;
            \mathbf{y}_{\mathbf{1}}=\mathbf{z}_{\mathbf{1}}-\mathbf{H}
            \overline{\mathbf{x}_{\mathbf{1}}}=[12]-\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right]\left[\begin{array}{l}
            10 \\
            10
            \end{array}\right]=[2] \\
            &amp; \mathbf{S}_{\mathbf{1}}=\mathbf{H}
            \overline{\mathbf{P}_{\mathbf{1}}}
            \mathbf{H}^{\mathrm{T}}+\mathbf{R}=\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right]\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right]\left[\begin{array}{l}
            1 \\
            0
            \end{array}\right]+[4]=[8] \\
            &amp;
            \mathbf{K}_{\mathbf{1}}=\overline{\mathbf{P}_{\mathbf{1}}}
            \mathbf{H}^{\mathrm{T}}
            \mathbf{S}_{\mathbf{1}}^{-1}=\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right]\left[\begin{array}{l}
            1 \\
            0
            \end{array}\right][1 / 8]=\left[\begin{array}{l}
            4 / 8 \\
            3 / 8
            \end{array}\right] \\
            &amp;
            \mathbf{x}_{1}=\overline{\mathbf{x}_{1}}+\mathbf{K}_{1}
            \mathbf{y}_{1}=\left[\begin{array}{l}
            10 \\
            10
            \end{array}\right]+\left[\begin{array}{l}
            4 / 8 \\
            3 / 8
            \end{array}\right]
            2=\textcolor{orange}{\left[\begin{array}{c}
            11 \\
            10.75
            \end{array}\right]} \\
            &amp;
            \mathbf{P}_{\mathbf{1}}=\left(\mathbf{I}-\mathbf{K}_{\mathbf{1}}
            \mathbf{H}\right)
            \overline{\mathbf{P}_{1}}=\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right]-\left[\begin{array}{l}
            4 / 8 \\
            3 / 8
            \end{array}\right]\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right]\left[\begin{array}{ll}
            4 &amp; 3 \\
            3 &amp; 3
            \end{array}\right] \\
            &amp; =\textcolor{orange}{\left[\begin{array}{cc}
            2 &amp; 1.5 \\
            1.5 &amp; 1.5
            \end{array}\right]}
            \end{aligned}
            $$</span></p>
            <p>And the result looks like,</p>
            <p><img src="img/ekf/multi_kalman/update_example.3.png" width="300"></p>
            <p><br></p>
            <p>When compared with the previous result, we see that the
            sensor’s accuracy isn’t very good so the update, while
            narrowing things down, is <strong>still
            imprecise</strong>.</p>
            <p>As we increase the values of <span
            class="math inline"><strong>R</strong></span>, we see that
            the sensor measurements matter less → at <span
            class="math inline"><strong>R</strong> = [200]</span>, the
            sensor values don’t matter at all!</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \mathbf{z}_{\mathbf{1}}=[12], \quad
            \textcolor{orange}{\mathbf{R}=[16]} \quad
            \mathbf{H}=\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right] \\
            &amp; \mathbf{x}_{\mathbf{1}}=\left[\begin{array}{l}
            10.4 \\
            10.3
            \end{array}\right] \\
            &amp; \mathbf{P}_{\mathbf{1}}=\left[\begin{array}{ll}
            3.2 &amp; 2.4 \\
            2.4 &amp; 2.4
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            <p><img src="img/ekf/multi_kalman/update_example.4.png" width="300"></p>
            <p><br></p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \mathbf{z}_{\mathbf{1}}=[12], \quad
            \textcolor{orange}{\mathbf{R}=[200]} \quad
            \mathbf{H}=\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right] \\
            &amp; \mathbf{x}_{\mathbf{1}}=\left[\begin{array}{l}
            10.039 \\
            10.029
            \end{array}\right] \\
            &amp; \mathbf{P}_{\mathbf{1}}=\left[\begin{array}{ll}
            3.922 &amp; 2.941 \\
            2.941 &amp; 2.941
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            <p><img src="img/ekf/multi_kalman/update_example.5.png" width="300"></p>
            <p><br></p>
            </section>
            <section id="estimating-hidden-variables" class="level3"
            data-number="8.4.6">
            <h3 data-number="8.4.6"><span
            class="header-section-number">8.4.6</span> Estimating Hidden
            Variables</h3>
            <p>In Kalman filter-based state estimations, <strong>hidden
            variables</strong> (also called <strong>latent
            variables</strong> or states) refer to those variables or
            quantities that cannot be measured directly but influence
            observable quantities. The Kalman filter estimates these
            hidden variables by processing available measurements
            through a mathematical model of system dynamics and
            measurement processes.</p>
            <p>Examples: velocity, attitude, orientation, angular
            velocity, bias of sensors or internal temperature of a
            component.</p>
            <p>Let’s see how <strong>velocity</strong> can be
            estimated.</p>
            <p>Initially, the velocity <em>estimate</em> is, say, <span
            class="math inline">10 m/s</span>, <em>i.e.,</em></p>
            <p><span class="math display">$$
            \overline{\mathbf{x}_{1}}=\left[\begin{array}{c}
            10 \mathrm{~m} \\
            10 \mathrm{~m} / \mathrm{s}
            \end{array}\right]
            $$</span></p>
            <p><img src="img/ekf/multi_kalman/hidden.1.png" width="300"></p>
            <p><br></p>
            <p>As we get more measurements (of position) and we work
            through the Kalman Filter predictions and state updates, we
            can get more precise estimates for → velocity!</p>
            <p><img src="img/ekf/multi_kalman/hidden.2.png" width="500"></p>
            <p><br></p>
            <p>The various values are:</p>
            <table>
            <colgroup>
            <col style="width: 7%" />
            <col style="width: 29%" />
            <col style="width: 27%" />
            <col style="width: 35%" />
            </colgroup>
            <thead>
            <tr>
            <th>Time</th>
            <th>Measurement ( z_k )</th>
            <th>Posterior ( x_k )</th>
            <th>Posterior ( _k )</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>1</td>
            <td>12</td>
            <td>11.60</td>
            <td>11.20</td>
            </tr>
            <tr>
            <td>2</td>
            <td>17</td>
            <td>18.38</td>
            <td>8.71</td>
            </tr>
            <tr>
            <td>3</td>
            <td>22</td>
            <td>23.67</td>
            <td>7.28</td>
            </tr>
            <tr>
            <td>4</td>
            <td>27</td>
            <td>28.63</td>
            <td>6.52</td>
            </tr>
            <tr>
            <td>5</td>
            <td>32</td>
            <td>33.52</td>
            <td>6.07</td>
            </tr>
            <tr>
            <td>6</td>
            <td>37</td>
            <td>38.40</td>
            <td>5.80</td>
            </tr>
            <tr>
            <td>7</td>
            <td>42</td>
            <td>43.29</td>
            <td>5.62</td>
            </tr>
            <tr>
            <td>8</td>
            <td>47</td>
            <td>48.19</td>
            <td>5.49</td>
            </tr>
            <tr>
            <td>9</td>
            <td>52</td>
            <td>53.10</td>
            <td>5.40</td>
            </tr>
            <tr>
            <td>10</td>
            <td>57</td>
            <td>58.03</td>
            <td>5.33</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/ekf/multi_kalman/hidden.3.png" width="500"></p>
            <p><br></p>
            <p>so then, how is this velocity “inferred”? Let’s look at
            the state equations:</p>
            <p><span class="math display">$$
            \begin{array}{ll}
            \mathbf{S}=\mathbf{H} \overline{\mathbf{P}}
            \mathbf{H}^{\mathrm{T}}+\mathbf{R} &amp;
            \mathbf{S}=\mathbf{H} \overline{\mathbf{P}}
            \mathbf{H}^{\mathrm{T}}+\mathbf{R}=\left[\begin{array}{ll}
            1 &amp; 0
            \end{array}\right]\left[\begin{array}{cc}
            \sigma_{\bar{x}}^{2} &amp; \sigma_{\bar{x} \bar{x}} \\
            \sigma_{\bar{x} \bar{x}} &amp; \sigma_{\bar{x}}^{2}
            \end{array}\right]\left[\begin{array}{l}
            1 \\
            0
            \end{array}\right]+\left[\sigma_{z}^{2}\right]=\left[\sigma_{\bar{x}}^{2}+\sigma_{z}^{2}\right]
            \\
            \mathbf{K}=\overline{\mathbf{P}} \mathbf{H}^{\mathrm{T}}
            \mathbf{S}^{-\mathbf{1}} &amp;
            \mathbf{K}=\overline{\mathbf{P}} \mathbf{H}^{\mathrm{T}}
            \mathbf{S}^{-\mathbf{1}}=\left[\begin{array}{cc}
            \sigma_{\bar{x}}^{2} &amp; \sigma_{\bar{x} \bar{x}} \\
            \sigma_{\bar{x} \bar{x}} &amp; \sigma_{\bar{x}}^{2}
            \end{array}\right]\left[\begin{array}{l}
            1 \\
            0
            \end{array}\right]\left[\begin{array}{c}
            1 \\
            \frac{\sigma_{\bar{x}}^{2}+\sigma_{z}^{2}}{2}
            \end{array}\right]=\left[\begin{array}{c}
            \sigma_{\bar{x}}^{2}
            /\left(\sigma_{\bar{x}}^{2}+\sigma_{z}^{2}\right) \\
            \sigma_{\bar{x} \bar{x}}^{2}
            /\left(\sigma_{\bar{x}}^{2}+\sigma_{z}^{2}\right)
            \end{array}\right] \\
            \mathbf{H}=\left[\begin{array}{cc}
            1 &amp; 0
            \end{array}\right] \quad
            \mathbf{R}=\left[\sigma_{z}^{2}\right] &amp;
            \mathbf{x}=\overline{\mathbf{x}}+\mathbf{K
            y}=\left[\begin{array}{l}
            \bar{x} \\
            \overline{\dot{x}}
            \end{array}\right]+\left[\begin{array}{c}
            \sigma_{\bar{x}}^{2}
            /\left(\sigma_{\bar{x}}^{2}+\sigma_{Z}^{2}\right) \\
            \sigma_{\overline{\bar{x}} \bar{x}}^{2}
            /\left(\sigma_{\bar{x}}^{2}+\sigma_{z}^{2}\right)
            \end{array}\right] y
            \end{array}
            $$</span></p>
            <p>Recall that,</p>
            <p><span class="math display">$$
            \mathbf{\overline{P}} = \left[\begin{array}{c}
            \sigma_{\bar{x}}^{2}
            /\left(\sigma_{\bar{x}}^{2}+\sigma_{z}^{2}\right) \\
            \sigma_{\bar{x} \bar{x}}^{2}
            /\left(\sigma_{\bar{x}}^{2}+\sigma_{z}^{2}\right)
            \end{array}\right]
            $$</span></p>
            <p>Hence, we can obtain the equation for the velocity,</p>
            <p><span class="math display">$$
            \dot{x}=\bar{x}+\frac{\sigma_{\bar{x}
            \bar{x}}^{2}}{\sigma_{x}^{2}+\sigma_{z}^{2}} y
            $$</span></p>
            <p>From this equation,</p>
            <table>
            <thead>
            <tr>
            <th><span class="math inline"><strong>y</strong></span></th>
            <th style="text-align: left;">meaning</th>
            <th style="text-align: left;">result</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span class="math inline"><em>y</em> = 0</span></td>
            <td style="text-align: left;"><strong>perfect</strong>
            prediction</td>
            <td style="text-align: left;">velocity → <strong>no
            change</strong></td>
            </tr>
            <tr>
            <td><span class="math inline"><em>y</em> ≠ 0</span></td>
            <td style="text-align: left;"><strong>wrong</strong>
            prediction</td>
            <td style="text-align: left;">velocity →
            <strong>updated</strong></td>
            </tr>
            <tr>
            <td></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>The delta of the update, in the second case, depends
            on:</p>
            <ul>
            <li>residual and</li>
            <li><span
            class="math inline"><em>σ</em><sub><em>x̄</em><em>x̄</em></sub><sup>2</sup></span></li>
            </ul>
            </section>
            <section id="multivariate-kalman-filter-summary"
            class="level3" data-number="8.4.7">
            <h3 data-number="8.4.7"><span
            class="header-section-number">8.4.7</span> Multivariate
            Kalman Filter Summary</h3>
            <p>To summarize,</p>
            <p><img src="img/ekf/multi_kalman/multivariate_summary.png" width="500"></p>
            <p>where,</p>
            <ol type="1">
            <li><strong>prediction</strong></li>
            </ol>
            <p><span class="math display">$$
            \begin{aligned}
            &amp; \overline{\mathbf{x}_k}=\mathbf{F}
            \mathbf{x}_{k-1}+\mathbf{B} \mathbf{u} \\
            &amp; \overline{\mathbf{P}_k}=\mathbf{F P}_{k-1}
            \mathbf{F}^{\mathrm{T}}+\mathbf{Q}
            \end{aligned}
            $$</span></p>
            <ol start="2" type="1">
            <li><strong>update</strong></li>
            </ol>
            <p><span class="math display">$$
            \begin{aligned}
            &amp;
            \mathbf{x}_{\mathbf{k}}=\overline{\mathbf{x}_k}+\mathbf{K}_{\mathbf{k}}
            \mathbf{y}_{\mathbf{k}} \\
            &amp;
            \mathbf{P}_{\mathbf{k}}=\left(\mathbf{I}-\mathbf{K}_{\mathbf{k}}
            \mathbf{H}\right) \overline{\mathbf{P}_{\mathbf{k}}}
            \end{aligned}
            $$</span></p>
            <p>where <span
            class="math inline"><strong>K</strong><sub><strong>k</strong></sub></span>
            and <span
            class="math inline"><strong>y</strong><sub><strong>k</strong></sub></span>
            are calculated as,</p>
            <p><span class="math display">$$
            \begin{aligned}
            &amp;
            \mathbf{y}_{\mathbf{k}}=\mathbf{z}_{\mathbf{k}}-\mathbf{H}
            \overline{\mathbf{x}_{\mathbf{k}}} \\
            &amp; \mathbf{S}_{\mathbf{k}}=\mathbf{H}
            \overline{\mathbf{P}_{\mathbf{k}}}
            \mathbf{H}^{\mathrm{T}}+\mathbf{R} \\
            &amp;
            \mathbf{K}_{\mathbf{k}}=\overline{\mathbf{P}_{\mathbf{k}}}
            \mathbf{H}^{\mathrm{T}} \mathbf{S}_{\mathbf{k}}^{-1}
            \end{aligned}
            $$</span></p>
            <p>An example of a 2D state estimation using multivariate
            Kalman filters:</p>
            <p><img src="img/ekf/multi_kalman/2d_estimation.png" width="500"></p>
            <p><strong>Note</strong>: the problem with Kalman Filters is
            that it only works for <strong>linear</strong> systems,
            <em>i.e.,</em></p>
            <ul>
            <li>next state is a linear function of the previous
            state</li>
            <li>example of non-linear systems: falling object with air
            resistance</li>
            </ul>
            <p>But the real world is actually
            <strong>non-linear</strong>! To deal with non-linear
            systems, we use → the <strong>extended Kalman
            filter</strong> (EKF). We will explore this using an
            important application → <strong>sensor fusion</strong>.</p>
            <p><strong>References:</strong></p>
            <ul>
            <li>Kalman, R. E. (1960). <a
            href="https://www.cs.unc.edu/~welch/kalman/kalmanPaper.html">A
            New Approach to Linear Filtering and Prediction
            Problems</a>. Journal of Basic Engineering, 82(1),
            35-45.</li>
            <li>Bar-Shalom, Y., Li, X. R., &amp; Kirubarajan, T. (2001).
            <a
            href="https://onlinelibrary.wiley.com/doi/book/10.1002/0471221279">Estimation
            with Applications to Tracking and Navigation</a>.
            Wiley-Interscience.</li>
            <li>Simon, D. (2006). <a
            href="https://onlinelibrary.wiley.com/doi/book/10.1002/0470045345">Optimal
            State Estimation: Kalman, H∞, and Nonlinear Approaches</a>.
            Wiley-Interscience.</li>
            <li>Thrun, S., Burgard, W., &amp; Fox, D. (2005). <a
            href="https://mitpress.mit.edu/books/probabilistic-robotics">Probabilistic
            Robotics</a>. MIT Press.</li>
            <li>Welch, G., &amp; Bishop, G. (2006). <a
            href="https://www.cs.unc.edu/~welch/media/pdf/kalman_intro.pdf">An
            Introduction to the Kalman Filter</a>. University of North
            Carolina at Chapel Hill.</li>
            <li>Julier, S. J., &amp; Uhlmann, J. K. (1997). <a
            href="https://www.cs.unc.edu/~welch/kalman/media/pdf/Julier1997_SPIE_KF.pdf">A
            New Extension of the Kalman Filter to Nonlinear Systems</a>.
            Proc. SPIE 3068, Signal Processing, Sensor Fusion, and
            Target Recognition VI.</li>
            <li><a href="https://www.kalmanfilter.net">Kalman Filter
            Tutorial</a> - A comprehensive online resource with examples
            and applications.</li>
            <li><a
            href="https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python">Kalman
            and Bayesian Filters in Python</a> - A free interactive book
            and Python library by Roger Labbe.</li>
            <li><a href="https://filterpy.readthedocs.io">FilterPy</a> -
            A Python library for Kalman filtering and optimal
            estimation.</li>
            <li>Särkkä, S. (2013). <a
            href="https://www.cambridge.org/core/books/bayesian-filtering-and-smoothing/C372FB31C5D9A100F8476C1A41A5B700">Bayesian
            Filtering and Smoothing</a>. Cambridge University
            Press.</li>
            <li><a
            href="https://www.bzarg.com/p/how-a-kalman-filter-works-in-pictures/">How
            a Kalman filter works, in pictures</a></li>
            <li><a
            href="https://engineeringmedia.com/controlblog/the-kalman-filter">#2:
            The Kalman Filter</a> by Brian Douglas</li>
            <li><a
            href="https://web.mat.upc.edu/josep.fabrega/pipe/gaussianes_ndim-e-handout-4pp.pdf">The
            multivariate gaussian distribution</a> by Josep
            FÀBREGA<!--rel="stylesheet" href="./custom.sibin.css"--></li>
            </ul>
            </section>
            </section>
            </section>
            <section id="sensor-fusion" class="level1" data-number="9">
            <h1 data-number="9"><span
            class="header-section-number">9</span> Sensor Fusion</h1>
            <p>As we’ve discussed so far, sensors and state estimation,
            by its very probabilistic and noisy nature, introduces
            <strong>errors</strong>. The <a href="#bayes-filter">example
            from Bayes’ Filter</a> where two (seemingly identical)
            sensors measure the same object differently, highlights
            this:</p>
            <p><img src="img/ekf/bayes.lidar.4.png" width="400"></p>
            <p><br></p>
            <p>So, the question is → <em>“which sensor do we
            believe?”</em></p>
            <p>Maybe, instead of framing it as picking one sensor
            vs. the other, perhaps we can use <strong>both</strong>?
            This is the objective of <strong>sensor fusion</strong>.</p>
            <blockquote>
            <p>Sensor fusion is the process of combining sensory data
            from multiple sources to obtain more accurate information
            than would be possible using individual sensors alone.</p>
            </blockquote>
            <p>Recall that individual sensors have inherent
            limitations:</p>
            <ul>
            <li><strong>limited accuracy</strong>: each sensor has
            measurement errors</li>
            <li><strong>limited range/coverage</strong>: sensors may
            only work reliably in certain conditions</li>
            <li><strong>limited sampling rates</strong>: some sensors
            cannot provide updates fast enough</li>
            <li><strong>sensor-specific weaknesses</strong>: GPS fails
            indoors, cameras struggle in darkness, etc.</li>
            </ul>
            <p>Sensor fusion addresses these limitations by combining
            complementary data from multiple sensors, each with
            different strengths and weaknesses. The goal is to produce a
            combined result that is more accurate, complete, and robust
            than any single sensor could provide.</p>
            <section id="kalman-filter-for-sensor-fusion" class="level3"
            data-number="9.0.1">
            <h3 data-number="9.0.1"><span
            class="header-section-number">9.0.1</span> Kalman Filter for
            Sensor Fusion</h3>
            <p>We can, of course, use the Kalman Filter to carry out the
            sensor fusion. For instance,</p>
            <ul>
            <li>consider State, <span
            class="math inline">$\mathbf{x}=\left[\begin{array}{l}x \\
            \dot{x}\end{array}\right]$</span></li>
            <li><strong>two sensors</strong>, GPS (in metres), odometry
            (in feet): <span
            class="math inline">$\mathbf{z}=\left[\begin{array}{c}z_{G P
            S} \\ z_{\text {odom }}\end{array}\right]$</span></li>
            </ul>
            <p>So, by using the parameters for the Kalman Filter,</p>
            <p><span class="math display">$$
            \begin{aligned}
            \mathbf{H} &amp; =\left[\begin{array}{cc}
            1 &amp; 0 \\
            1 / 0.3048 &amp; 0
            \end{array}\right] \\
            \mathbf{R} &amp; =\left[\begin{array}{cc}
            \sigma_{\text {GPS }}^{2} &amp; 0 \\
            0 &amp; \sigma_{\text {Odom }}^{2}
            \end{array}\right]
            \end{aligned}
            $$</span></p>
            <p><span class="math display">$$
            \begin{aligned}
            \mathbf{y}= &amp; \mathbf{z}-\mathbf{H}
            \overline{\mathbf{x}} \\
            \mathbf{y}= &amp; {\left[\begin{array}{c}
            z_{G P S} \\
            z_{\text {Odom }}
            \end{array}\right]-\left[\begin{array}{cc}
            1 &amp; 0 \\
            1 / 0.3048 &amp; 0
            \end{array}\right]\left[\begin{array}{l}
            \bar{x} \\
            \bar{\dot{x}}
            \end{array}\right] } \\
            \mathbf{x}= &amp; \overline{\mathbf{x}}+{\mathbf{K}
            \mathbf{y}} \\
            \end{aligned}
            $$</span></p>
            <p>Note that <span
            class="math inline"><strong>K</strong></span> and <span
            class="math inline"><strong>y</strong></span> are <span
            class="math inline">2 × 2</span> matrices → it <strong>mixes
            the GPS and odometry</strong>!</p>
            <p>As mentioned earlier, the Kalman Filter doesn’t work well
            with *non-linear** systems. But what <em>exactly</em> is the
            problem with nonlinear systems?</p>
            <p>Recall that Kalman Filter (and any Bayes’ Filter)
            requires the use of Gaussians that has some nice properties
            but that breaks down in the face of non-linearities:</p>
            <table>
            <thead>
            <tr>
            <th style="text-align: left;">mixing of</th>
            <th style="text-align: left;">result</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;">two Gaussians</td>
            <td style="text-align: left;">Gaussian</td>
            </tr>
            <tr>
            <td style="text-align: left;">Gaussian and linear
            function</td>
            <td style="text-align: left;">Gaussian</td>
            </tr>
            <tr>
            <td style="text-align: left;">Gaussian and non-linear
            function</td>
            <td
            style="text-align: left;"><strong>non-Gaussian</strong></td>
            </tr>
            <tr>
            <td style="text-align: left;">linear and non-linear
            function</td>
            <td
            style="text-align: left;"><strong>non-Gaussian</strong></td>
            </tr>
            <tr>
            <td style="text-align: left;">two non-linear function</td>
            <td
            style="text-align: left;"><strong>non-Gaussian</strong></td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>So, all the nice Kalman filter steps (prediction,
            measurement, update) <strong>falls apart</strong> when the
            results are no longer Gaussians. Let’s see <a
            href="https://soulhackerslabs.com/sensor-fusion-with-the-extended-kalman-filter-in-ros-2-d33dbab1829d">two
            instances of passing a Gaussian through other functions</a>,
            <span
            class="math inline"><em>y</em> = <em>g</em>(<em>x</em>)</span></p>
            <ul>
            <li>a linear one, <span
            class="math inline"><em>g</em> = 0.5 * <em>x</em> + 1</span></li>
            <li>a non-linear one, <span class="math inline">$g = \cos(3
            * (\frac{x}{2} + 0.7)) * \sin(1.3 * x) — 1.6 *
            x$</span></li>
            </ul>
            <p><img src="img/fusion/ekf/gaussian_linear.gif" width="300">
            <img src="img/fusion/ekf/gaussian_non_linear.gif" width="300"></p>
            <p><br></p>
            <p>As we see from the second animation, the <strong>output
            is no longer a Gaussian</strong>.</p>
            <p><strong>Note:</strong> This output distribution also
            violates the <strong>unimodality assumption</strong> of the
            Kalman Filter, which requires a single peak. Hence, all of
            our elegant Kalman filter equations are rendered
            useless!</p>
            <p>Of course, we can try to compute an equivalent of a
            Gaussian using,</p>
            <ul>
            <li><a
            href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte
            Carlo simulations</a> that uses <strong>repeated random
            sampling</strong> to obtain numerical results</li>
            </ul>
            <p><img src="img/fusion/ekf/monte_carlo_wiki.gif" width="300" title="Monte Carlo Animation By Titouan Christophe - Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid=30599003"></p>
            <p><br></p>
            <p>The problem is that this is <strong>not a closed
            form</strong>, <em>i.e.,</em> it <strong>cannot</strong> be
            represented as s combination of,</p>
            <ul>
            <li>constants</li>
            <li>variables</li>
            <li>a finite set of basic functions connected by arithmetic
            operations (+, −, ×, /, and integer powers)</li>
            <li>function composition</li>
            </ul>
            <p>Hence, we still do not have a mathematical function for
            the Gaussian that can be plugged into the Kalman
            Equations.</p>
            <p>Enter the <strong><a
            href="#extended-kalman-filter-ekf">Extended Kalman Filter
            (EKF)</a></strong> that uses <strong>linearization</strong>
            → to <strong>approximate</strong> a non-linear function,
            <span class="math inline"><em>g</em>(.)</span>, by a
            <strong>linear function that is tangent to <span
            class="math inline"><em>g</em>(.)</span></strong> at the
            <strong>point of interest</strong>.</p>
            <p>EKF achieves this by use of the <a
            href="#taylor-series">Taylor Series Expansion</a>.</p>
            </section>
            <section id="taylor-series" class="level3"
            data-number="9.0.2">
            <h3 data-number="9.0.2"><span
            class="header-section-number">9.0.2</span> Taylor
            Series</h3>
            <p>Before diving into the Extended Kalman Filter, it’s
            important to understand Taylor series expansions, which are
            the mathematical foundation for linearization in EKF.</p>
            <p>A Taylor series expansion approximates a function around
            a specific point using polynomial terms. The Taylor
            Expansion of a function is an infinite sum of terms
            (polynomials) expressed in terms of the function’s
            derivatives at a single point, <span
            class="math inline"><em>a</em></span>, expanded as:</p>
            <p><span class="math display">$$
            f(x) = f(a) + f'(a)(x-a) + \frac{f''(a)}{2!}(x-a)^2 +
            \frac{f'''(a)}{3!}(x-a)^3 + \ldots
            $$</span></p>
            <p>where:</p>
            <table>
            <colgroup>
            <col style="width: 31%" />
            <col style="width: 68%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">term</th>
            <th style="text-align: left;">description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><span
            class="math inline"><em>f</em>(<em>a</em>)</span></td>
            <td style="text-align: left;">the function value at point
            <span class="math inline"><em>a</em></span></td>
            </tr>
            <tr>
            <td style="text-align: left;"><span
            class="math inline"><em>f</em>′(<em>a</em>)</span>, <span
            class="math inline"><em>f</em>″(<em>a</em>)</span>,
            etc.</td>
            <td style="text-align: left;">the derivatives of <span
            class="math inline"><em>f</em></span> evaluated at point
            <span class="math inline"><em>a</em></span></td>
            </tr>
            <tr>
            <td style="text-align: left;"><span
            class="math inline">(<em>x</em> − <em>a</em>)</span></td>
            <td style="text-align: left;">represents the deviation from
            the expansion point</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><img src="img/fusion/ekf/taylor_series_degrees.webp" width="300"></p>
            <p><br></p>
            <p>Note that the higher-order Taylor Expansions provide a
            <strong>closer approximation</strong> of the original
            function. But the amount of computation required quickly
            makes it <strong>intractable</strong>! There is still a use
            for it though, as we shall see shortly.</p>
            <p>For multivariate functions <span
            class="math inline"><em>f</em>(<strong>x</strong>)</span>
            where <span class="math inline"><strong>x</strong></span> is
            a vector, the first-order Taylor expansion around a point
            <span class="math inline"><strong>a</strong></span> is:</p>
            <p><span
            class="math inline"><em>f</em>(<strong>x</strong>) ≈ <em>f</em>(<strong>a</strong>) + ∇<em>f</em>(<strong>a</strong>)<sup><em>T</em></sup>(<strong>x</strong> − <strong>a</strong>)</span></p>
            <p>Where <span
            class="math inline">∇<em>f</em>(<strong>a</strong>)</span>
            is the gradient of <span
            class="math inline"><em>f</em></span> evaluated at <span
            class="math inline"><strong>a</strong></span>, containing
            all partial derivatives.</p>
            <p><strong>Example</strong>: Consider a simple nonlinear
            function <span
            class="math inline"><em>f</em>(<em>x</em>) = <em>x</em><sup>2</sup></span>
            expanded around <span
            class="math inline"><em>a</em> = 1</span>: 1. <span
            class="math inline"><em>f</em>(1) = 1</span> 2. <span
            class="math inline"><em>f</em>′(<em>x</em>) = 2<em>x</em></span>,
            so <span class="math inline"><em>f</em>′(1) = 2</span> 3.
            first-order Taylor approximation: <span
            class="math inline"><em>f</em>(<em>x</em>) ≈ 1 + 2(<em>x</em> − 1)</span>
            4. this gives us a linear approximation: <span
            class="math inline"><em>f</em>(<em>x</em>) ≈ 2<em>x</em> − 1</span></p>
            <p>This linear approximation is accurate near <span
            class="math inline"><em>x</em> = 1</span> but becomes less
            accurate as we move away from this point.</p>
            <p><img src="img/fusion/ekf/taylor-series-fixed.svg" width="300"></p>
            <p><br></p>
            <p><strong>Example 2</strong>: consider the non-linear
            function we discussed earlier,</p>
            <p><span class="math display">$$
            g = \cos(3 * (\frac{x}{2} + 0.7)) * \sin(1.3 * x) — 1.6 * x
            $$</span></p>
            <p>The first-order Taylor expansion of <span
            class="math inline"><em>g</em></span> at a point <span
            class="math inline"><em>a</em></span> is shown below in red.
            It clearly <strong>does not provide a good
            approximation</strong> → if we observe it over a large range
            of values for <span
            class="math inline"><em>x</em></span>.</p>
            <p><img src="img/fusion/ekf/taylor.1.webp" width="300"></p>
            <p><br></p>
            <p>Remember that</p>
            <ul>
            <li>we are only concerned with the approximation at the
            values of → the posterior</li>
            <li>we will be <strong>recomputing</strong> such
            approximations for the new posteriors <strong>after a very
            short period of time</strong></li>
            <li>our approximation → is <strong>quite good</strong> in
            the <strong>close vicinity of the point of
            interest</strong></li>
            </ul>
            <p><img src="img/fusion/ekf/taylor.2.webp" width="300"></p>
            <p><br></p>
            <p>For EKF, we use <strong>first-order Taylor
            expansions</strong> to → linearize our nonlinear system and
            measurement functions.</p>
            </section>
            <section id="extended-kalman-filter-ekf" class="level2"
            data-number="9.1">
            <h2 data-number="9.1"><span
            class="header-section-number">9.1</span> Extended Kalman
            Filter (EKF)</h2>
            <p>The Extended Kalman Filter addresses the limitations of
            the linear Kalman Filter by linearizing nonlinear functions
            <strong>around the current estimate</strong>. For nonlinear
            systems of the form:</p>
            <p><strong>State Equation</strong>: <span
            class="math inline"><em>x</em><sub><em>k</em></sub> = <em>f</em>(<em>x</em><sub><em>k</em> − 1</sub>, <em>u</em><sub><em>k</em></sub>) + <em>w</em><sub><em>k</em></sub></span></p>
            <p><strong>Measurement Equation</strong>: <span
            class="math inline"><em>z</em><sub><em>k</em></sub> = <em>h</em>(<em>x</em><sub><em>k</em></sub>) + <em>v</em><sub><em>k</em></sub></span></p>
            <p>The EKF linearizes these equations using first-order
            Taylor series expansions:</p>
            <p><span
            class="math inline"><em>f</em>(<em>x</em><sub><em>k</em> − 1</sub>, <em>u</em><sub><em>k</em></sub>) ≈ <em>f</em>(<em>x̂</em><sub><em>k</em> − 1|<em>k</em> − 1</sub>, <em>u</em><sub><em>k</em></sub>) + <em>F</em><sub><em>k</em></sub>(<em>x</em><sub><em>k</em> − 1</sub> − <em>x̂</em><sub><em>k</em> − 1|<em>k</em> − 1</sub>)</span></p>
            <p><span
            class="math inline"><em>h</em>(<em>x</em><sub><em>k</em></sub>) ≈ <em>h</em>(<em>x̂</em><sub><em>k</em>|<em>k</em> − 1</sub>) + <em>H</em><sub><em>k</em></sub>(<em>x</em><sub><em>k</em></sub> − <em>x̂</em><sub><em>k</em>|<em>k</em> − 1</sub>)</span></p>
            <p>Where: - <span class="math inline">$F_k =
            \left.\frac{\partial f}{\partial
            x}\right|_{\hat{x}_{k-1|k-1},u_k}$</span> is the Jacobian of
            <span class="math inline"><em>f</em></span> with respect to
            <span class="math inline"><em>x</em></span> - <span
            class="math inline">$H_k = \left.\frac{\partial h}{\partial
            x}\right|_{\hat{x}_{k|k-1}}$</span> is the Jacobian of <span
            class="math inline"><em>h</em></span> with respect to <span
            class="math inline"><em>x</em></span></p>
            <p><br></p>
            <details>
            <summary>
            subscripts explained
            </summary>
            <p>The notation <span
            class="math inline"><em>x̂</em><sub><em>k</em> − 1 ∣ <em>k</em> − 1</sub></span>:</p>
            <ul>
            <li><span
            class="math inline"><em>x</em><sub><em>k</em> − 1 ∣ <em>k</em> − 1</sub></span>​
            in the equation represents a specific type of state estimate
            in Kalman filtering</li>
            <li><span class="math inline"><em>x̂</em></span> symbol
            indicates it’s an estimate of the true state xx x</li>
            <li>The first subscript <span
            class="math inline"><em>k</em> − 1</span> indicates the time
            step of the state being estimated</li>
            <li>The second subscript <span
            class="math inline"><em>k</em> − 1|<em>k</em> − 1</span>
            (after the vertical bar) indicates that this estimate is
            based on all measurements up to and including time step
            <span class="math inline"><em>k</em> − 1</span></li>
            <li>So <span
            class="math inline"><em>x̂</em><sub><em>k</em> − 1 ∣ <em>k</em> − 1</sub></span>,
            specifically means “the estimate of the state at time <span
            class="math inline"><em>k</em> − 1</span> given all
            measurements up to time <span
            class="math inline"><em>k</em> − 1</span></li>
            <li>This is also called the a posteriori (or
            updated/corrected) state estimate from the previous time
            step.</li>
            </ul>
            <p>In Kalman filter terminology:</p>
            <ul>
            <li><span
            class="math inline"><em>x̂</em><sub><em>k</em> ∣ <em>k</em> − 1</sub></span>
            would be the a priori estimate (prediction before
            measurement update)</li>
            <li><span
            class="math inline"><em>x̂</em><sub><em>k</em> ∣ <em>k</em></sub></span>
            would be the a posteriori estimate (after measurement
            update)</li>
            </ul>
            The equation is linearizing the nonlinear state transition
            function <span class="math inline"><em>f</em></span> around
            the best estimate we have of the previous state (<span
            class="math inline"><em>x̂</em><sub><em>k</em> − 1|<em>k</em> − 1</sub></span>)
            which makes sense since that’s our most accurate knowledge
            of where the system was at the previous time step.
            </details>
            <p><br></p>
            <p>These Jacobian matrices contain all the partial
            derivatives of the nonlinear functions with respect to each
            state variable, evaluated at the current estimate. They
            represent the sensitivity of the functions to small changes
            in the state.</p>
            <p>This linearization allows us to</p>
            <ul>
            <li>apply the Kalman filter equations to nonlinear
            systems</li>
            <li>but introduces <strong>approximation errors</strong>
            when the system is highly nonlinear or when the state
            estimate is far from the true state.</li>
            </ul>
            <p>Let’s revisit our earlie example function, <span
            class="math inline"><em>g</em>(.)</span> and see what
            happens when we use the linearized functions (as compared to
            the Monte-Carlo output):</p>
            <p><img src="img/fusion/ekf/gaussian_linearization.2.webp" width="500"></p>
            <p><br></p>
            <p>We see that the EKF Gaussian is <strong>not exactly the
            same</strong> as the one fitted from the Monte Carlo
            simulation, but it is <strong>close enough</strong>. This
            discrepancy (albeit small) is the price we pay for,</p>
            <ul>
            <li>obtaining a closed form estimate and</li>
            <li>the low computational costs.</li>
            </ul>
            <p><strong>Note</strong>: these graphs show the results of
            EKF approximations applied to <strong>scalar</strong>
            functions but our <strong>state is a vector</strong> so we
            need to:</p>
            <ul>
            <li>we compute the partial derivative of g with respect to
            the state</li>
            <li>use <a
            href="https://en.wikipedia.org/wiki/Jacobian_matrix_and_determinant">Jacobian
            matrices</a>.</li>
            </ul>
            <section id="ekf-algorithm" class="level3"
            data-number="9.1.1">
            <h3 data-number="9.1.1"><span
            class="header-section-number">9.1.1</span> EKF
            Algorithm</h3>
            <p>The EKF algorithm follows these steps:</p>
            <p><strong>1. Prediction Step</strong>: <span
            class="math display"><em>x̂</em><sub><em>k</em>|<em>k</em> − 1</sub> = <em>f</em>(<em>x̂</em><sub><em>k</em> − 1|<em>k</em> − 1</sub>, <em>u</em><sub><em>k</em></sub>)</span>
            <span
            class="math display"><em>P</em><sub><em>k</em>|<em>k</em> − 1</sub> = <em>F</em><sub><em>k</em></sub><em>P</em><sub><em>k</em> − 1|<em>k</em> − 1</sub><em>F</em><sub><em>k</em></sub><sup><em>T</em></sup> + <em>Q</em><sub><em>k</em></sub></span></p>
            <p><strong>2. Update Step</strong>: <span
            class="math display"><em>K</em><sub><em>k</em></sub> = <em>P</em><sub><em>k</em>|<em>k</em> − 1</sub><em>H</em><sub><em>k</em></sub><sup><em>T</em></sup>(<em>H</em><sub><em>k</em></sub><em>P</em><sub><em>k</em>|<em>k</em> − 1</sub><em>H</em><sub><em>k</em></sub><sup><em>T</em></sup> + <em>R</em><sub><em>k</em></sub>)<sup>−1</sup></span>
            <span
            class="math display"><em>x̂</em><sub><em>k</em>|<em>k</em></sub> = <em>x̂</em><sub><em>k</em>|<em>k</em> − 1</sub> + <em>K</em><sub><em>k</em></sub>(<em>z</em><sub><em>k</em></sub> − <em>h</em>(<em>x̂</em><sub><em>k</em>|<em>k</em> − 1</sub>))</span>
            <span
            class="math display"><em>P</em><sub><em>k</em>|<em>k</em></sub> = (<em>I</em> − <em>K</em><sub><em>k</em></sub><em>H</em><sub><em>k</em></sub>)<em>P</em><sub><em>k</em>|<em>k</em> − 1</sub></span></p>
            <p>The key difference compared to the linear Kalman filter
            is that we use the <strong>nonlinear functions</strong>, -
            <span class="math inline"><em>f</em></span> → state - <span
            class="math inline"><em>h</em></span> → measurement
            prediction</p>
            <p>but use their <strong>linearized versions</strong>
            (Jacobians) for → covariance calculation.</p>
            </section>
            <section id="ekf-and-sensor-fusion" class="level3"
            data-number="9.1.2">
            <h3 data-number="9.1.2"><span
            class="header-section-number">9.1.2</span> EKF and Sensor
            Fusion</h3>
            <p>There are <strong>two main approaches</strong> to
            multi-sensor fusion with EKF:</p>
            <ol type="1">
            <li><strong>Centralized Fusion</strong>:</li>
            </ol>
            <ul>
            <li>all sensor measurements are processed in a single
            EKF</li>
            <li>the measurement vector combines all sensor
            readings:</li>
            </ul>
            <p><span class="math display">$$z_k = \begin{bmatrix} z_k^1
            \\ z_k^2 \\ \vdots \\ z_k^n \end{bmatrix}$$</span></p>
            <ul>
            <li>the measurement function and noise covariance are:</li>
            </ul>
            <p><span class="math display">$$h(x_k) = \begin{bmatrix}
            h^1(x_k) \\ h^2(x_k) \\ \vdots \\ h^n(x_k) \end{bmatrix},
            \quad R_k = \begin{bmatrix} R_k^1 &amp; 0 &amp; \cdots &amp;
            0 \\ 0 &amp; R_k^2 &amp; \cdots &amp; 0 \\ \vdots &amp;
            \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; 0 &amp; \cdots
            &amp; R_k^n \end{bmatrix}$$</span></p>
            <ol start="3" type="1">
            <li><strong>Decentralized Fusion</strong>:</li>
            </ol>
            <ul>
            <li>each sensor → has its own local filter</li>
            <li>the results are combined at a <strong>fusion
            center</strong></li>
            </ul>
            <p>This approach is <strong>more modular</strong> and
            <strong>fault-tolerant</strong>.</p>
            <ul>
            <li>the state estimates from individual filters can be
            combined using covariance intersection:</li>
            </ul>
            <p><span class="math display">$$P_{fused}^{-1} =
            \sum_{i=1}^n P_i^{-1}$$</span> <span
            class="math display">$$P_{fused}^{-1}\hat{x}_{fused} =
            \sum_{i=1}^n P_i^{-1}\hat{x}_i$$</span></p>
            <p><strong>Sensor Covariance and Weighting</strong></p>
            <p>The measurement noise covariance matrix <span
            class="math inline"><em>R</em><sub><em>k</em></sub></span>
            determines how much weight each sensor’s measurements
            receive during fusion. Sensors with <strong>lower
            measurement uncertainty</strong> (smaller values in <span
            class="math inline"><em>R</em><sub><em>k</em></sub></span>)
            → have <strong>more influence on final state
            estimate</strong>.</p>
            <p>Adaptive methods can dynamically adjust these covariances
            based on:</p>
            <ul>
            <li>current operating conditions</li>
            <li>sensor health monitoring</li>
            <li>consistency checks between sensors</li>
            <li>historical performance</li>
            </ul>
            <p>For example, GPS accuracy might degrade in urban canyons,
            so its covariance should increase in those environments.</p>
            </section>
            <section id="example-imu-and-gps-fusion-using-ekf"
            class="level3" data-number="9.1.3">
            <h3 data-number="9.1.3"><span
            class="header-section-number">9.1.3</span> Example: IMU and
            GPS Fusion using EKF</h3>
            <p>Let’s consider a common problem: fusing IMU
            (accelerometer and gyroscope) and GPS data for position
            tracking.</p>
            <p><strong>State Vector</strong>: <span
            class="math display"><em>x</em> = [position<sub><em>x</em></sub>, position<sub><em>y</em></sub>, velocity<sub><em>x</em></sub>, velocity<sub><em>y</em></sub>, heading]<sup><em>T</em></sup></span></p>
            <p><strong>Note:</strong> The “<span
            class="math inline"><em>T</em></span>” as a superscript
            after the closing bracket indicates that this is a column
            vector (transposed from a row vector). This is standard
            mathematical notation for representing a column vector in a
            single line of text.</p>
            <p><strong>State Transition</strong>: Using a constant
            velocity model with heading changes from gyroscope:</p>
            <p><span class="math display">$$f(x_{k-1}, u_k) =
            \begin{bmatrix}
            \text{position}_x + \text{velocity}_x \Delta t \\
            \text{position}_y + \text{velocity}_y \Delta t \\
            \text{velocity}_x + a_x \Delta t \\
            \text{velocity}_y + a_y \Delta t \\
            \text{heading} + \omega_z \Delta t
            \end{bmatrix}$$</span></p>
            <p>Where <span
            class="math inline"><em>a</em><sub><em>x</em></sub>, <em>a</em><sub><em>y</em></sub></span>
            are accelerometer readings and <span
            class="math inline"><em>ω</em><sub><em>z</em></sub></span>
            is the gyroscope reading (yaw rate).</p>
            <p><strong>Measurement Models</strong>: - GPS: <span
            class="math inline"><em>h</em><sub><em>G</em><em>P</em><em>S</em></sub>(<em>x</em><sub><em>k</em></sub>) = [position<sub><em>x</em></sub>, position<sub><em>y</em></sub>]<sup><em>T</em></sup></span>
            - IMU (for update): <span
            class="math inline"><em>h</em><sub><em>I</em><em>M</em><em>U</em></sub>(<em>x</em><sub><em>k</em></sub>) = [velocity<sub><em>x</em></sub>, velocity<sub><em>y</em></sub>, heading]<sup><em>T</em></sup></span></p>
            <p><strong>Jacobian Matrices</strong>: The state transition
            Jacobian <span
            class="math inline"><em>F</em><sub><em>k</em></sub></span>
            is:</p>
            <p><span class="math display">$$F_k = \begin{bmatrix}
            1 &amp; 0 &amp; \Delta t &amp; 0 &amp; 0 \\
            0 &amp; 1 &amp; 0 &amp; \Delta t &amp; 0 \\
            0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
            0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
            0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
            \end{bmatrix}$$</span></p>
            <p>The measurement Jacobians for GPS and IMU are:</p>
            <p><span class="math display">$$H_{GPS} = \begin{bmatrix}
            1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 \\
            0 &amp; 1 &amp; 0 &amp; 0 &amp; 0
            \end{bmatrix}$$</span></p>
            <p><span class="math display">$$H_{IMU} = \begin{bmatrix}
            0 &amp; 0 &amp; 1 &amp; 0 &amp; 0 \\
            0 &amp; 0 &amp; 0 &amp; 1 &amp; 0 \\
            0 &amp; 0 &amp; 0 &amp; 0 &amp; 1
            \end{bmatrix}$$</span></p>
            <p><strong>Pseudocode for Implementation</strong></p>
            <div class="sourceCode" id="cb21"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Initialize state and covariance</span></span>
<span id="cb21-2"><a href="#cb21-2" aria-hidden="true" tabindex="-1"></a>x_hat <span class="op">=</span> np.zeros(<span class="dv">5</span>)  <span class="co"># [pos_x, pos_y, vel_x, vel_y, heading]</span></span>
<span id="cb21-3"><a href="#cb21-3" aria-hidden="true" tabindex="-1"></a>P <span class="op">=</span> np.eye(<span class="dv">5</span>) <span class="op">*</span> <span class="dv">1000</span>  <span class="co"># Initial uncertainty</span></span>
<span id="cb21-4"><a href="#cb21-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-5"><a href="#cb21-5" aria-hidden="true" tabindex="-1"></a><span class="co"># Process noise covariance</span></span>
<span id="cb21-6"><a href="#cb21-6" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.diag([<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">0.01</span>])</span>
<span id="cb21-7"><a href="#cb21-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-8"><a href="#cb21-8" aria-hidden="true" tabindex="-1"></a><span class="co"># Measurement noise covariance</span></span>
<span id="cb21-9"><a href="#cb21-9" aria-hidden="true" tabindex="-1"></a>R_gps <span class="op">=</span> np.diag([<span class="fl">5.0</span>, <span class="fl">5.0</span>])  <span class="co"># GPS position accuracy (in meters)</span></span>
<span id="cb21-10"><a href="#cb21-10" aria-hidden="true" tabindex="-1"></a>R_imu <span class="op">=</span> np.diag([<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span>])  <span class="co"># IMU velocity and heading accuracy</span></span>
<span id="cb21-11"><a href="#cb21-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb21-12"><a href="#cb21-12" aria-hidden="true" tabindex="-1"></a><span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb21-13"><a href="#cb21-13" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Get IMU data (acceleration and angular velocity)</span></span>
<span id="cb21-14"><a href="#cb21-14" aria-hidden="true" tabindex="-1"></a>    accel_x, accel_y, gyro_z <span class="op">=</span> read_imu()</span>
<span id="cb21-15"><a href="#cb21-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-16"><a href="#cb21-16" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Prediction step</span></span>
<span id="cb21-17"><a href="#cb21-17" aria-hidden="true" tabindex="-1"></a>    dt <span class="op">=</span> time_since_last_update()</span>
<span id="cb21-18"><a href="#cb21-18" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-19"><a href="#cb21-19" aria-hidden="true" tabindex="-1"></a>    <span class="co"># State transition</span></span>
<span id="cb21-20"><a href="#cb21-20" aria-hidden="true" tabindex="-1"></a>    x_hat[<span class="dv">0</span>] <span class="op">+=</span> x_hat[<span class="dv">2</span>] <span class="op">*</span> dt  <span class="co"># pos_x += vel_x * dt</span></span>
<span id="cb21-21"><a href="#cb21-21" aria-hidden="true" tabindex="-1"></a>    x_hat[<span class="dv">1</span>] <span class="op">+=</span> x_hat[<span class="dv">3</span>] <span class="op">*</span> dt  <span class="co"># pos_y += vel_y * dt</span></span>
<span id="cb21-22"><a href="#cb21-22" aria-hidden="true" tabindex="-1"></a>    x_hat[<span class="dv">2</span>] <span class="op">+=</span> accel_x <span class="op">*</span> dt   <span class="co"># vel_x += accel_x * dt</span></span>
<span id="cb21-23"><a href="#cb21-23" aria-hidden="true" tabindex="-1"></a>    x_hat[<span class="dv">3</span>] <span class="op">+=</span> accel_y <span class="op">*</span> dt   <span class="co"># vel_y += accel_y * dt</span></span>
<span id="cb21-24"><a href="#cb21-24" aria-hidden="true" tabindex="-1"></a>    x_hat[<span class="dv">4</span>] <span class="op">+=</span> gyro_z <span class="op">*</span> dt    <span class="co"># heading += gyro_z * dt</span></span>
<span id="cb21-25"><a href="#cb21-25" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-26"><a href="#cb21-26" aria-hidden="true" tabindex="-1"></a>    <span class="co"># State transition Jacobian</span></span>
<span id="cb21-27"><a href="#cb21-27" aria-hidden="true" tabindex="-1"></a>    F <span class="op">=</span> np.eye(<span class="dv">5</span>)</span>
<span id="cb21-28"><a href="#cb21-28" aria-hidden="true" tabindex="-1"></a>    F[<span class="dv">0</span>, <span class="dv">2</span>] <span class="op">=</span> dt</span>
<span id="cb21-29"><a href="#cb21-29" aria-hidden="true" tabindex="-1"></a>    F[<span class="dv">1</span>, <span class="dv">3</span>] <span class="op">=</span> dt</span>
<span id="cb21-30"><a href="#cb21-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-31"><a href="#cb21-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Update covariance</span></span>
<span id="cb21-32"><a href="#cb21-32" aria-hidden="true" tabindex="-1"></a>    P <span class="op">=</span> F <span class="op">@</span> P <span class="op">@</span> F.T <span class="op">+</span> Q</span>
<span id="cb21-33"><a href="#cb21-33" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb21-34"><a href="#cb21-34" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Check if GPS reading is available</span></span>
<span id="cb21-35"><a href="#cb21-35" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> gps_available():</span>
<span id="cb21-36"><a href="#cb21-36" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Get GPS position</span></span>
<span id="cb21-37"><a href="#cb21-37" aria-hidden="true" tabindex="-1"></a>        gps_x, gps_y <span class="op">=</span> read_gps()</span>
<span id="cb21-38"><a href="#cb21-38" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-39"><a href="#cb21-39" aria-hidden="true" tabindex="-1"></a>        <span class="co"># GPS measurement Jacobian</span></span>
<span id="cb21-40"><a href="#cb21-40" aria-hidden="true" tabindex="-1"></a>        H_gps <span class="op">=</span> np.zeros((<span class="dv">2</span>, <span class="dv">5</span>))</span>
<span id="cb21-41"><a href="#cb21-41" aria-hidden="true" tabindex="-1"></a>        H_gps[<span class="dv">0</span>, <span class="dv">0</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb21-42"><a href="#cb21-42" aria-hidden="true" tabindex="-1"></a>        H_gps[<span class="dv">1</span>, <span class="dv">1</span>] <span class="op">=</span> <span class="fl">1.0</span></span>
<span id="cb21-43"><a href="#cb21-43" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-44"><a href="#cb21-44" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Innovation</span></span>
<span id="cb21-45"><a href="#cb21-45" aria-hidden="true" tabindex="-1"></a>        y <span class="op">=</span> np.array([gps_x, gps_y]) <span class="op">-</span> x_hat[<span class="dv">0</span>:<span class="dv">2</span>]</span>
<span id="cb21-46"><a href="#cb21-46" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-47"><a href="#cb21-47" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Innovation covariance</span></span>
<span id="cb21-48"><a href="#cb21-48" aria-hidden="true" tabindex="-1"></a>        S <span class="op">=</span> H_gps <span class="op">@</span> P <span class="op">@</span> H_gps.T <span class="op">+</span> R_gps</span>
<span id="cb21-49"><a href="#cb21-49" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-50"><a href="#cb21-50" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Kalman gain</span></span>
<span id="cb21-51"><a href="#cb21-51" aria-hidden="true" tabindex="-1"></a>        K <span class="op">=</span> P <span class="op">@</span> H_gps.T <span class="op">@</span> np.linalg.inv(S)</span>
<span id="cb21-52"><a href="#cb21-52" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-53"><a href="#cb21-53" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update state</span></span>
<span id="cb21-54"><a href="#cb21-54" aria-hidden="true" tabindex="-1"></a>        x_hat <span class="op">+=</span> K <span class="op">@</span> y</span>
<span id="cb21-55"><a href="#cb21-55" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb21-56"><a href="#cb21-56" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Update covariance</span></span>
<span id="cb21-57"><a href="#cb21-57" aria-hidden="true" tabindex="-1"></a>        P <span class="op">=</span> (np.eye(<span class="dv">5</span>) <span class="op">-</span> K <span class="op">@</span> H_gps) <span class="op">@</span> P</span></code></pre></div>
            <p><strong>Analysis</strong></p>
            <ol type="1">
            <li><strong>high-frequency updates</strong> → IMU provides
            updates at <span class="math inline">100 − 1000</span> Hz,
            allowing for <strong>smooth tracking</strong></li>
            <li><strong>drift correction</strong> → GPS (updating at
            <span class="math inline">1 − 10</span> Hz) corrects the
            <strong>accumulated drift</strong> from IMU integration</li>
            <li><strong>robustness to GPS outages</strong> → system can
            continue to provide <strong>reasonable position estimates
            during short GPS outages</strong>!</li>
            </ol>
            <p>The following graph shows a typical position tracking
            result comparing:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;"><strong>data
            source</strong></th>
            <th
            style="text-align: left;"><strong>description</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><strong><span
            style="color:blue">blue dots</span></strong></td>
            <td style="text-align: left;">raw gps data</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong><span
            style="color:red">red line</span></strong></td>
            <td style="text-align: left;">dead reckoning using only imu
            data</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong><span
            style="color:green">green line</span></strong></td>
            <td style="text-align: left;">ekf fusion of gps and imu</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/fusion/ekf/positional-tracking.svg" width="300"></p>
            <p><br></p>
            <details>
            <summary>
            Why does the EKF follow the GPS line more closely even
            though the IMU updates more frequently and (seemingly)
            starts with lower error?
            </summary>
            <p>From the code, we see:</p>
            <pre><code># Measurement noise covariance
R_gps = np.diag([5.0, 5.0])  # GPS position accuracy (in meters)
R_imu = np.diag([0.5, 0.5, 0.1])  # IMU velocity and heading accuracy</code></pre>
            <p>This might <em>seem</em> counterintuitive, but there are
            several important reasons for this behavior:</p>
            <ol type="1">
            <li><p><strong>complementary error characteristics</strong>:
            GPS provides <strong>absolute position measurements with
            bounded error</strong>, while IMU measures <strong>relative
            changes</strong> (acceleration and angular velocity) that
            suffer from <strong>unbounded error accumulation</strong>
            (drift). The Kalman filter weights these complementary
            information sources.</p></li>
            <li><p><strong>different types of uncertainty</strong>:
            although IMU updates more frequently, its measurements must
            be <strong>integrated</strong> once or even twice (for
            angular position and linear position respectively) to obtain
            position estimates. This integration process significantly
            <strong>amplifies errors over time</strong>, resulting in
            <strong>drift</strong>.</p></li>
            <li><p><strong>Observability concerns</strong>: position is
            <strong>directly observable</strong> from GPS but only
            <strong>indirectly observable</strong> from IMU via double
            integration. The EKF tends to give more weight to directly
            observable states when determining the optimal
            fusion.</p></li>
            <li><p><strong>Bias and scale factor errors</strong>: IMUs
            typically suffer from bias and scale factor errors that,
            while small initially, <strong>compound over time through
            the integration</strong> process, making their long-term
            position estimates less reliable.</p></li>
            <li><p><strong>Anchoring effect</strong>: GPS provides
            “ground truth” position fixes that anchor the estimated
            trajectory, while IMU primarily contributes to tracking
            short-term dynamics between GPS updates.</p></li>
            </ol>
            <p>This behavior is actually <strong>desirable</strong> in
            navigation systems – the fusion algorithm,</p>
            <ul>
            <li>leverages the IMU’s excellent short-term accuracy and
            high update rate to track dynamic movements while</li>
            <li>using the GPS’s long-term stability to periodically
            correct the accumulated drift.</li>
            </ul>
            <p>This is why in the visualization, during GPS outages, the
            EKF fusion trajectory <em>initially</em> follows the
            IMU-only path but <em>gradually converges</em> back to the
            GPS trajectory once GPS measurements become available
            again.</p>
            <p>Let’s work through the equations using
            <strong>concrete</strong> values. First, let’s understand
            the key mathematical reason for IMU drift versus GPS
            stability:</p>
            <p><strong>1.</strong> Integration Error Analysis for
            IMU</p>
            <p>For an IMU, we integrate acceleration twice to get
            position:</p>
            <pre><code>velocity(t) = ∫ Acceleration(τ) dτ
position(t) = ∫∫ Acceleration(τ) dτ²</code></pre>
            <p>Let’s assume a constant acceleration bias error of ε in
            an IMU. After time t, the position error becomes:</p>
            <pre><code>position error = (ε·t²)/2</code></pre>
            <p>This shows the error grows quadratically with time - a
            fundamental problem with inertial navigation.</p>
            <p><strong>2.</strong> GPS Error Characteristics</p>
            <p>GPS error is relatively constant over time and doesn’t
            accumulate:</p>
            <pre><code>position error(GPS) ≈ constant</code></pre>
            <p><strong>3.</strong> Quantitative Analysis Based on the
            Example</p>
            <p>Looking at the code in the document:</p>
            <div class="sourceCode" id="cb26"><pre
            class="sourceCode python"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Process noise covariance</span></span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>Q <span class="op">=</span> np.diag([<span class="fl">0.1</span>, <span class="fl">0.1</span>, <span class="fl">1.0</span>, <span class="fl">1.0</span>, <span class="fl">0.01</span>])</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Measurement noise covariance</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>R_gps <span class="op">=</span> np.diag([<span class="fl">5.0</span>, <span class="fl">5.0</span>])  <span class="co"># GPS position accuracy (in meters)</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>R_imu <span class="op">=</span> np.diag([<span class="fl">0.5</span>, <span class="fl">0.5</span>, <span class="fl">0.1</span>])  <span class="co"># IMU velocity and heading accuracy</span></span></code></pre></div>
            <p>Let’s calculate how errors propagate:</p>
            <ol type="1">
            <li><strong>IMU velocity error after <span
            class="math inline">10</span> seconds</strong>:
            <ul>
            <li>initial uncertainty: <span
            class="math inline">0.5<em>m</em>/<em>s</em></span> (from
            <span
            class="math inline"><em>R</em><sub><em>i</em><em>m</em><em>u</em></sub></span>)</li>
            <li>process noise: <span
            class="math inline">1.0<em>m</em>/<em>s</em>²</span> (from
            <span class="math inline"><em>Q</em></span> for
            velocity)</li>
            <li>After <span class="math inline">10<em>s</em></span>:
            <span class="math inline">$\sqrt{(0.5)^2 + (1.0 \cdot 10)^2}
            \approx 10.01 \text{ m/s}$</span></li>
            </ul></li>
            <li><strong>Position error from IMU after 10
            seconds</strong>:
            <ul>
            <li>From velocity error: <span class="math inline">$10.01
            m/s \cdot \frac{10s}{2} \approx 50.05 m$</span></li>
            <li>From position process noise: <span
            class="math inline">0.1<em>m</em> ⋅ 10<em>s</em> ≈ 1<em>m</em></span></li>
            <li>Total IMU-derived position error <span
            class="math inline"> ≈ 51.05<em>m</em></span></li>
            </ul></li>
            <li><strong>GPS position error</strong>:
            <ul>
            <li>Constant: <span class="math inline">5.0<em>m</em></span>
            (from <span
            class="math inline"><em>R</em><sub><em>g</em><em>p</em><em>s</em></sub></span>)</li>
            </ul></li>
            </ol>
            <p>The Kalman gain K is calculated as:</p>
            <p><span
            class="math display"><em>K</em><sub><em>k</em></sub> = <em>P</em><sub><em>k</em>|<em>k</em> − 1</sub><em>H</em><sub><em>k</em></sub><sup><em>T</em></sup>(<em>H</em><sub><em>k</em></sub><em>P</em><sub><em>k</em>|<em>k</em> − 1</sub><em>H</em><sub><em>k</em></sub><sup><em>T</em></sup> + <em>R</em><sub><em>k</em></sub>)<sup>−1</sup></span></p>
            <p>After 10 seconds, the predicted error covariance P for
            position from IMU-only would be dominated by the integrated
            errors (~51 m), while the GPS measurement error remains at 5
            m. This leads to:</p>
            <p><span class="math display">$$K \approx \frac{51^2}{51^2 +
            5^2} \approx 0.99$$</span></p>
            <p>for GPS updates.</p>
            <p>This means the filter will weigh the GPS measurement at
            about <span class="math inline">99%</span> and the
            IMU-propagated position at only about <span
            class="math inline">1%</span> when a GPS measurement arrives
            after <span class="math inline">10</span> seconds of
            IMU-only navigation.</p>
            <p><strong>4.</strong> Visual Evidence from the Graph</p>
            <p>In the position tracking visualization, you can see:</p>
            <ol type="1">
            <li>where GPS is available, the green EKF line closely
            follows the blue GPS points</li>
            <li>during the GPS outage region (red shaded area), the EKF
            initially follows the orange IMU track</li>
            <li>the IMU-only orange line progressively deviates from the
            ground truth (black dashed line)</li>
            <li>when GPS becomes available again, the EKF quickly
            converges back to GPS measurements</li>
            </ol>
            This behavior directly confirms our mathematical analysis.
            The Kalman filter is doing exactly what it should: using
            high-frequency IMU data for short-term tracking but relying
            more heavily on GPS for long-term stability due to the
            fundamentally different error accumulation characteristics
            of the two sensors.
            </details>
            </section>
            <section id="miscellaneous-and-advanced-topics"
            class="level3" data-number="9.1.4">
            <h3 data-number="9.1.4"><span
            class="header-section-number">9.1.4</span> Miscellaneous and
            Advanced Topics</h3>
            <p><strong>Best Practices</strong> when implementing EKF for
            sensor fusion,</p>
            <table>
            <colgroup>
            <col style="width: 60%" />
            <col style="width: 39%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;"><strong>best
            practices</strong></th>
            <th style="text-align: left;">description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><strong>careful state
            selection</strong></td>
            <td style="text-align: left;">include only necessary states
            to avoid computational burden</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>proper
            initialization</strong></td>
            <td style="text-align: left;">set initial covariance to
            reflect actual uncertainty</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>tuning noise
            parameters</strong></td>
            <td style="text-align: left;">adjust <span
            class="math inline"><em>Q</em></span> and <span
            class="math inline"><em>R</em></span> based on empirical
            data</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>consistency
            monitoring</strong></td>
            <td style="text-align: left;">check filter consistency using
            normalized innovation squared (NIS)</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>fault
            detection</strong></td>
            <td style="text-align: left;">implement mechanisms to detect
            sensor failures</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>numerical
            stability</strong></td>
            <td style="text-align: left;">use square-root or UD
            factorization for improved numerical properties</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><strong>Handling Non-Gaussian Noise</strong></p>
            <p>EKF assumes that both process and measurement noise are
            Gaussian. For systems with non-Gaussian noise, consider:</p>
            <table>
            <colgroup>
            <col style="width: 48%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;"><strong>method</strong></th>
            <th style="text-align: left;">description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><strong>particle
            filters</strong></td>
            <td style="text-align: left;">represent the probability
            distribution using samples</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>robust kalman
            filters</strong></td>
            <td style="text-align: left;">use heavy-tailed distributions
            to model outliers</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>pre-filtering</strong></td>
            <td style="text-align: left;">apply outlier rejection before
            using EKF</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>The Extended Kalman Filter is a powerful tool for sensor
            fusion in nonlinear systems. It provides a principled
            approach for combining measurements from multiple sensors
            with different characteristics, rates and accuracies.</p>
            <p>Despite its limitations with highly nonlinear systems,
            EKF remains the workhorse of many practical sensor fusion
            applications due to its relatively simple implementation and
            computational efficiency.</p>
            <p>For more complex scenarios, consider alternatives like
            the <a href="#unscented-kalman-filter-comparison">Unscented
            Kalman Filter</a>, [Particle
            Filters[(https://en.wikipedia.org/wiki/Monte_Carlo_localization)]
            or more recent developments in factor graph-based
            fusion.</p>
            </section>
            <section id="unscented-kalman-filter-comparison"
            class="level3" data-number="9.1.5">
            <h3 data-number="9.1.5"><span
            class="header-section-number">9.1.5</span> Unscented Kalman
            Filter Comparison</h3>
            <p>The EKF uses first-order linearization, which can
            introduce significant errors for highly nonlinear systems.
            The <strong><a
            href="https://soulhackerslabs.com/the-unreasonable-power-of-the-unscented-kalman-filter-with-ros-2-d4c97d4b4bb9">Unscented
            Kalman Filter (UKF)</a></strong> is an alternative that:</p>
            <ol type="1">
            <li>selects a set of sigma points around the current state
            estimate</li>
            <li>propagates these points through the nonlinear
            functions</li>
            <li>computes a weighted mean and covariance from the
            transformed points</li>
            </ol>
            <p>This <strong>avoids the need for explicit Jacobian
            calculations</strong> and can <strong>handle nonlinearities
            better</strong>. The computational complexity is similar to
            EKF for most practical applications.</p>
            <p><strong>Example</strong>: Target Tracking with Radar</p>
            <p>To illustrate the difference between EKF and UKF
            performance, consider a radar-based target tracking
            scenario:</p>
            <p><strong>System Configuration:</strong></p>
            <ul>
            <li>single radar measuring range, azimuth and range-rate to
            a target</li>
            <li>target following a coordinated turn trajectory (highly
            nonlinear dynamics)</li>
            <li>measurement frequency: <span
            class="math inline">1</span> Hz</li>
            <li>state vector: <span
            class="math inline">[<em>x</em>, <em>y</em>, <em>v</em><em>e</em><em>l</em><em>o</em><em>c</em><em>i</em><em>t</em><em>y</em><sub><em>x</em></sub>, <em>v</em><em>e</em><em>l</em><em>o</em><em>c</em><em>i</em><em>t</em><em>y</em><sub><em>y</em></sub>, <em>t</em><em>u</em><em>r</em><em>n</em>_<em>r</em><em>a</em><em>t</em><em>e</em>]</span></li>
            </ul>
            <p><strong>Nonlinearity Challenges:</strong></p>
            <ol type="1">
            <li>coordinate conversion (polar to Cartesian)</li>
            <li>rotational motion during turns</li>
            <li>range-dependent measurement noise</li>
            </ol>
            <p><strong>Testing Methodology:</strong></p>
            <ul>
            <li>100 Monte Carlo simulations of 120-second
            trajectories</li>
            <li>Process noise: Medium (acceleration uncertainty σ = 0.5
            m/s²)</li>
            <li>Measurement noise: Range (σ = 25m), Azimuth (σ = 0.5°),
            Range-rate (σ = 3m/s)</li>
            </ul>
            <p>Comparison of EKF vs UKF error performance:</p>
            <p><img src="img/fusion/ekf/ukf-radar-diagram.svg" width="300"></p>
            <p><br></p>
            <table>
            <thead>
            <tr>
            <th>Parameter</th>
            <th>EKF Error</th>
            <th>UKF Error</th>
            <th>Improvement</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>Position</td>
            <td>1.45 m</td>
            <td>0.95 m</td>
            <td>34.5%</td>
            </tr>
            <tr>
            <td>Velocity</td>
            <td>0.32 m/s</td>
            <td>0.25 m/s</td>
            <td>21.9%</td>
            </tr>
            <tr>
            <td>Heading</td>
            <td>2.1 deg</td>
            <td>1.7 deg</td>
            <td>19.0%</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Analysis:</strong></p>
            <ul>
            <li>UKF <strong>outperforms</strong> the EKF in all state
            variables</li>
            <li>most significant improvement is in <strong>position
            estimation</strong> (34.5%)</li>
            <li>improvement becomes more pronounced during
            <strong>high-rate turns</strong></li>
            <li><strong>computational load increased</strong> by
            approximately <span class="math inline">15%</span> for the
            UKF</li>
            </ul>
            <p><strong>When to Use UKF Instead of EKF:</strong></p>
            <ul>
            <li>dealing with highly nonlinear system or measurement
            models</li>
            <li>high accuracy is needed during rapid maneuvers</li>
            <li>Jacobian matrices are difficult to derive or
            implement</li>
            <li>the additional computational cost is acceptable</li>
            </ul>
            <p>The UKF’s ability to better capture nonlinear
            transformations makes it particularly valuable for
            aerospace, underwater navigation and other domains with
            complex motion models.</p>
            <p><strong>References</strong>:</p>
            <ul>
            <li>Thrun, S., Burgard, W., &amp; Fox, D. (2005). <a
            href="http://www.probabilistic-robotics.org">Probabilistic
            Robotics</a>. MIT Press. <a
            href="http://probabilistic-robotics.informatik.uni-freiburg.de/ppt/">Slides</a></li>
            <li>Bar-Shalom, Y., Li, X.R., &amp; Kirubarajan, T. (2001).
            Estimation with Applications to Tracking and Navigation.
            Wiley-Interscience.</li>
            <li>Grewal, M.S., &amp; Andrews, A.P. (2014). Kalman
            Filtering: Theory and Practice Using MATLAB (4th ed.).
            Wiley.</li>
            <li>Simon, D. (2006). Optimal State Estimation: Kalman, H∞,
            and Nonlinear Approaches. Wiley-Interscience.</li>
            <li>Crassidis, J.L., &amp; Junkins, J.L. (2011). Optimal
            Estimation of Dynamic Systems (2nd ed.). CRC Press.</li>
            <li><a
            href="https://soulhackerslabs.com/sensor-fusion-with-the-extended-kalman-filter-in-ros-2-d33dbab1829d">Sensor
            Fusion with the Extended Kalman Filter in ROS 2</a></li>
            <li><a
            href="https://soulhackerslabs.com/the-unreasonable-power-of-the-unscented-kalman-filter-with-ros-2-d4c97d4b4bb9">The
            Unreasonable Power of The Unscented Kalman Filter with ROS
            2</a></li>
            <li><a
            href="https://medium.com/@sasha_przybylski/the-math-behind-extended-kalman-filtering-0df981a87453">The
            math behind Extended Kalman Filtering</a> by Sasha
            Przybylski. Check out the cool video:</li>
            </ul>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/dFPCFmd5uJE?si=Iy8K0EeV6TP3amor" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
            </iframe>
            </section>
            </section>
            <section id="high-order-sensor-fusions" class="level2"
            data-number="9.2">
            <h2 data-number="9.2"><span
            class="header-section-number">9.2</span> High-Order Sensor
            Fusions</h2>
            <p>What we’ve seen so far is <strong>low-level</strong>
            sensor fusion where the sensor values are fused as soon as
            the sensor readings are available. But we can imagine a
            situation where,</p>
            <ul>
            <li>first, extract some “knowledge” out of the sensor values
            → say, bounding boxes</li>
            <li>next, “fuse” this higher-oder knowledge.</li>
            </ul>
            <p>Consider the following two scenarios of a car trying to
            detect a pedestrian crossing the road using two different
            sensors → a <strong>camera</strong> and a
            <strong>lidar</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 47%" />
            <col style="width: 52%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">camera</th>
            <th style="text-align: left;">lidar</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td
            style="text-align: left;"><img src="img/fusion/higher_fusion/fusion_camera.png" width="300"></td>
            <td
            style="text-align: left;"><img src="img/fusion/higher_fusion/fusion_lidar.jpeg" width="300"></td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>2D</strong> bounding
            box</td>
            <td style="text-align: left;"><strong>3D</strong> bounding
            box</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>The problem is that they generate very different types of
            output (2D vs 3d bonding boxes). So how do we fuse them?</p>
            <section id="sensor-fusion-classes" class="level3"
            data-number="9.2.1">
            <h3 data-number="9.2.1"><span
            class="header-section-number">9.2.1</span> Sensor Fusion |
            Classes</h3>
            <p>Before we go ahead, let’s take a look at the various
            <strong>classes of sensor fusion</strong>. The
            classification tries to extract answers to the following
            questions:</p>
            <ol type="1">
            <li><strong>abstraction</strong> level →
            <strong>when</strong> should we carry out the fusion?</li>
            <li><strong>centralization</strong> level →
            <strong>where</strong> should we carry out the fusion?</li>
            <li><strong>competition</strong> level →
            <strong>what</strong> should the fusion do?</li>
            </ol>
            <p>Let’s briefly look at each one.</p>
            <p><strong>1. Abstraction level fusion</strong></p>
            <p>We try to answer the question → <strong>when</strong>
            should we carry out the fusion, <em>i.e.,</em> should we do
            it as soon as we get sensor values or after extracting some
            knowledge from it?</p>
            <p>There are three ways to classify sensor fusion based on
            the abstraction levels:</p>
            <table>
            <colgroup>
            <col style="width: 35%" />
            <col style="width: 18%" />
            <col style="width: 20%" />
            <col style="width: 12%" />
            <col style="width: 12%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">abstraction level</th>
            <th style="text-align: left;">details</th>
            <th style="text-align: left;">examples</th>
            <th style="text-align: left;">pros</th>
            <th style="text-align: left;">cons</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td
            style="text-align: left;"><strong>low-level</strong></td>
            <td style="text-align: left;">fuse the <strong>raw</strong>
            sensor values → project 3D point clouds onto pixels</td>
            <td style="text-align: left;">point clouds from LiDAR and
            pixels from camera images</td>
            <td style="text-align: left;">future proof</td>
            <td style="text-align: left;">large computational
            requirements</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>mid-level</strong></td>
            <td style="text-align: left;">each sensor does its own
            detection → project 3D bounding box onto 2D bounding
            box</td>
            <td style="text-align: left;">camera and LiDAR (separately)
            detect objects that are fused</td>
            <td style="text-align: left;">simplicity</td>
            <td style="text-align: left;">loses some information</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>high-level</strong></td>
            <td style="text-align: left;">fuse <strong>objects</strong>
            and <strong>trajectories</strong> →
            predictions+tracking</td>
            <td style="text-align: left;">trajectories from camera and
            from LiDAR (separately)</td>
            <td style="text-align: left;">further simplicity</td>
            <td style="text-align: left;">too much information loss</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/fusion/higher_fusion/abstraction_levels.png" height="300"></p>
            <p><strong>2. centralization level</strong></p>
            <p>We try to answer the question → <strong>where</strong>
            should we carry out the fusion, <em>i.e.,</em></p>
            <ul>
            <li>central computer or</li>
            <li>each sensor does it independently</li>
            </ul>
            <p>There are three high-level choices:</p>
            <table>
            <colgroup>
            <col style="width: 74%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;"><strong>centralization
            level</strong></th>
            <th style="text-align: left;">details</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td
            style="text-align: left;"><strong>centralized</strong></td>
            <td style="text-align: left;">one central unit deals with it
            [low-level]</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>decentralized</strong></td>
            <td style="text-align: left;">each sensor fuses data and
            forwards to next one</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>distributed</strong></td>
            <td style="text-align: left;">each sensor processes data
            locally and sends to next unit [late]</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/fusion/higher_fusion/centralization_levels.png" width="400"></p>
            <p>Recently automakers have started to use a
            <strong>satellite</strong> architecture for the design of
            computing elements interacting with sensors (and sensor
            fusion). The process is:</p>
            <ul>
            <li>plug many sensors [satellites]</li>
            <li>fuse together either on a single central unit [active
            safety domain controller] or a bunch of separate
            processors</li>
            <li>360 degree fusion+detection on controller</li>
            <li>sensors do not have to be extremely good → there is
            redundancy to make up for it</li>
            </ul>
            <p><img src="img/fusion/higher_fusion/satellite_architecture.png" width="400"></p>
            <p><strong>3. Competition level</strong></p>
            <p>We try to answer <strong>what</strong> should the fusion
            do? Again, there are <strong>three</strong>
            possibilities,</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 25%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">competition level</th>
            <th style="text-align: left;">meaning</th>
            <th style="text-align: left;">example</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td
            style="text-align: left;"><strong>competitive</strong></td>
            <td style="text-align: left;">sensors meant for same
            purpose</td>
            <td style="text-align: left;">RADAR, LiDAR</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>complementary</strong></td>
            <td style="text-align: left;">different sensors looking at
            different scenes</td>
            <td style="text-align: left;">multiple cameras → to create a
            panorama</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>coordinated</strong></td>
            <td style="text-align: left;">sensors produce a new scene
            from same object</td>
            <td style="text-align: left;">3D reconstruction</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>Examples of each of the above:</p>
            <ol type="1">
            <li>competitive level</li>
            </ol>
            <p><img src="img/fusion/higher_fusion/3d_iou_matching.png" width="400"></p>
            <ol start="2" type="1">
            <li>complementary level</li>
            </ol>
            <p><img src="img/fusion/higher_fusion/panorama.png" width="400"></p>
            <ol start="3" type="1">
            <li>coordinated level</li>
            </ol>
            <p><img src="img/fusion/higher_fusion/3d_reconstruction.png" width="400"></p>
            </section>
            <section id="what-is-iou" class="level3"
            data-number="9.2.2">
            <h3 data-number="9.2.2"><span
            class="header-section-number">9.2.2</span> What is
            “IOU”?</h3>
            <p>Recall that YOLO creates “bounding boxes”. But, it
            usually starts by creating <strong>multiple</strong>
            bounding boxes on/around an object, <em>e.g.,</em></p>
            <p>So, one example could be,</p>
            <p><img src="img/fusion/higher_fusion/yolo_multiple_bounding_boxes.png" width="300"></p>
            <p><br></p>
            <p>But what we really want is a <strong>single</strong>
            bounding box that encapsulates the <strong>entire</strong>
            object,</p>
            <p><img src="img/fusion/higher_fusion/yolo_single_bounding_box.png" width="300"></p>
            <p>So how do we get from one (multiple) to the other
            (single)? We compute the <strong>IoU</strong> →
            <strong>intersections over union</strong>, defined as:</p>
            <p><span class="math display">$$
            IoU = \frac{\text{area of \textbf{intersection} of bounding
            boxes}}{\text{area of \textbf{union} of bounding boxes}}
            $$</span></p>
            <p>Visually, this can be seen as:</p>
            <p><img src="img/fusion/higher_fusion/iou_visual.png" height="150"></p>
            <p><br></p>
            <p>The closer that an IoU is to <code>1</code>, the better
            the bounding box, <em>e.g.,</em></p>
            <p><img src="img/fusion/higher_fusion/iou_examples.png" width="400"></p>
            <p><br></p>
            <p>IoU also requires a notion of <strong>ground
            truth</strong> → the <strong>precise</strong> bounding boxes
            surrounding the items of interest in an image. <strong>Human
            experts manually mark or label these boundary
            boxes</strong>. The IoU score is calculated by comparing the
            predicted bounding boxes produced by an object detection
            model to the ground-truth bounding boxes during
            evaluation.</p>
            </section>
            <section id="sensor-fusion-example-camera-and-lidar"
            class="level3" data-number="9.2.3">
            <h3 data-number="9.2.3"><span
            class="header-section-number">9.2.3</span> Sensor Fusion
            Example | Camera and LiDAR</h3>
            <p>Let’s get back to our example, camera+LiDAR. So, we have
            the following types of data:</p>
            <p><img src="img/fusion/higher_fusion/yolo_single_bounding_box.png" height="200">
            <img src="img/fusion/higher_fusion/car_lidar.jpeg" height="200"></p>
            <ul>
            <li>camera → excellent for <strong>object
            classification</strong> and <strong>understanding
            scenes</strong></li>
            <li>LiDAR →good for <strong>estimating
            distances</strong></li>
            </ul>
            <p>Here is a comparison of the strengths and weaknesses of
            the two:</p>
            <p><img src="img/fusion/higher_fusion/camera_vs_lidar.png" width="400"></p>
            <p><br></p>
            <p>So let’s look at this from the perspective of the three
            classes we discussed earlier:</p>
            <table>
            <thead>
            <tr>
            <th style="text-align: left;">class</th>
            <th style="text-align: left;">notes</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><strong>what</strong>?</td>
            <td style="text-align: left;">competition and
            redundancy</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>where</strong>?</td>
            <td style="text-align: left;">doesn’t matter → any of the
            options work</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>when</strong>?</td>
            <td style="text-align: left;">multiple options →
            <strong>early</strong> or <strong>late</strong></td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><strong>Early Fusion</strong></p>
            <ul>
            <li>fuse <strong>raw data</strong> as soon as sensors are
            plugged</li>
            <li><strong>project</strong> 3D LiDAR point clouds onto 2D
            image</li>
            <li>check whether point clouds belong to 2D bounding boxes
            from camera</li>
            </ul>
            <p>The entire process looks like:</p>
            <p><img src="img/fusion/higher_fusion/camera_lidar_early.final.png" width="400"></p>
            <p><br></p>
            <p>to translate 3D point cloud [LiDAR frame] → 2D projection
            [camera frame],</p>
            <ul>
            <li>convert each 3D LiDAR point into <strong><a
            href="https://www.tomdalling.com/blog/modern-opengl/explaining-homogenous-coordinates-and-projective-geometry/">homogeneous
            coordinates</a></strong></li>
            <li>apply <strong>projection equations</strong>
            [translation/rotation] to convert from LiDAR to camera</li>
            <li>transform back into Euclidean coordinates</li>
            </ul>
            <details>
            <summary>
            Homogenous coordinates
            </summary>
            <p>They are a <a
            href="https://en.wikipedia.org/wiki/Homogeneous_coordinates">system
            of coordinates</a> used in projective geometry, just as
            Cartesian coordinates are used in Euclidean geometry. They
            have the advantage that the coordinates of points, including
            points at infinity, can be represented using finite
            coordinates. Formulas involving homogeneous coordinates are
            often simpler and more symmetric than their Cartesian
            counterparts. Homogeneous coordinates have a range of
            applications, including computer graphics and 3D computer
            vision, where they allow affine transformations and, in
            general, projective transformations to be easily represented
            by a matrix. They are also used in fundamental elliptic
            curve cryptography algorithms.</p>
            <p>Here is an example of a Rational Bézier curve –
            polynomial curve defined in homogeneous coordinates (blue)
            and its projection on plane – rational curve (red).</p>
            <p><img src="img/fusion/higher_fusion/homogenous.png" width="300"></p>
            <p><br></p>
            <p>If homogeneous coordinates of a point are multiplied by a
            non-zero scalar then the <strong>resulting coordinates
            represent the same point</strong>. Since homogeneous
            coordinates are also given to points at infinity, the
            <strong>number of coordinates</strong> required to allow
            this extension is <strong>one more than the dimension of the
            projective space</strong> being considered. For example, two
            homogeneous coordinates are required to specify a point on
            the projective line and three homogeneous coordinates are
            required to specify a point in the projective plane.</p>
            <p><a
            href="https://www.tomdalling.com/blog/modern-opengl/explaining-homogenous-coordinates-and-projective-geometry/">Read</a>
            for a more detailed explanation with examples.</p>
            </details>
            <p>So, the output of this step looks like,</p>
            <p><img src="img/fusion/higher_fusion/3d_to_2d.png" width="400"></p>
            <p><br></p>
            <p>For the object detection on the camera image → use
            YOLO!</p>
            <p>The next step is → <strong>region of interest (ROI)
            mapping</strong>, where we must <strong>fuse</strong> the
            objects inside each bounding box. We get the following
            outputs:</p>
            <ul>
            <li>for each bounding box → camera gives us the
            <strong>classification</strong></li>
            <li>for each <strong>LiDAR projected point</strong> → we get
            an <strong>accurate distance measure</strong></li>
            </ul>
            <p>There can be some problems though. Which point (in the
            bounding box) do we pick for the <strong>distance</strong>?
            Recall that we are projecting/capture a 3D object on a 2D
            plane. So, distance measures could be one of:</p>
            <ul>
            <li>average</li>
            <li>center point</li>
            <li>closest?</li>
            </ul>
            <p>There are other issues, for instance, consider the yellow
            region pointed to by the arrow,</p>
            <p><img src="img/fusion/higher_fusion/roi_matching.png" width="300"></p>
            <p><br></p>
            <p>Do these points belong to the (shown) bounding box or a
            different one?</p>
            <p>We need to develop some <em>consistent</em> policies to
            deal with such issues.</p>
            <p><strong>Late Fusion</strong></p>
            <p>Fusing results <strong>after</strong> independent
            detection and there can be two ways to do it:</p>
            <ul>
            <li>get 3D bounding boxes on both ends → fuse results</li>
            <li>get 2D bounding boxes on both sides → fuse results</li>
            </ul>
            <p><img src="img/fusion/higher_fusion/late_fusion_example.png" height="200"></p>
            <p>For the first type (late fusion in 3D), the steps
            are:</p>
            <ol type="1">
            <li><strong>3D obstacle detection for LiDAR</strong>. The
            idea is to look at the point cloud and extract meaningful
            information/bounding boxes for the objects. Various methods
            have been developed over the years for this,
            <ul>
            <li>unsupervised machine learning</li>
            <li>deep learning algorithms (<em>e.g.,</em> <a
            href="https://github.com/QingyongHu/RandLA-Net?tab=readme-ov-file">RANDLA-NET</a>)</li>
            </ul></li>
            </ol>
            <p><br></p>
            <p><img src="img/fusion/higher_fusion/3d_obstacle_detection.png" width="400"></p>
            <p><br></p>
            <details>
            <summary>
            RANDLA-NET
            </summary>
            <p>This is a simple and efficient neural architecture for
            semantic segmentation of large-scale 3D point clouds.</p>
            <ul>
            <li>link to the <a
            href="https://arxiv.org/abs/1911.11236">paper</a></li>
            <li>link to <a
            href="https://github.com/QingyongHu/RandLA-Net?tab=readme-ov-file">gitHub
            repo</a></li>
            <li>the <a
            href="https://www.mathworks.com/help/lidar/ref/randlanet.html">Matlab</a>
            implementation</li>
            </ul>
            <p>Some examples:</p>
            <p><img src="img/fusion/higher_fusion/randla_net.1.gif" width="400">
            <img src="img/fusion/higher_fusion/randla_net.2.gif" width="400"></p>
            </details>
            <p><br></p>
            <ol start="2" type="1">
            <li><strong>3D Obstacle detection for Camera</strong>. We
            want to take the 2D images from the camera and
            <strong>extrapolate 3D</strong> bounding boxes from them.
            This is much harder than the previous situation. Here are
            the steps:</li>
            </ol>
            <p><img src="img/fusion/higher_fusion/2d_to_3d_extrpolation.png" width="400"></p>
            <p><br></p>
            <ul>
            <li>2D object detection (say using YOLO)</li>
            <li>3D projections from those bounding boxes → essentially
            extract the size/shape/depth of the object:</li>
            </ul>
            <p><img src="img/fusion/higher_fusion/2d_to_3d_car.png" width="300"></p>
            <p><br></p>
            <p>One method is to use deep learning along with estimates
            of the size/orientation of the vehicle.</p>
            <p><br></p>
            <ol start="3" type="1">
            <li><strong>IoU matching in 3D space</strong> → find a way
            to align the 3D bounding boxes in space.</li>
            </ol>
            <p><img src="img/fusion/higher_fusion/3d_iou_matching.png" width="500"></p>
            <p><br></p>
            <p>Numerous methods have been proposed in literature → <a
            href="https://github.com/facebookresearch/votenet">votenet</a>,
            <a
            href="https://github.com/Na-Z/sess?tab=readme-ov-file">sess</a>
            and <a
            href="https://github.com/yezhen17/3DIoUMatch?tab=readme-ov-file">3dIoUmatch</a>.</p>
            <p>Here is a high-level diagram that shows how SESS
            works:</p>
            <p><img src="img/fusion/higher_fusion/sess_3d_matching.jpg" width="400"></p>
            <p><strong>IoU matching in Time</strong></p>
            <p>So far, we have discussed how to match the IoU in
            <strong>space</strong> but there is a <strong>time</strong>
            element as well → we need ensure that the <strong>frames
            match up in time</strong>! We need to evaluate the accuracy
            of object bounding box predictions across consecutive frames
            → basically associate objects in time, from frame to frame,
            <em>i.e.,</em> associate temporal segments (events,
            intervals, actions) based on their <strong>Intersection over
            Union (IoU)</strong></p>
            <p>We also need to <strong>predict future
            positions</strong>!</p>
            <p>On intuition is that if <strong>bounding boxes overlap
            between consecutive frames</strong> → same obstacle.</p>
            <p>Here are some well known algorithms:</p>
            <ol type="1">
            <li><strong>Greedy Matching</strong></li>
            </ol>
            <p>Greedy matching is a <strong>heuristic</strong> approach
            where each predicted segment is matched to the best ground
            truth interval based on IoU. Common in benchmarks like <a
            href="https://github.com/activitynet/ActivityNet/tree/master"><strong>ActivityNet</strong></a>
            and <a
            href="https://cove.thecvf.com/datasets/593"><strong>THUMOS</strong></a>.</p>
            <p>How it works:</p>
            <ul>
            <li>sort predictions by <strong>confidence
            score</strong></li>
            <li>for each prediction,
            <ul>
            <li>match with the <strong>highest-IoU unmatched</strong>
            ground truth</li>
            <li>if <span
            class="math inline"><em>I</em><em>o</em><em>U</em> ≥ <em>t</em><em>h</em><em>r</em><em>e</em><em>s</em><em>h</em><em>o</em><em>l</em><em>d</em></span>
            (<em>e.g.,</em> <code>0.5</code>)* → *true positive**</li>
            </ul></li>
            <li>unmatched predictions → <strong>false
            positives</strong></li>
            <li>unmatched ground truths → <strong>false
            negatives</strong></li>
            </ul>
            <p><br> <br></p>
            <ol start="2" type="1">
            <li><strong>Hungarian Matching algorithm</strong> (Optimal
            One-to-One)</li>
            </ol>
            <p>Uses the <strong><a
            href="https://en.wikipedia.org/wiki/Hungarian_algorithm">Hungarian
            algorithm</a></strong> to find an optimal one-to-one
            matching between predictions and ground truths by maximizing
            total IoU.</p>
            <p>Methodology:</p>
            <ul>
            <li>build a <strong>cost matrix</strong>, <span
            class="math inline">1 − <em>I</em><em>o</em><em>U</em></span>,
            for each prediction-ground truth pair</li>
            <li>solve <strong>assignment</strong> using the Hungarian
            algorithm</li>
            </ul>
            <p><br> <br></p>
            <ol start="3" type="1">
            <li><strong>IoU Threshold Matching</strong></li>
            </ol>
            <p>Matches predictions to ground truths if <span
            class="math inline"><em>I</em><em>o</em><em>U</em> ≥ <em>t</em><em>h</em><em>r</em><em>e</em><em>s</em><em>h</em><em>o</em><em>l</em><em>d</em></span>.
            Can be,</p>
            <ul>
            <li>one-to-one</li>
            <li>one-to-many</li>
            <li>many-to-many.</li>
            </ul>
            <p>How it works,</p>
            <ul>
            <li>compute the IoU for <strong>all pairs</strong></li>
            <li>match if the IoU exceeds threshold</li>
            <li>we can create customizable matching policies</li>
            </ul>
            <p>Some frameworks that use this: <a
            href="http://host.robots.ox.ac.uk/pascal/VOC/">PASCAL
            VOC</a>.</p>
            <p><br> <br></p>
            <ol start="4" type="1">
            <li><strong>Temporal Non-Maximum Suppression
            (NMS)</strong></li>
            </ol>
            <p>Removes redundant overlapping predictions and keeps the
            <strong>highest confidence prediction per overlapping
            group</strong>,</p>
            <ul>
            <li>sort the predictions by <strong>confidence</strong></li>
            <li>iteratively <strong>remove overlapping
            predictions</strong> (<span
            class="math inline"><em>I</em><em>o</em><em>U</em> &gt; <em>t</em><em>h</em><em>r</em><em>e</em><em>s</em><em>h</em><em>o</em><em>l</em><em>d</em></span>).</li>
            </ul>
            <p>Frameworks that use this method: <a
            href="https://arxiv.org/abs/1504.08083">Fast R-CNN</a>.</p>
            <p><br> <br></p>
            <ol start="5" type="1">
            <li><strong>Soft-NMS / Soft Matching</strong></li>
            </ol>
            <p>Instead of removing overlapping segments, Soft-NMS
            <strong>decreases their scores</strong> based on IoU overlap
            using a <strong>decay function</strong>. Basically apply a
            decay to confidence scores such as,</p>
            <p><span class="math display">$$
            s = s * e^{\frac{-IoU^2}{\sigma}}
            $$</span></p>
            <p>Framework: <a
            href="https://arxiv.org/abs/1704.04503">Soft-NMS</a>.</p>
            <p>Here is a comparison between the various methods:</p>
            <table>
            <thead>
            <tr>
            <th style="text-align: left;">Method</th>
            <th style="text-align: left;">Matching Type</th>
            <th style="text-align: left;">Optimized?</th>
            <th style="text-align: left;">Handles Overlaps?</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;">Greedy</td>
            <td style="text-align: left;">One-to-one</td>
            <td style="text-align: left;">No</td>
            <td style="text-align: left;">Partially</td>
            </tr>
            <tr>
            <td style="text-align: left;">Hungarian</td>
            <td style="text-align: left;">One-to-one</td>
            <td style="text-align: left;">Yes</td>
            <td style="text-align: left;">Yes</td>
            </tr>
            <tr>
            <td style="text-align: left;">IoU Threshold</td>
            <td style="text-align: left;">Flexible</td>
            <td style="text-align: left;">No</td>
            <td style="text-align: left;">No</td>
            </tr>
            <tr>
            <td style="text-align: left;">NMS</td>
            <td style="text-align: left;">Pre/Post</td>
            <td style="text-align: left;">No</td>
            <td style="text-align: left;">Yes</td>
            </tr>
            <tr>
            <td style="text-align: left;">Soft-NMS</td>
            <td style="text-align: left;">Pre/Post</td>
            <td style="text-align: left;">No (heuristic)</td>
            <td style="text-align: left;">Yes</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><strong>References</strong></p>
            <ul>
            <li><a
            href="https://medium.com/@nikhilnair8490/3d-object-tracking-using-lidar-and-camera-187040062b2c">3D
            Object Tracking using LiDAR and CAMERA</a></li>
            <li><a
            href="https://www.mdpi.com/2072-4292/17/3/357">Enhancing
            Cross-Modal Camera Image and LiDAR Data Registration Using
            Feature-Based Matching</a> by Leahy et al.</li>
            <li><a
            href="https://www.thinkautonomous.ai/blog/lidar-and-camera-sensor-fusion-in-self-driving-cars/">LiDAR
            and Camera Sensor Fusion in Self-Driving Cars</a></li>
            </ul>
            <!--rel="stylesheet" href="./custom.sibin.css"-->
            </section>
            </section>
            </section>
            <section id="slam-1" class="level1" data-number="10">
            <h1 data-number="10"><span
            class="header-section-number">10</span> SLAM</h1>
            <p>Consider a robot (a Roomba vacuum cleaner, say) trying to
            navigate inside a room. Now, it can do this in one of two
            ways:</p>
            <ul>
            <li><strong>randomly</strong> move around the room, hoping
            that it will <em>eventually</em> cover the whole place
            and</li>
            <li><strong>map</strong> out the room, figure out where it
            is and <em>systematically</em> ensure that it cover the
            entire room.</li>
            </ul>
            <p><img src="img/slam/roomba_random.png" width="200">   
            <img src="img/slam/roomba_slam.png" width="200"></p>
            <p><br></p>
            <p>This is particulalry effective when the system has
            <strong>obstacles</strong> in it, <em>e.g.,</em> a couch and
            other furniture. This requires the robot to do do
            things:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <tbody>
            <tr>
            <td>create a <strong>map</strong> of its surroundings</td>
            <td><strong>locate</strong> itself within that map</td>
            </tr>
            <tr>
            <td><img src="img/slam/mapping.png" width="100"></td>
            <td><img src="img/slam/truck.png" width="100"></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Enter <strong>simultaneous localization and
            mapping</strong> (SLAM) which represents one of the
            fundamental challenges in mobile robotics. As the name
            suggests, SLAM involves solving two interconnected problems
            at once: building a map of an unknown environment while
            simultaneously tracking the robot’s position within that
            environment. The need for SLAM arises from a seemingly
            paradoxical situation: to build an accurate map, a robot
            needs to know its precise location, but to determine its
            precise location, the robot needs an accurate map. This
            chicken-and-egg problem is what makes SLAM challenging and
            fascinating.</p>
            <p>SLAM is also crucial for autonomous mobile robots
            operating in <strong>unknown environments</strong> where
            external positioning systems (like GPS) are unavailable or
            unreliable, such as:</p>
            <ul>
            <li>indoor environments</li>
            <li>underground mines and caves</li>
            <li>underwater exploration</li>
            <li>planetary exploration</li>
            <li>dense urban areas with GPS signal blockage.</li>
            </ul>
            <p>SLAM is more of a concept than a single algorithm. There
            are many approaches to implementing SLAM, varying in
            complexity, accuracy and computational requirements.</p>
            <p>Some examples of SLAM output:</p>
            <ol type="1">
            <li><strong>grid maps</strong> or
            <strong>scans</strong></li>
            </ol>
            <p><img src="img/slam/grid_map.1.png" width="200">
            <img src="img/slam/grid_map.2.jpg" width="200">
            <img src="img/slam/grid_map.3.png" width="200"></p>
            <ol start="2" type="1">
            <li><strong>landmark</strong>-based</li>
            </ol>
            <p><img src="img/slam/landmark.1.png" width="200">
            <img src="img/slam/landmark.2.png" width="200">
            <img src="img/slam/landmark.3.png" width="200"></p>
            <p>We can also classify SLAM based on the type of sensor
            (and corresponding outputs),</p>
            <ol type="a">
            <li><strong>visual slam</strong> that uses → cameras</li>
            </ol>
            <ul>
            <li>sparse methods match feature points of images [PTAM,
            ORB_SLAM]</li>
            <li>dense methods use overall brightness of images [DTAM,
            LSD- SLAM, DSO, SVO]</li>
            </ul>
            <p><img src="img/slam/visual_slam.1.png" width="300">
            <img src="img/slam/visual_slam.2.png" width="300"></p>
            <ol start="2" type="a">
            <li><strong>LiDAR SLAM</strong></li>
            </ol>
            <ul>
            <li>laser point cloud provides high-precision distance
            measurements</li>
            <li>not as finely detailed as camera images</li>
            <li>environments with fewer obstacles → less precision</li>
            <li>may require fusion with other sensors (e.g., GPS,
            odometry)</li>
            </ul>
            <p><img src="img/slam/lidar_slam.1.png" width="300">
            <img src="img/slam/lidar_slam.2.png" width="300"></p>
            <section id="slam-problem-definition" class="level2"
            data-number="10.1">
            <h2 data-number="10.1"><span
            class="header-section-number">10.1</span> SLAM Problem
            Definition</h2>
            <p>At its core, SLAM is a state estimation problem.
            Given,</p>
            <ul>
            <li>the robot’s control inputs (odometry)</li>
            <li>observations of nearby features/landmarks</li>
            <li>no prior map</li>
            </ul>
            <p>The objective is to estimate,</p>
            <ul>
            <li>the <strong>map</strong> of the environment</li>
            <li>the <strong>path</strong> (or current position) of the
            robot</li>
            </ul>
            <p>So why is SLAM a hard problem?</p>
            <p>In the real world, the mapping between observations and
            landmarks is unknown. Picking wrong data associations can
            have catastrophic consequences. Pose error correlates data
            associations.</p>
            <blockquote>
            <p>In robotics and computer vision, <strong>“pose”</strong>
            refers to the position and orientation of an object or a
            robot in a given space.</p>
            <p>For instance, 2D Pose (in planar systems)</p>
            <ul>
            <li>position: (x, y)</li>
            <li>orientation: <span class="math inline"><em>θ</em></span>
            (rotation around the z-axis)</li>
            </ul>
            </blockquote>
            <p>The various parts of SLAM:</p>
            <p><img src="img/slam/slam_workflow.4.png" width="400"></p>
            <p><br></p>
            <p>Let’s go over each component:</p>
            <ol type="1">
            <li><strong>Sensor Data</strong>
            <ul>
            <li>the input to SLAM comes from sensors, such as:
            <ul>
            <li>camera (for visual SLAM, extracting features from
            images)</li>
            <li>a LiDAR or depth sensor (for distance and obstacle
            detection)</li>
            </ul></li>
            <li>the sensors provide raw data about the environment.</li>
            </ul></li>
            <li><strong>Front End</strong> (sensor-dependent processing)
            <ul>
            <li>front end is responsible for data association and motion
            estimation</li>
            <li>it processes sensor-specific information to extract
            meaningful features. It includes:
            <ul>
            <li>motion estimation: Determines how the robot is moving by
            tracking sensor data over time</li>
            <li>obstacle location estimation: Identifies objects and
            landmarks in the environment</li>
            </ul></li>
            <li>goal of the front end → preprocess sensor data and
            reduce noise before passing it to the back end.</li>
            </ul></li>
            <li><strong>Back End</strong> (sensor-independent
            processing)
            <ul>
            <li>back end performs global optimization of the estimated
            trajectory</li>
            <li>it is <strong>sensor-independent</strong>, meaning it
            can work with data from different sensors as long as it
            receives processed pose and landmark information. It
            includes:
            <ul>
            <li><strong>register pose graphs</strong> → combines
            different pose estimates into a coherent structure</li>
            <li>graph optimization → uses mathematical optimization
            techniques (e.g., bundle adjustment, factor graphs) to
            refine the trajectory and correct drift</li>
            </ul></li>
            <li>back end <strong>corrects errors</strong> that
            accumulate over time, leading to a more accurate map.</li>
            </ul></li>
            <li><strong>Pose Graph and Map Information</strong>
            <ul>
            <li><strong>final output</strong> of SLAM is a pose graph (a
            representation of the robot’s trajectory) and a map of the
            environment</li>
            <li>map is constructed using optimized pose estimates from
            the back end</li>
            <li>map can be used for navigation, obstacle avoidance, and
            autonomous decision-making.</li>
            </ul></li>
            </ol>
            <p>Hence,</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>component</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>front end (sensor-dependent)</td>
            <td>processes raw sensor data (motion estimation, obstacle
            detection)</td>
            </tr>
            <tr>
            <td>back end (sensor-independent)</td>
            <td>optimizes the trajectory and map</td>
            </tr>
            <tr>
            <td>final output</td>
            <td>a refined pose graph and map for accurate
            navigation</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>This structured approach ensures real-time and accurate
            localization while reducing computational complexity.</p>
            <p>We can distill all of the above “steps” into the
            following. Each has its own drawbacks but we can deal with
            them.</p>
            <table>
            <colgroup>
            <col style="width: 30%" />
            <col style="width: 34%" />
            <col style="width: 34%" />
            </colgroup>
            <thead>
            <tr>
            <th>step</th>
            <th>description</th>
            <th>challenge</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>prediction step</strong></td>
            <td>update the robot’s position estimate using odometry data
            (motion model)</td>
            <td><strong>correlated errors</strong> in position
            estimation affect the map</td>
            </tr>
            <tr>
            <td><strong>landmark extraction</strong></td>
            <td>identify landmarks or features from sensor
            measurements</td>
            <td><strong>environmental dynamics</strong> may cause
            landmarks to change</td>
            </tr>
            <tr>
            <td><strong>data association</strong></td>
            <td>match observed landmarks with previously mapped
            landmarks</td>
            <td><strong>incorrect associations</strong> lead to mapping
            errors</td>
            </tr>
            <tr>
            <td><strong>update step</strong></td>
            <td>update robot position and landmark estimates based on
            observations</td>
            <td><strong>computational complexity</strong> increases with
            more data</td>
            </tr>
            <tr>
            <td><strong>map expansion</strong></td>
            <td>add newly observed landmarks to the map</td>
            <td>loop closure is required to recognize
            <strong>revisited</strong> places</td>
            </tr>
            </tbody>
            </table>
            </section>
            <section id="mathematical-formulations" class="level2"
            data-number="10.2">
            <h2 data-number="10.2"><span
            class="header-section-number">10.2</span> Mathematical
            Formulations</h2>
            <p>First, let’s define some terms,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>x</em><sub>1 : <em>t</em></sub></span></td>
            <td>robot path</td>
            </tr>
            <tr>
            <td><span class="math inline"><em>m</em></span></td>
            <td>map</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>z</em><sub>1 : <em>t</em></sub></span></td>
            <td>sensor observations</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>u</em><sub>1 : <em>t</em></sub></span></td>
            <td>control inputs</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <ol type="1">
            <li><strong>Full SLAM</strong> → estimates the
            <strong>entire</strong> robot path and map</li>
            </ol>
            <p><img src="img/slam/full_slam_graphical.png" width="300"></p>
            <ul>
            <li>the model estimates the <strong>entire
            trajectory</strong> <span
            class="math inline"><em>x</em><sub>1 : <em>t</em></sub></span>
            along with the map <span
            class="math inline"><em>m</em></span></li>
            <li>the equation:</li>
            </ul>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub>1 : <em>t</em></sub>, <em>m</em>|<em>z</em><sub>1 : <em>t</em></sub>, <em>u</em><sub>1 : <em>t</em></sub>)</span></p>
            <ul>
            <li>indicates that we compute the joint probability over all
            past and current poses, rather than marginalizing them
            out</li>
            <li>gray-shaded area (from the figure) covers all past poses
            <span
            class="math inline"><em>x</em><sub>1 : <em>t</em></sub></span>,
            indicating that they are explicitly maintained in the
            estimation</li>
            <li>this approach provides <strong>smoothing</strong>,
            allowing the system to refine past estimates when new
            observations are received</li>
            </ul>
            <p><strong>Interpretation</strong>:</p>
            <ul>
            <li>full slam is useful when all sensor data is available
            after execution (batch processing)</li>
            <li>allows for loop closure corrections, where the
            <strong>trajectory can be optimized</strong>
            retrospectively</li>
            <li>used in graph-based slam and optimization-based slam
            techniques like gtsam and pose graph optimization</li>
            </ul>
            <ol start="2" type="1">
            <li><strong>Online SLAM</strong>: → estimates only the
            <strong>most recent</strong> robot pose and the map</li>
            </ol>
            <ul>
            <li>the model estimates only the most recent pose <span
            class="math inline"><em>x</em><sub><em>t</em></sub></span>
            and the map <span class="math inline"><em>m</em></span></li>
            <li>the equation:</li>
            </ul>
            <p><span
            class="math display"><em>p</em>(<em>x</em><sub><em>t</em></sub>, <em>m</em> ∣ <em>z</em><sub>1 : <em>t</em></sub>, <em>u</em><sub>1 : <em>t</em></sub>) = ∫∫...∫<em>p</em>(<em>x</em><sub>1 : <em>t</em></sub>, <em>m</em> ∣ <em>z</em><sub>1 : <em>t</em></sub>, <em>u</em><sub>1 : <em>t</em></sub>)<em>d</em><em>x</em><sub>1</sub><em>d</em><em>x</em><sub>2</sub>...<em>d</em><em>x</em><sub><em>t</em> − 1</sub></span></p>
            <p>shows that all past poses <span
            class="math inline"><em>x</em><sub>1 : <em>t</em> − 1</sub></span>
            are marginalized out, leaving only the current pose and the
            map - gray-shaded area in the graphical model highlights
            that only the current pose is maintained explicitly - this
            approach is <strong>computationally efficient</strong> and
            suitable for real-time applications</p>
            <section id="interpretation" class="level3"
            data-number="10.2.1">
            <h3 data-number="10.2.1"><span
            class="header-section-number">10.2.1</span>
            interpretation</h3>
            <ul>
            <li>online slam is designed for real-time operation, where
            storing the entire trajectory is infeasible</li>
            <li>it relies on filtering methods such as the extended
            kalman filter (ekf) and particle filters</li>
            <li>used in applications where <strong>fast localization and
            mapping</strong> are required, such as autonomous navigation
            and robotics</li>
            </ul>
            <p><img src="img/slam/online_slam_graphical.png" width="300"></p>
            <p><strong>Comparison</strong> of the two methods:</p>
            </section>
            <section
            id="key-differences-between-full-slam-and-online-slam"
            class="level3" data-number="10.2.2">
            <h3 data-number="10.2.2"><span
            class="header-section-number">10.2.2</span> key differences
            between full slam and online slam</h3>
            <table>
            <colgroup>
            <col style="width: 42%" />
            <col style="width: 27%" />
            <col style="width: 30%" />
            </colgroup>
            <thead>
            <tr>
            <th>feature</th>
            <th>full slam</th>
            <th>online slam</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>estimates</strong></td>
            <td>entire trajectory <span
            class="math inline"><em>x</em><sub>1 : <em>t</em></sub></span>
            and map <span class="math inline"><em>m</em></span></td>
            <td>only current pose <span
            class="math inline"><em>x</em><sub><em>t</em></sub></span>
            and map <span class="math inline"><em>m</em></span></td>
            </tr>
            <tr>
            <td><strong>past poses</strong></td>
            <td>explicitly maintained</td>
            <td>marginalized out</td>
            </tr>
            <tr>
            <td><strong>computational complexity</strong></td>
            <td>higher (batch processing)</td>
            <td>lower (real-time feasible)</td>
            </tr>
            <tr>
            <td><strong>approach</strong></td>
            <td>smoothing-based (e.g., graph-based slam)</td>
            <td>filtering-based (e.g., ekf-slam, fastslam)</td>
            </tr>
            <tr>
            <td><strong>application</strong></td>
            <td>useful for post-processing and high-accuracy
            mapping</td>
            <td>used in real-time robotics, drones, autonomous
            vehicles</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            </section>
            <section id="slam-hardware" class="level2"
            data-number="10.3">
            <h2 data-number="10.3"><span
            class="header-section-number">10.3</span> SLAM Hardware</h2>
            <p>The main hardare for SLAM consists of</p>
            <ul>
            <li>a robot</li>
            <li>a range measurement device</li>
            </ul>
            <p>For SLAM implementation, a mobile robot requires,</p>
            <ul>
            <li>wheel encoders for odometry estimation</li>
            <li>a processor for calculation</li>
            <li>appropriate motion capabilities for the environment</li>
            </ul>
            <p>Several sensing technologies can be used for SLAM:</p>
            <ul>
            <li><p><strong>Laser Scanners</strong> → provide
            high-precision measurements with minimal processing
            required.</p></li>
            <li><p><strong>Sonar</strong> → less expensive than laser
            scanners but provide lower quality measurements with wider
            beam width (up to <span
            class="math inline">$30\degree$</span> compared to <span
            class="math inline">$0.25\degree$</span> for laser
            scanners).</p></li>
            <li><p><strong>Vision</strong> → cameras provide rich
            environmental information but require more computational
            processing. Stereo vision can provide depth information
            similar to range finders.</p></li>
            </ul>
            <p>Each sensing technology has advantages and limitations
            depending on the environment and application.</p>
            </section>
            <section id="landmark-identification" class="level2"
            data-number="10.4">
            <h2 data-number="10.4"><span
            class="header-section-number">10.4</span> Landmark
            Identification</h2>
            <p>Identifying <strong>landmarks</strong> is a crucial step
            in SLAM. Without this, it will be hard for a robot to place
            itself in its surroundings.</p>
            <p>A landmark is,</p>
            <blockquote>
            <p>a feature that can be <strong>re-observed</strong> and
            <strong>distinguished from environment</strong></p>
            </blockquote>
            <p>Landmarks are:</p>
            <ul>
            <li>used by robot to find out where it is → localize
            itself</li>
            <li>types of landmarks depend on the environment</li>
            </ul>
            <p>Good landmarks for SLAM should have the following
            properties:</p>
            <ul>
            <li>easily <strong>re-observable</strong> from different
            positions and angles</li>
            <li><strong>distinguishable</strong> from other
            landmarks</li>
            <li><strong>stationary</strong> (not moving objects)</li>
            <li><strong>plentiful</strong> in the environment</li>
            </ul>
            <p>The advantages of <strong>indoor landmarks</strong>,</p>
            <ul>
            <li>lots of <strong>straight lines</strong></li>
            <li>well defined corners</li>
            </ul>
            <p>The <em>common</em> methods to find/distinguish landmarks
            use these properties,</p>
            <ol type="1">
            <li><a href="#landmarks--spikes">spikes</a></li>
            <li><a
            href="#landmarks--ransac-random-sample-consensus">RANSAC</a></li>
            <li><a href="#landmarks--scan-matching">scan
            matching</a></li>
            </ol>
            <section id="landmarks-spikes" class="level3"
            data-number="10.4.1">
            <h3 data-number="10.4.1"><span
            class="header-section-number">10.4.1</span> Landmarks |
            Spikes</h3>
            <p>The main idea is → detecting <strong>significant
            changes</strong> in range measurements that indicate
            distinctive features.</p>
            <p>This technique,</p>
            <ul>
            <li>uses <strong>extrema</strong> to find landmarks</li>
            <li>if two values differ &gt; certain amount
            [e.g. <code>0.5</code>]</li>
            <li>detects <strong>big changes</strong></li>
            <li>some beams reflect from walls, others don’t</li>
            </ul>
            <p>Here is a sample output from such an algorithm:</p>
            <p><img src="img/slam/landmark_spikes.png" width="300"></p>
            <p><br></p>
            </section>
            <section id="landmarks-ransac-random-sample-consensus"
            class="level3" data-number="10.4.2">
            <h3 data-number="10.4.2"><span
            class="header-section-number">10.4.2</span> Landmarks |
            RANSAC (Random Sample Consensus)</h3>
            <p>A robust method for extracting lines from laser scans,
            useful for detecting walls in indoor environments.</p>
            <p>RANSAC (Random Sample Consensus) is a robust iterative
            method for estimating mathematical model parameters from a
            set of observed data that contains outliers. Developed by
            Fischler and Bolles in 1981, it’s widely used in computer
            vision and image processing tasks.</p>
            <p>The fundamental insight behind RANSAC is that most
            estimation techniques work well when fitting a model to data
            containing only inliers (data points that follow the model)
            but break down when outliers (data points that don’t fit the
            model) are present. RANSAC addresses this by:</p>
            <ol type="1">
            <li><strong>randomly sampling</strong> the minimum number of
            points required to determine model parameters</li>
            <li><strong>fitting</strong> a model to those points</li>
            <li>checking which other data points are
            <strong>consistent</strong> with this model</li>
            <li>keeping the model if it has <strong>sufficient
            support</strong> (enough inliers)</li>
            <li>repeating until finding a model with <strong>adequate
            consensus</strong> or reaching iteration limits</li>
            </ol>
            <p><strong>Algorithm Steps</strong></p>
            <ol type="1">
            <li><strong>select randomly</strong> → choose a random
            subset of the original data (the minimum needed to fit the
            model)</li>
            <li><strong>fit model</strong> → compute model parameters
            using only this subset</li>
            <li><strong>determine consensus</strong> → count how many
            data points from the entire dataset are consistent with the
            model (within a specified error threshold)</li>
            <li><strong>evaluate quality</strong> → if enough points
            agree with the model, consider it a good fit</li>
            <li><strong>refine model</strong> → optionally recompute the
            model using all identified inliers</li>
            <li><strong>repeat</strong> → try again with new random
            samples until a sufficiently good model is found or
            iteration limit is reached</li>
            </ol>
            <p>The following demonstrates RANSAC at a high level,</p>
            <p><img src="img/slam/ransac.3.png" width="300">
            <img src="img/slam/ransac.4.png" width="300"></p>
            <p><br></p>
            <p><strong>Important question</strong>: how many readings
            lie close to best fit line?</p>
            <p><strong>Mathematical Formulation</strong></p>
            <p>For a dataset with,</p>
            <ul>
            <li><span class="math inline"><em>n</em></span> total
            points</li>
            <li>a hypothesized probability <span
            class="math inline"><em>ϵ</em></span> of a point being an
            <strong>inlier</strong></li>
            <li>a <strong>minimum</strong> of <span
            class="math inline"><em>m</em></span> points needed to fit
            the model</li>
            <li>a desired probability <span
            class="math inline"><em>p</em></span> of finding at least
            one good sample</li>
            </ul>
            <p>The <a
            href="https://dl.acm.org/doi/pdf/10.1145/358669.358692">number
            of iterations</a> needed (<span
            class="math inline"><em>k</em></span>) is:</p>
            <p><span class="math display">$$
            k = \frac{log(1 - p)}{log(1 - (1 - ε)^m)}
            $$</span></p>
            <p><strong>Key Parameters</strong></p>
            <ol type="1">
            <li><strong>inlier threshold</strong> → the maximum distance
            a point can be from the model to be considered an
            inlier</li>
            <li><strong>iteration count</strong> → maximum number of
            iterations to attempt</li>
            <li><strong>consensus threshold</strong> → minimum number of
            inliers required to accept a model</li>
            <li><strong>probability threshold</strong> → confidence
            level for finding an optimal model</li>
            </ol>
            <p><strong>Advantages</strong></p>
            <ul>
            <li><strong>robustness</strong> → highly resistant to
            outliers, even when they compose the majority of the
            data</li>
            <li><strong>versatility</strong> → applicable to many
            different model types (lines, planes, homographies,
            etc.)</li>
            <li><strong>simplicity</strong> → conceptually
            straightforward and relatively easy to implement</li>
            <li><strong>efficiency</strong> → often performs well with
            fewer computations than exhaustive methods</li>
            </ul>
            <p><strong>Limitations</strong></p>
            <ul>
            <li><strong>non-deterministic</strong> → produces different
            results on different runs</li>
            <li><strong>parameter sensitivity</strong> → performance
            depends heavily on tuning threshold parameters</li>
            <li><strong>computational cost</strong> → may require many
            iterations for complex models or high outlier ratios</li>
            <li><strong>no guarantee</strong> → can fail to find the
            optimal solution, especially with poor parameter
            selection</li>
            </ul>
            <p><strong>Applications</strong></p>
            <ul>
            <li><strong>feature matching</strong> → robust matching of
            image features across multiple views</li>
            <li><strong>homography estimation</strong> → computing
            transformations between images</li>
            <li><strong>3D reconstruction</strong> → estimating camera
            poses and scene structure</li>
            <li><strong>line/curve fitting</strong> → finding geometric
            primitives in noisy data</li>
            <li><strong>object recognition</strong> → matching object
            models to observed data</li>
            </ul>
            <p>Another interesting property of RANSAC is that it can
            <strong>extrapolate lines as dots</strong> as well → easier
            calculations.</p>
            <p>One of the main advantages of RANSAC is that it is
            <strong>robust against people</strong>!</p>
            <p>Here is an example of an RANSAC output obtained from a
            LiDAR scan:</p>
            <p><img src="img/slam/ransac_lidar_output.1.png" width="200">
            <img src="img/slam/ransac_lidar_output.2.png" width="200"></p>
            <section id="pseudocode" class="level4"
            data-number="10.4.2.1">
            <h4 data-number="10.4.2.1"><span
            class="header-section-number">10.4.2.1</span>
            Pseudocode</h4>
            <p>Let,</p>
            <table>
            <thead>
            <tr>
            <th>symbol</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>N</strong></td>
            <td>max number of attempts</td>
            </tr>
            <tr>
            <td><strong>S</strong></td>
            <td>number of samples to compute initial line</td>
            </tr>
            <tr>
            <td><strong>D</strong></td>
            <td>degrees from initial reading to sample from</td>
            </tr>
            <tr>
            <td><strong>X</strong></td>
            <td>max distance reading may be from line</td>
            </tr>
            <tr>
            <td><strong>C</strong></td>
            <td>number of points that must line on line</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <pre><code>1 function RANSAC( N, S, D, X, C )

2 while 
3    there are still unassociated laser readings
4    number of readings &gt; consensus C
5    completed &lt; N trials

6 do
7    select random laser data reading, R
8    random sample S data readings within D degrees of R

9    calculate least_squares_best_fit_line(S,R)

10   how many readings within X cm of best fit line

11   if num of readings on best fit line &gt; consensus C
12      calculate new least squares best fit line
13          based on all readings that lie on old line

14      add new best fit line to lines extracted so far

15      remove number of readings lying on this line
17          from total set of unassociated readings</code></pre>
            <p>RANSAC’s ability to produce reasonable results even with
            a significant percentage of outliers makes it an important
            algorithm in modern computer vision and many other
            fields.</p>
            </section>
            <section id="visual-example" class="level4"
            data-number="10.4.2.2">
            <h4 data-number="10.4.2.2"><span
            class="header-section-number">10.4.2.2</span> Visual
            Example</h4>
            <p><img src="img/slam/ransac_visualization.full.svg" width="500"></p>
            <p>The diagrams illustrate the key concepts of RANSAC to
            help better understand how this robust estimation algorithm
            works:</p>
            <p>Fig. <strong>1</strong> → <strong>Initial
            Dataset</strong></p>
            <ul>
            <li>green points represent inliers that follow the true
            model</li>
            <li>red points represent outliers that don’t fit the
            model</li>
            <li>this shows the challenge RANSAC addresses → finding the
            correct model despite the presence of outliers</li>
            </ul>
            <p>Fig. <strong>2</strong> → <strong>Random Sampling &amp;
            Bad Model Fit</strong></p>
            <ul>
            <li>blue circles highlight randomly selected points
            (including an outlier)</li>
            <li>the orange dashed line shows a <strong>poor
            model</strong> fit resulting from including outliers</li>
            <li>this demonstrates why random sampling alone isn’t
            enough</li>
            </ul>
            <p>Fig. <strong>3</strong> → <strong>Better Random
            Sampling</strong> - blue circles highlight a better random
            selection (both inliers) - the solid green line shows a much
            better model fit - this illustrates how RANSAC can find good
            models through multiple random attempts</p>
            <p>Fig. <strong>4</strong> → <strong>Consensus Set
            Identification</strong></p>
            <ul>
            <li>gray dashed lines show the error threshold
            boundaries</li>
            <li>green points are the identified inliers (consensus
            set)</li>
            <li>faded red points are rejected outliers</li>
            </ul>
            <p>Fig. <strong>5</strong> → <strong>RANSAC Process
            Flow</strong> - flowchart showing the iterative nature of
            RANSAC: - random sampling → Model fitting → Counting inliers
            → Keeping best model - repeat until meeting termination
            criteria</p>
            <p><strong>Key Insights</strong> from the Visualizations</p>
            <p>RANSAC’s power comes from its iterative approach → if one
            random sample contains outliers (as in figure 2), future
            iterations can find better samples (as in figure 3). The
            error threshold (shown in figure 4) is crucial → it
            determines which points are considered inliers. The
            consensus step is what makes RANSAC robust → it measures how
            many points support each model. The iterative process
            (figure 5) ensures that with enough attempts, RANSAC can
            find a good model even with significant outlier
            contamination.</p>
            <p>These visualizations help demonstrate why RANSAC is so
            effective for computer vision tasks like image registration,
            feature matching and 3D reconstruction where <strong>data
            often contains many outliers</strong>.</p>
            <p><strong>Variations and Extensions</strong></p>
            <p>There are various updates and variations of RANSAC in
            use:</p>
            <ul>
            <li><strong>PROSAC</strong>: progressive sampling that
            prioritizes more promising matches</li>
            <li><strong>MLESAC</strong>: maximum Likelihood Estimation
            version that improves the cost function</li>
            <li><strong>LMEDS</strong>: least Median of Squares, a
            related robust estimation approach</li>
            <li><strong>Preemptive RANSAC</strong>: early termination
            strategies to improve efficiency</li>
            <li><strong>Multi-RANSAC</strong>: finding multiple models
            simultaneously in the same dataset</li>
            </ul>
            </section>
            </section>
            <section id="landmarks-scan-matching" class="level3"
            data-number="10.4.3">
            <h3 data-number="10.4.3"><span
            class="header-section-number">10.4.3</span> Landmarks | Scan
            Matching</h3>
            <p>Scan matching is the process of <strong>aligning</strong>
            current sensor readings with previous scans to determine
            position changes. It is also used for <strong>landmark
            association</strong> (see below).</p>
            <p>Here is a list of the <strong>top five</strong> SCAN
            matching algorithms:</p>
            <ul>
            <li><p><strong>Iterative Closest Point (ICP)</strong> →
            iteratively aligns point clouds by finding closest point
            correspondences and computing optimal rigid transformations
            [<a href="https://ieeexplore.ieee.org/document/121791">Besl
            &amp; McKay, 1992</a>].</p></li>
            <li><p><strong>Normal Distributions Transform (NDT)</strong>
            → represents point clouds as collections of normal
            distributions for registration, offering better noise
            handling than ICP [<a
            href="https://ieeexplore.ieee.org/document/1249285">Biber
            &amp; Straßer, 2003</a>].</p></li>
            <li><p><strong>LOAM</strong> → lidar Odometry and Mapping
            that extracts and tracks edge and planar features for
            efficient and accurate real-time mapping [<a
            href="https://www.ri.cmu.edu/pub_files/2014/7/Ji_LidarMapping_RSS2014_v8.pdf">Zhang
            &amp; Singh, 2014</a>].</p></li>
            <li><p><strong>Generalized ICP (GICP)</strong> → unifies
            point-to-point and point-to-plane approaches using a
            probabilistic framework that handles uncertainty [<a
            href="https://www.robots.ox.ac.uk/~avsegal/resources/papers/Generalized_ICP.pdf">Segal
            et al., 2009</a>].</p></li>
            <li><p><strong>Correlative Scan Matching (CSM)</strong> →
            searches for the best alignment using correlation between
            grid representations, handling large initial misalignments
            better than iterative methods [<a
            href="https://april.eecs.umich.edu/papers/details.php?name=olson2009icra">Olson,
            2009</a>].</p></li>
            </ul>
            </section>
            <section id="comparison-of-spike-ransac-and-scan-matching"
            class="level3" data-number="10.4.4">
            <h3 data-number="10.4.4"><span
            class="header-section-number">10.4.4</span> Comparison of
            Spike, RANSAC and Scan Matching</h3>
            <table>
            <colgroup>
            <col style="width: 23%" />
            <col style="width: 17%" />
            <col style="width: 20%" />
            <col style="width: 38%" />
            </colgroup>
            <thead>
            <tr>
            <th>feature</th>
            <th>spike</th>
            <th>ransac</th>
            <th>scan matching</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>primary purpose</strong></td>
            <td>sudden changes (or spikes) in readings</td>
            <td>outlier rejection and model fitting from data that’s
            mixed</td>
            <td>alignment of point clouds/scans to determine relative
            transformation</td>
            </tr>
            <tr>
            <td><strong>core mechanism</strong></td>
            <td>creates histogram “spikes” of feature parameters to
            identify consistent patterns</td>
            <td>randomly samples minimal data subsets to fit models and
            evaluates consensus</td>
            <td>aligns sensor data by finding the transformation that
            best aligns current scan with reference</td>
            </tr>
            <tr>
            <td><strong>application in slam</strong></td>
            <td>feature detection in point clouds and images</td>
            <td>robust estimation of transformation from noisy feature
            matches</td>
            <td>direct alignment of consecutive sensor measurements</td>
            </tr>
            <tr>
            <td><strong>computational complexity</strong></td>
            <td>medium to high (depends on resolution)</td>
            <td>variable (depends on ratio of regular to outliers and
            model complexity)</td>
            <td>high (especially for dense point clouds)</td>
            </tr>
            <tr>
            <td><strong>robustness to noise</strong></td>
            <td>good (naturally filters noise)</td>
            <td>excellent (explicitly designed for outlier
            rejection)</td>
            <td>varies by method (ICP is sensitive, NDT is more
            robust)</td>
            </tr>
            <tr>
            <td><strong>initial estimate required</strong></td>
            <td>no (can work without prior knowledge)</td>
            <td>no (global approach)</td>
            <td>often yes (especially for ICP)</td>
            </tr>
            <tr>
            <td><strong>best use case</strong></td>
            <td>environments with distinctive geometric features</td>
            <td>data with high number of outliers</td>
            <td>sequential pose estimation with good initial guess</td>
            </tr>
            <tr>
            <td><strong>limitations</strong></td>
            <td>less effective in feature-poor environments</td>
            <td>may be computationally expensive for complex models</td>
            <td>often susceptible to local minima</td>
            </tr>
            <tr>
            <td><strong>example</strong></td>
            <td><a
            href="https://dl.acm.org/doi/10.1145/882262.882310">Spike</a>
            (Körtgen et al.)</td>
            <td><a
            href="https://dl.acm.org/doi/10.1145/358669.358692">RANSAC</a>
            (Fischler &amp; Bolles)</td>
            <td><a
            href="https://ieeexplore.ieee.org/document/121791">ICP</a>,
            <a
            href="https://ieeexplore.ieee.org/document/1249285">NDT</a>,
            <a
            href="https://april.eecs.umich.edu/papers/details.php?name=olson2009icra">correlative
            scan matching</a></td>
            </tr>
            <tr>
            <td><strong>parameters</strong></td>
            <td>histogram bin size, peak detection threshold</td>
            <td>inlier threshold, iteration count, consensus size</td>
            <td>convergence criteria, correspondence method, error
            metric</td>
            </tr>
            <tr>
            <td><strong>output</strong></td>
            <td>features</td>
            <td>model parameters and inlier set</td>
            <td>rigid transformation (rotation and translation)</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            </section>
            </section>
            <section id="data-association" class="level2"
            data-number="10.5">
            <h2 data-number="10.5"><span
            class="header-section-number">10.5</span> Data
            Association</h2>
            <p>As we see from Scan matching and in working with
            landmarks in general, <strong>matching</strong> an observed
            landmark from <strong>different</strong> scans is important!
            And what happens with landmarks are
            <strong>“reobserved”</strong>?</p>
            <p>This is not as simple as it looks.</p>
            <p>Consider the followint <a
            href="https://dspace.mit.edu/bitstream/handle/1721.1/36832/16-412JSpring2004/NR/rdonlyres/Aeronautics-and-Astronautics/16-412JSpring2004/A3C5517F-C092-4554-AA43-232DC74609B3/0/1Aslam_blas_report.pdf">example</a>:</p>
            <p>Let’s consider a chair a landmark. Let us say we are in a
            room and see a specific chair.</p>
            <p><img src="img/slam/chair.jpg" width="300"></p>
            <p><br></p>
            <p>Now we leave the room and then at some later point
            subsequently return to the room. If we then see a chair in
            the room and say that it is the same chair we previously saw
            then we have associated this chair to the old chair.</p>
            <p>This may seem simple but data association is hard to do
            well. Say the room had <strong>two chairs</strong> that
            looked practically <strong>identical</strong>!</p>
            <p><img src="img/slam/2_chair.png" width="300"></p>
            <p><br></p>
            <p>When we subsequently return to the room we might not be
            able to distinguish accurately which of the chairs were
            which of the chairs we originally saw (as they all look the
            same!). Our best bet is to say that the one to the left must
            be the one we previously saw to the left, and the one to the
            right must be the one we previously saw on the right.</p>
            <p>So we need a way to distinguish between previously seen
            landmarks and new ones.</p>
            <p>In practice, we may encounter the following issues:</p>
            <ul>
            <li>you might not re-observe landmarks every time step</li>
            <li>you might observe something as being a landmark but fail
            to ever see it again</li>
            <li>you might wrongly associate a landmark to a previously
            seen landmark.</li>
            </ul>
            <p>Recall that it should be <strong>easy</strong> to
            re-observe landmarks. The first two cases are not acceptable
            for for this purpose; rather they’re bad landmarks. Even
            with a very good landmark extraction algorithm you may run
            into these → it is best to define a suitable
            <strong>data-association policy</strong> to minimize
            this.</p>
            <p>The final point (wrong association of landmark to a
            previously seen one) can be really problematic → the robot
            thinks it is at a different location than where it is!</p>
            <p>At a high level, here’s a <strong>SLAM data association
            policy</strong>:</p>
            <ul>
            <li>assume a <strong>database of previously seen
            landmarks</strong> → initially empty</li>
            <li>don’t consider a landmark → unless seen
            <strong>N</strong> times</li>
            </ul>
            <p>To find the landmark, we use the <strong>nearest neighbor
            approach</strong>:</p>
            <pre><code>1   landmark_extraction(...) to extract all visible landmarks
2   associate each extracted landmark to closest landmark 
3       seen &gt; N times in database

4   each association --&gt; validation_gate(extracted, seen in database)
5       if validation_gate(...) passes --&gt; same landmark
6           increment number in database
7       if validation_gate(...) fails --&gt; add as new landmark in database</code></pre>
            <p>The simplest way to calculate the “nearest landmark” is
            to calculate the <strong><a
            href="https://hlab.stanford.edu/brian/euclidean_distance_in.html">Euclidean
            distance</a></strong>. Other methods include calculating the
            <strong><a
            href="https://www.mathworks.com/help/stats/mahal.html#mw_c4c6cf50-a523-47ef-adaf-085c19dff68d">Mahalanobis
            distance</a></strong> which is better but more
            complicated.</p>
            <p><strong>Validation Gate</strong> → check if landmark lies
            <strong>within area of uncertainty from EKF</strong>. The
            validation gate uses the fact that an EKF implementation
            gives a bound on the uncertainty of an observation of a
            landmark. Thus we can determine if an observed landmark is a
            landmark in the database by checking if the landmark lies
            within the area of uncertainty. This area can actually be
            drawn graphically and is known as an <strong>error
            ellipse</strong>. By setting a constant <span
            class="math inline"><em>λ</em></span> an observed landmark
            is associated to a landmark if the following formula
            holds:</p>
            <p><span
            class="math display"><em>v</em><sub><em>i</em></sub><sup><em>T</em></sup><em>S</em><sub><em>i</em></sub><sup>−1</sup><em>v</em><sub><em>i</em></sub> ≤ <em>λ</em></span></p>
            <p>This formula represents the validation gate condition
            where:</p>
            <table>
            <colgroup>
            <col style="width: 38%" />
            <col style="width: 61%" />
            </colgroup>
            <thead>
            <tr>
            <th>symbol</th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>v</em><sub><em>i</em></sub></span></td>
            <td>the innovation (difference between observed and
            predicted landmark measurement)</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>S</em><sub><em>i</em></sub></span></td>
            <td>the innovation covariance matrix</td>
            </tr>
            <tr>
            <td><span class="math inline"><em>λ</em></span></td>
            <td>a constant threshold value</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>This is the mathematical expression of the Mahalanobis
            distance test that determines whether an observed landmark
            should be associated with a previously seen landmark in the
            database.</p>
            </section>
            <section id="ekf-slam" class="level2" data-number="10.6">
            <h2 data-number="10.6"><span
            class="header-section-number">10.6</span> EKF-SLAM</h2>
            <p>Extended Kalman Filter is used to estimate the state
            (position) of the robot from odometry data and landmark
            observations. The EKF is usually described in terms of state
            estimation alone → the robot is given a “<em>perfect
            map</em>”.</p>
            <p>But when carrying out SLAM, there needs to be a
            <strong>map update</strong> step since the robot is figuring
            out the map as it goes along.</p>
            <p>Most of the EKF is standard, (<em>i.e.,</em> normal EKF)
            → once the matrices are set up, it is basically just a set
            of EKF equations. So the trick lies in setting up the right
            equations.</p>
            <p>Let’s go over one such process → extracted from <a
            href="https://dspace.mit.edu/bitstream/handle/1721.1/36832/16-412JSpring2004/NR/rdonlyres/Aeronautics-and-Astronautics/16-412JSpring2004/A3C5517F-C092-4554-AA43-232DC74609B3/0/1Aslam_blas_report.pdf">SLAM
            for Dummies</a>.</p>
            <p>Once the landmark extraction and the data association are
            complete, SLAM has three steps:</p>
            <ol type="1">
            <li>update the current state estimate using the odometry
            data</li>
            <li>update the estimated state from re-observing
            landmarks.</li>
            <li>add new landmarks to the current state.</li>
            </ol>
            <p>The <strong>first step</strong> is easy → an addition of
            the controls of the robot to the old state estimate</p>
            <ul>
            <li>the robot is at point <span
            class="math inline">(<em>x</em>, <em>y</em>)</span></li>
            <li>with rotation <span
            class="math inline"><em>θ</em></span></li>
            <li>controls are <span
            class="math inline">(<em>d</em><em>x</em>, <em>d</em><em>y</em>)</span></li>
            <li>change in rotation is <span
            class="math inline"><em>d</em><em>θ</em></span>.</li>
            </ul>
            <p>The result of the first step → the <strong>new
            state</strong> of the robot,</p>
            <p><span
            class="math display">(<em>x</em> + <em>d</em><em>x</em>, <em>y</em> + <em>d</em><em>y</em>)
            with rotation (<em>θ</em> + <em>d</em><em>θ</em>)</span></p>
            <p>In the <strong>second step</strong> → <strong>re-observed
            landmarks</strong> are considered. Using the estimate of the
            current position it is possible to estimate where the
            landmark should be. There is usually some difference →
            “<strong>innovation</strong>”. The innovation is basically
            the difference between the estimated robot position and the
            actual robot position, based on what the robot is able to
            see.</p>
            <p>In the second step the uncertainty of each observed
            landmark is also updated to reflect recent changes. An
            example could be if the uncertainty of the current landmark
            position is very low. Re-observing a landmark from this
            position with low uncertainty will increase the landmark
            certainty, <em>i.e.,</em> the variance of the landmark with
            respect to the current position of the robot.</p>
            <p><strong>Third step</strong> → new landmarks are added to
            the state, the robot map of the world. This is done using
            information about the current position and adding
            information about the relation between the new landmark and
            the old landmarks.</p>
            <p>Please read <strong>Chapter 11</strong> in the <a
            href="https://dspace.mit.edu/bitstream/handle/1721.1/36832/16-412JSpring2004/NR/rdonlyres/Aeronautics-and-Astronautics/16-412JSpring2004/A3C5517F-C092-4554-AA43-232DC74609B3/0/1Aslam_blas_report.pdf">SLAM
            for Dummies</a> book for all the details of the EKF
            modeling, materices and implementation details.</p>
            <p><strong>Important properties</strong> of EKF-SLAM
            include:</p>
            <ol type="1">
            <li>computational complexity → <strong>quadratic</strong> in
            the number of landmarks: <span
            class="math inline"><em>O</em>(<em>n</em><sup>2</sup>)</span></li>
            <li>the determinant of any sub-matrix of the map covariance
            matrix → <strong>decreases monotonically</strong> as
            observations are made</li>
            <li>at the limit → landmark estimates become <strong>fully
            correlated</strong></li>
            <li>EKF approach → <strong>can diverge</strong> if
            <strong>nonlinearities</strong> are significant</li>
            </ol>
            </section>
            <section id="slam-implementations" class="level2"
            data-number="10.7">
            <h2 data-number="10.7"><span
            class="header-section-number">10.7</span> SLAM
            Implementations</h2>
            <p>There exist various implementations of SLAM. It is one of
            the most studied areas in robotics and autonomous
            systems.</p>
            <ol type="1">
            <li><p><strong><a
            href="https://link.springer.com/chapter/10.1007/978-3-642-13408-1_14">Scan
            matching</a></strong> → can also be used a generic SLAM
            method. It attempts to align consecutive laser scans to
            determine robot displacement. This approach is
            computationally efficient but may accumulate errors over
            time.</p></li>
            <li><p><strong><a
            href="https://www.atlantis-press.com/proceedings/icaic-24/126003458">Submaps
            Approach</a></strong> → to manage computational complexity,
            large environments can be <strong>divided into smaller
            submaps</strong>. Each submap is built using standard SLAM
            techniques and then the submaps are connected. This
            divide-and-conquer approach reduces the computational burden
            and has been successfully applied in large
            environments.</p></li>
            <li><p><strong><a
            href="http://robots.stanford.edu/papers/thrun.seif.pdf">Sparse
            Extended Information Filters</a></strong> (SEIF) → SEIF is
            the <strong>information form</strong> of EKF, which
            maintains the information matrix (inverse of covariance
            matrix). The information matrix naturally becomes sparse in
            SLAM, allowing for efficient algorithms.</p></li>
            <li><p><strong><a
            href="http://robots.stanford.edu/papers/Thrun03g.pdf">FastSLAM</a></strong>
            → uses <strong>particle filters</strong> to represent the
            robot’s path and maintains separate EKFs for each landmark.
            This factorization takes advantage of conditional
            independence properties. This approach scales better than
            standard EKF-SLAM, with complexity <span
            class="math inline"><em>O</em>(<em>M</em> ⋅ <em>l</em><em>o</em><em>g</em><em>N</em>)</span>
            where <span class="math inline"><em>M</em></span> is the
            number of particles and <span
            class="math inline"><em>N</em></span> is number of
            landmarks.</p></li>
            </ol>
            <p><strong>References</strong></p>
            <ul>
            <li><a
            href="https://dspace.mit.edu/bitstream/handle/1721.1/36832/16-412JSpring2004/NR/rdonlyres/Aeronautics-and-Astronautics/16-412JSpring2004/A3C5517F-C092-4554-AA43-232DC74609B3/0/1Aslam_blas_report.pdf">SLAM
            for Dummies</a></li>
            <li><a
            href="https://www.mathworks.com/discovery/slam.html">SLAM
            Overview, MATLAB</a></li>
            <li><a
            href="https://www.mathworks.com/help/nav/slam.html">SLAM
            Examples and MATLAB code</a></li>
            <li><a
            href="http://probabilistic-robotics.informatik.uni-freiburg.de/ppt/slam.ppt">SLAM
            Slides in Probabilistic Robotics</a> by Thrun et al.</li>
            <li><a
            href="https://docs.ufpr.br/~danielsantos/ProbabilisticRobotics.pdf">Probabilistic
            Robotics Chapter 10</a> by Thrun et al.</li>
            <li><a
            href="https://dl.acm.org/doi/pdf/10.1145/358669.358692">“Random
            sample consensus: a paradigm for odel fitting with
            applications to image analysis and automated
            cartography.”</a> by Fischler, Martin A. and Robert C.
            Bolles. (Original RANSAC paper)</li>
            <li><a
            href="https://www.youtube.com/watch?v=saVZtgPyyJQ&amp;list=PLn8PRpmsu08rLRGrnF-S6TyGrmcA2X7kg&amp;index=4">SLAM
            using POSE Estimation</a> by MATLAB:</li>
            </ul>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/saVZtgPyyJQ?si=RDxScUQpWqC-7raB" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
            </iframe>
            <!--rel="stylesheet" href="./custom.sibin.css"-->
            </section>
            </section>
            <section id="path-planning-1" class="level1"
            data-number="11">
            <h1 data-number="11"><span
            class="header-section-number">11</span> Path Planning</h1>
            <p>Consider the following two options for the car,</p>
            <p><img src="img/path/alternate_paths.png"></p>
            <p><br></p>
            <p>Which path should the car choose? Assume that the car
            <strong>has</strong> to choose one of the two paths.</p>
            <p><a
            href="https://www.sapien.io/blog/path-planning-for-self-driving-cars">Path
            planning</a> for self-driving cars is the process of
            determining an optimal trajectory for an autonomous vehicle
            from its current position to its intended destination. This
            process involves the use of sophisticated algorithms that
            account for various environmental factors such as road
            conditions, traffic laws, obstacles and potential
            hazards.</p>
            <p>It typically involves:</p>
            <ul>
            <li>decision-making and</li>
            <li>predictions (of <strong>other</strong> cars,
            pedestrians, traffic signals, <em>etc.</em>)</li>
            </ul>
            <p>Effective autonomous vehicle path planning is not just
            about following a pre-determined path, but also about making
            real-time adjustments based on immediate surroundings. Any
            routes that are picked must be,</p>
            <ul>
            <li>safe</li>
            <li>convenient</li>
            <li>economically beneficial.</li>
            </ul>
            <p>Now, consider this example:</p>
            <p><img src="img/path/car_lane.png" width="400"></p>
            <p><br></p>
            <p>What should the red car do?</p>
            <ol type="1">
            <li>stay in lane, speed up</li>
            <li>stay in lane, slow down</li>
            <li>stay in lane, constant speed</li>
            <li>change lane</li>
            </ol>
            <p>The idea is that each scenario → probability associated
            with it.</p>
            <p>Let’s define a few terms. Consider the following scenario
            of a car traveling on a road.</p>
            <p><img src="img/path/car.2.png" width="400"></p>
            <p><br></p>
            <p>We see the car in its <strong>starting
            configuration</strong> and its <strong>final
            destination</strong>.</p>
            <p>Now, there are many possible <strong>intermediate
            configurations</strong> → between the starting configuration
            and final destination.</p>
            <p><img src="img/path/car.3.png" width="400"></p>
            <p><br></p>
            <p>The objective is to pick a <strong>sequence of
            configurations</strong>.</p>
            <p><img src="img/path/car.5.png" width="400"></p>
            <p><br></p>
            <p>This is the <strong>path</strong>, defined as</p>
            <blockquote>
            <p>continuous sequence of configurations → starting/ending
            with boundary configurations.</p>
            </blockquote>
            <p><br></p>
            <p><strong>Path planning</strong> is then defined as,</p>
            <blockquote>
            <p>the process of finding a geometric path from initial to
            given config such that each configuration state is
            <strong>feasible</strong>.</p>
            </blockquote>
            <p>Of course, there is a possibility of <strong>alternate
            paths</strong> for the same starting and ending
            configurations.</p>
            <p><img src="img/path/car.6.png" width="400"></p>
            <p><br></p>
            <p>Now, if another vehicle is present in this space, our car
            has to make some decisions, <em>e.g.,</em> does it,</p>
            <ol type="1">
            <li>continue going straight?</li>
            <li>change lanes</li>
            <li><em>actively</em> overtake?</li>
            </ol>
            <p><img src="img/path/car.8.png" width="400"></p>
            <p><br></p>
            <p>So, the car has to execute a <strong>maneuver</strong>
            defined as,</p>
            <blockquote>
            <p>a high-level characteristic of vehicle’s motion that
            encompasses position+speed of vehicle.</p>
            </blockquote>
            <p><br></p>
            <p>So then <strong>maneuver planning</strong> is defined
            as,</p>
            <blockquote>
            <p>taking best high-level decision for vehicle.</p>
            </blockquote>
            <p><img src="img/path/car.9.png" width="400"></p>
            <p><br></p>
            <p>In this case it could mean just changing the lanes since
            our objective is to get to the final destination. This
            involves some <strong>maneuver planning</strong>,
            <em>i.e.,</em></p>
            <blockquote>
            <p>take the best high-level decision for vehicle that
            accounts for the path from the planning algorithm.</p>
            </blockquote>
            <p><img src="img/path/car.10.png" width="400"></p>
            <p><br></p>
            <p>Clearly the path is neither instantaneous not continuous.
            The car needs to go through a sequence of **discrete
            configurations*,</p>
            <p><img src="img/path/car.11.png" width="400"></p>
            <p><br></p>
            <p>This is known as a <strong>trajectory</strong> which
            is,</p>
            <blockquote>
            <p>sequence of states visited by vehicle → parameterized by
            time and velocity.</p>
            </blockquote>
            <p><br></p>
            <p>This leads us to <strong>trajectory planning</strong>
            that can e defined as,</p>
            <blockquote>
            <p><strong>real-time planning</strong> of vehicle’s move →
            from one feasible state to next.</p>
            </blockquote>
            <p><img src="img/path/car.13.png" width="400"></p>
            <p><br></p>
            <p>An important aspect of trajectory planning is that it has
            to <strong>satisfy the car’s kinematics</strong>,
            <em>i.e.,</em> not break the laws of physics!</p>
            <p>To summarize the various definitions,</p>
            <table>
            <colgroup>
            <col style="width: 21%" />
            <col style="width: 34%" />
            <col style="width: 43%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;"><strong>term</strong></th>
            <th
            style="text-align: left;"><strong>definition</strong></th>
            <th
            style="text-align: left;"><strong>examples/notes</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><strong>path</strong></td>
            <td style="text-align: left;">continuous sequence of
            configurations</td>
            <td style="text-align: left;">starting/ending with boundary
            configurations</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>path
            planning</strong></td>
            <td style="text-align: left;">find a geometric path from
            initial to given config</td>
            <td style="text-align: left;">each configuration and state
            on path is feasible</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>maneuver</strong></td>
            <td style="text-align: left;">high-level characteristic of
            vehicle’s motion</td>
            <td style="text-align: left;">encompasses position and speed
            of vehicle on road <br> <em>e.g.,</em> going straight,
            changing lanes, turning, overtaking</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>maneuver
            planning</strong></td>
            <td style="text-align: left;">take best high-level decision
            for vehicle</td>
            <td style="text-align: left;">take into account path
            specified by planning algorithm</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>trajectory</strong></td>
            <td style="text-align: left;">sequence of states visited by
            vehicle</td>
            <td style="text-align: left;">parameterized by time and
            velocity</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>trajectory
            planning</strong></td>
            <td style="text-align: left;">real-time planning of
            vehicle’s moves</td>
            <td style="text-align: left;">from one feasible state to the
            next <br> satisfied by car’s kinematics</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>At its core, path planning involves the following
            components:</p>
            <ul>
            <li>sensing</li>
            <li>mapping</li>
            <li>localization.</li>
            </ul>
            <section id="path-planning-approaches" class="level2"
            data-number="11.1">
            <h2 data-number="11.1"><span
            class="header-section-number">11.1</span> Path Planning |
            Approaches</h2>
            <p>A lot of path planning involves
            <strong>predicting</strong> what the environment around us
            will do, a few seconds into the future. For instance,</p>
            <ul>
            <li>pedestrian will move (and direction)</li>
            <li>traffic sign remains still</li>
            </ul>
            <p>At a high-level, there are <a
            href="https://www.sapien.io/blog/path-planning-for-self-driving-cars"><strong>three</strong>
            types</a> of path planning:</p>
            <ol type="1">
            <li><strong>Global Path Planning</strong>: planning a route
            from starting point to the destination, typically over a
            <strong>long distance</strong>. This considers high-level
            road network constraints (<em>e.g.</em>, available routes,
            traffic regulations and road conditions). Path planning
            algorithms must ensure that overall trajectory avoids major
            hazards and follows optimal paths, while adhering to traffic
            laws and minimizing fuel consumption.</li>
            </ol>
            <ul>
            <li>To summarize → compute an <strong>efficient</strong>
            route for long-distance travel, which is then adjusted by
            local planning systems as needed.</li>
            </ul>
            <ol start="2" type="1">
            <li><strong>Local/partial Path Planning</strong> is
            responsible for <strong>navigating</strong> the vehicle
            through its <strong>immediate surroundings</strong> dealing
            with real-time adjustments, such as obstacle avoidance,
            managing intersections and handling other dynamic obstacles
            (<em>e.g.,</em> pedestrians, cyclists and vehicles).
            Self-driving car routing involves adjusting the vehicle’s
            trajectory to respond to changing conditions and to ensure
            smooth and safe navigation.</li>
            </ol>
            <ul>
            <li>In summary → requires vehicle to constantly re-evaluate
            its environment; typically using algorithms calculate the
            <strong>best local trajectory</strong> at any given moment,
            based on sensor data.</li>
            </ul>
            <ol start="3" type="1">
            <li><strong>Behavioral Path Planning</strong> focuses on
            <strong>anticipating and responding</strong> to the behavior
            of other road users. This approach simulate
            <strong>human-like decision-making</strong> to ensure
            <strong>safe</strong> interactions with pedestrians,
            cyclists and other vehicles. Behavioral path planning allows
            the vehicle to <strong>adjust its actions</strong> to
            prevent collisions and ensure smooth traffic flow.</li>
            </ol>
            <ul>
            <li>In summary → plays a crucial role in <strong>urban
            environments</strong> (dense traffic) and vehicles must make
            <strong>real-time decisions</strong> to adapt to other
            drivers.</li>
            </ul>
            <p><img src="img/path/path_planning_strucutre.png" width="400"></p>
            <p>As the figure shows, path planning uses a combination of
            known and unknown information in conjunction with the sensor
            and mapping. The <strong>horizontal</strong> planning is
            essentially the decision on what <strong>trajectory</strong>
            to follow while the <strong>vertical</strong> planning is to
            decide on <strong>speed</strong> (accelerate, constant
            speed, decelerate).</p>
            </section>
            <section id="path-planning-predictions-and-decision-making"
            class="level2" data-number="11.2">
            <h2 data-number="11.2"><span
            class="header-section-number">11.2</span> Path Planning |
            Predictions and Decision Making</h2>
            <p>In order to predict what will happen in the immediate
            future, multiple approaches have been developed:</p>
            <ol type="1">
            <li><strong>machine-learning</strong> based</li>
            </ol>
            <p><img src="img/path/paths_ml.png" width="300"></p>
            <p><br></p>
            <p>The main idea has the following two phases:</p>
            <ul>
            <li><strong>training</strong> phase:
            <ul>
            <li>gather massive history of vehicles and paths</li>
            <li>hundreds/thousands of vehicles, different actions at
            intersection</li>
            </ul></li>
            <li><strong>unsupervised</strong> learning
            <ul>
            <li>clustering algorithms</li>
            <li>each cluster a typical trajectory for vehicle</li>
            </ul></li>
            </ul>
            <p>More driving leads to more data and <strong>better
            estimates</strong> → past behavior can affect current
            decisions.</p>
            <ol start="2" type="1">
            <li><strong>model</strong>-based</li>
            </ol>
            <p>This methods attempts to <strong>imagine possible
            choices</strong> for the vehicle.</p>
            <p><img src="img/path/path_model.png" width="400"></p>
            <p><br></p>
            <p>In this example, the green car is attempting to merge
            into the highway but the red car’s behavior can cause
            problems. So we imagine the four choices for the other
            car:</p>
            <ul>
            <li>speed up</li>
            <li>slow down</li>
            <li>constant speed</li>
            <li>change lanes</li>
            </ul>
            <p>And, as mentioned earlier, each has a <strong>probability
            that changes with observations</strong> → sensors work in
            real-time.</p>
            <p>This method,</p>
            <ul>
            <li>implements feasibility of trajectory</li>
            <li>eliminates <em>impossible behaviors</em></li>
            <li>focus on what’s possible, <strong>not on past</strong>
            (compare with ML-based approach).</li>
            </ul>
            <p><br></p>
            <section id="decision-making" class="level3"
            data-number="11.2.1">
            <h3 data-number="11.2.1"><span
            class="header-section-number">11.2.1</span> Decision
            Making</h3>
            <p>Once we have an estimate of the immediate future, we need
            → a <strong>decision</strong>, <em>e.g.,</em></p>
            <ul>
            <li>brake if obstacle detected?</li>
            <li>accelerate or change lanes?</li>
            </ul>
            <p><img src="img/path/decision_making.jpg" height="200"></p>
            <p>A lot of this depends on the <strong>environment</strong>
            (highway vs parking lot). We need to consider issues such
            as:</p>
            <ul>
            <li>safety</li>
            <li>feasibility</li>
            <li>efficiency</li>
            <li>legality</li>
            <li>passenger comfort</li>
            </ul>
            <p>Enter <strong>finite state machines</strong>.</p>
            <p>First we need to define a couple of things:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <tbody>
            <tr>
            <td>define <strong>states</strong> of a car</td>
            <td><strong>cost functions</strong> to define *choice** of
            state</td>
            </tr>
            <tr>
            <td><img src="img/path/fsm_car_states.png" width="100"></td>
            <td><img src="img/path/fsm_calculator.png" width="100"></td>
            </tr>
            <tr>
            <td><em>e.g.,</em> on highway, stationary,
            <em>etc.</em></td>
            <td>Computed (independently) for each possible scenario</td>
            </tr>
            <tr>
            <td>options → stay in lane, change to left lane, overtake a
            car</td>
            <td><strong>lowest cost wins</strong></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>So how do we define this <strong>cost</strong>?</p>
            <p>The cost can be calculated using various factors,
            <em>e.g.</em>, feasibility, security, legal, comfort, speed,
            <em>etc</em>.</p>
            <p><strong>One</strong> way to estimate this could be:</p>
            <p><img src="img/path/equations/pngs/equations-2.png"></p>
            <p>Each factor can either be defined using number or
            functions, <em>e.g.,</em> the cost for speeding can be
            defined as:</p>
            <p><img src="img/path/speed_cost.png" width="300"></p>
            <p><br></p>
            <p>The “<em>weight</em>” for each of the factors helps to
            define the <em>importance</em> of that factor in the cost
            calculations.</p>
            </section>
            </section>
            <section id="path-planning-setup" class="level2"
            data-number="11.3">
            <h2 data-number="11.3"><span
            class="header-section-number">11.3</span> Path Planning |
            Setup</h2>
            <p>The main objective of path planning is to
            <strong>generate a trajectory</strong> → a polynomial that
            pass through <strong>waypoints</strong>. Essentially the
            trajectory is a <em>curve</em> through a set of
            waypoints.</p>
            <p>Many path planning algorithms use the <strong><a
            href="https://roboticsknowledgebase.com/wiki/planning/frenet-frame-planning">Frenet
            coordinate system</a></strong> which defines:</p>
            <div class="multicolumn">
            <div>
            <p><img src="img/path/equations/pngs/equations-4.png" width="500"></p>
            </div>
            <div>
            <p><img src="img/path/frenet.png" width="300"></p>
            </div>
            </div>
            <p>The idea is that the Frenet coordinate system is easier
            to use for trajectory and waypoint calculations since it is
            always relative to the center of the lane. A cartesian
            system would be absolute and be harder to manage with
            changing geographies (of the road).</p>
            <p>Once a decision has been made (<em>e.g.,</em> overtake),
            a path planning algorithm → <strong>generates multiple
            trajectories</strong>,</p>
            <p><img src="img/path/multiple_trajectories.png" width="400"></p>
            <p><br></p>
            <p>The idea is to choose one based on the criteria/costs
            that we have established so far.</p>
            <p><a
            href="https://www.sciencedirect.com/science/article/pii/S0968090X15003447?via%3Dihub">Trajectory
            planning</a> (also known as trajectory generation) is the
            real-time planning of an actual vehicle’s transition from
            one feasible state to the next, satisfying the vehicle’s
            <em>kinematic limits</em> based on,</p>
            <ul>
            <li>vehicle dynamics and</li>
            <li>constrained by the navigation comfort.</li>
            </ul>
            <p>All this while respecting lane boundaries and traffic
            rules, while avoiding, at the same time,
            <strong>obstacles</strong> (other road users, ground
            conditions, ditches, <em>etc.</em>). Trajectory planning is
            parameterized by <span
            class="math inline">(<em>t</em><em>i</em><em>m</em><em>e</em>, <em>a</em><em>c</em><em>c</em><em>e</em><em>l</em><em>e</em><em>r</em><em>a</em><em>t</em><em>i</em><em>o</em><em>n</em>, <em>v</em><em>e</em><em>l</em><em>o</em><em>c</em><em>i</em><em>t</em><em>y</em>)</span>
            and is frequently referred to as <strong>motion
            planning</strong>. During each “planning cycle”, a path
            planner module,</p>
            <ul>
            <li>generates a number of trajectories from the vehicle’s
            current location</li>
            <li>with a look-ahead distance,</li>
            <li>depending on the speed and line-of-sight of the
            vehicle’s on-board sensors and</li>
            <li>evaluating each trajectory w.r.t. some <strong>cost
            function</strong> i</li>
            </ul>
            <p>to determine an <strong>optimal trajectory</strong>.</p>
            <p>Trajectory planning is scheduled at regular intervals →
            length of which largely depends on the frequency of
            receiving fresh sensor data.</p>
            <p>Most existing trajectory planning algorithms follows two
            steps:</p>
            <ol type="1">
            <li>trajectory generated on a low resolution/lower
            dimensional search spaced</li>
            <li>the resulting optimal trajectory smoothed out on a
            higher resolution/higher dimensional search space.</li>
            </ol>
            <p>The planning module is integral to rendering complete
            autonomy to the vehicle with the outputs of the trajectory
            planner feeding into the low-level steering/manoeuvre
            control unit.</p>
            <section id="graphs-used-for-path-planning" class="level3"
            data-number="11.3.1">
            <h3 data-number="11.3.1"><span
            class="header-section-number">11.3.1</span> Graphs Used for
            Path Planning</h3>
            <p>One of the challenges for finding a path is: <strong>how
            to represent the search space?</strong> Essentially, the
            environment should be represented so that we can query for
            paths, optimal or otherwise. Hence, the physical environment
            must be transformed into some sort of a state space, a
            <strong>graph</strong> even → discretizing it, for easier
            computations. Most methods start with the bare
            representation (lanes, road boundaries) and then convert
            them into higher-order graphical representations.</p>
            <p>Here are some popular graphical representations used in
            path planning:</p>
            <ol type="1">
            <li><strong><a
            href="https://ics.uci.edu/~goodrich/teach/geom/notes/Voronoi1.pdf">Voronoi
            diagrams</a></strong> (aka Dirichlet Tessellation)
            partitions a plane with <span
            class="math inline"><em>n</em></span> points into convex
            polygons such that each polygon contains <strong>exactly
            one</strong> generating point and every point in a given
            polygon is closer to its generating point than to any other.
            In the simplest case, these objects are just finitely many
            points in the plane (called seeds, sites, or generators).
            For each seed there is a corresponding region, called
            a Voronoi cell, consisting of all points of the plane closer
            to that seed than to any other.</li>
            </ol>
            <p><img src="img/path/path_voronoi.1.png" height="150">
            <img src="img/path/path_voronoi.2.png" height="150"></p>
            <p><br></p>
            <p>Voronoi Diagrams generate paths which
            <strong>maximize</strong> the <a
            href="https://www.sciencedirect.com/science/article/pii/S0968090X15003447?via%3Dihub#b0535">distance
            between the vehicle and surrounding obstacles</a>.
            Algorithms which are used for searching on Voronoi Diagrams
            are **complete* → <em>i.e.,</em> if a path exists in the
            free space, it would also appear on the Voronoi Diagram.</p>
            <p>Voronoi Diagrams are typically used for planning in
            static environments, such as parking lots. Voronoi diagrams
            on their own are not suitable for on-road path-planning,
            since Voronoi edges, along which a vehicle navigates, can
            potentially be discontinuous.</p>
            <ol start="2" type="1">
            <li><strong><a
            href="https://www.mathworks.com/help/robotics/ug/occupancy-grids.html">Occupancy
            Grids</a></strong> work in manner similar to that of Voronoi
            diagrams,</li>
            </ol>
            <ul>
            <li>they discretise the state space into a grid and</li>
            <li>each cell of the grid is associated with a probability
            of the cell being occupied by an obstacle, or a cost
            proportional to the feasibility or risk of traversal.</li>
            </ul>
            <p><img src="img/path/path_occupancy_grid.png" height="150"></p>
            <p>Risk or feasibility is calculated by considering presence
            of obstacles, lane and road boundaries.</p>
            <p>While grid-based approaches require <strong>low
            computational power</strong>, they cannot handle:</p>
            <ul>
            <li><strong>non-linear dynamics</strong> or</li>
            <li>obstacles.</li>
            </ul>
            <ol start="3" type="1">
            <li><strong><a
            href="https://www.cs.cmu.edu/~alonzo/pubs/papers/isairas05Planning.pdf">State
            lattices</a></strong> is a <em>generalization</em> of grids.
            They are built by the repetition of rectangles or squares to
            <strong>discretize a continuous space</strong>. Lattices are
            constructed by regularly repeating primitive paths which
            connect possible states for the vehicle → in terms of,</li>
            </ol>
            <ul>
            <li>position,</li>
            <li>curvature or</li>
            <li>time.</li>
            </ul>
            <p><img src="img/path/path_state_lattices.png" height="150"></p>
            <p><br></p>
            <p>The problem of planning then reduces to a “<a
            href="https://www.sciencedirect.com/science/article/pii/S0968090X15003447?via%3Dihub#b0420">boundary
            value problem</a>” → connecting original state with the
            required final state. This method overcomes the limitations
            of grid-based techniques without increasing the
            computational requirements.</p>
            <ol start="4" type="1">
            <li><strong><a
            href="https://ai.stanford.edu/~ddolgov/papers/dolgov_gpp_stair08.pdf">Driving
            Corridors</a></strong> operate in a manner that’s similar to
            the <a
            href="https://pmc.ncbi.nlm.nih.gov/articles/PMC8624552/">air
            corridors</a> air traffic control provides for modern
            aircraft. They represent a <strong>continuous collision-free
            space</strong>, bounded by road and lane boundaries as well
            as other obstacles, where a vehicle is expected to
            move.</li>
            </ol>
            <p><img src="img/path/path_driving_corridors.png" height="150"></p>
            <p><br></p>
            <p>Driving corridors are based on lane boundary information
            given by <strong>detailed digital maps</strong> or a map
            built using SLAM. Lane boundaries form the outer bound of
            the driving corridors, restricted in the presence of
            obstacles. A driving corridor is constructed for
            <strong>each car</strong>.</p>
            <p>A major drawback is that intensive computational power is
            needed for planning for the entire range of coordinates
            regarding the road network, representation of roads or lanes
            and various vehicles. This could prove to be
            prohibitive.</p>
            </section>
            </section>
            <section id="path-planning-algorithms" class="level2"
            data-number="11.4">
            <h2 data-number="11.4"><span
            class="header-section-number">11.4</span> Path Planning |
            Algorithms</h2>
            <p>So far we have defined,</p>
            <ul>
            <li>a coordinate system</li>
            <li>problem definition and setup and</li>
            <li>representation of the state space.</li>
            </ul>
            <p>But we haven’t <em>actually</em> seen how to <strong>find
            paths</strong> (rather, trajectories)! Now let’s look at
            some of the popular trajectory estimation strategies.</p>
            <p>There are <strong>three</strong> well-known classes of
            algorithms for this purpose:</p>
            <ol type="1">
            <li>traditional/<strong>physics</strong>-based methods →
            <em>e.g.,</em> <a
            href="#artificial-potential-field-apf">artificial potential
            field (APF)</a></li>
            <li><strong>graph</strong>-based → <em>e.g.,</em> <a
            href="#a-and-d-search">A* and D*</a></li>
            <li><strong>heauristic random traversal</strong>,
            <em>e.g.,</em> <a
            href="#rapidly-exploring-random-tree-rrt-algorithm">RRT</a></li>
            </ol>
            <section id="artificial-potential-field-apf" class="level3"
            data-number="11.4.1">
            <h3 data-number="11.4.1"><span
            class="header-section-number">11.4.1</span> Artificial
            Potential Field (APF)</h3>
            <p>A (scalar) <a
            href="https://en.wikipedia.org/wiki/Scalar_potential">potential
            field</a> describes the situation where the difference in
            the potential energies of an object in two different
            positions <strong>depends only on the positions</strong>,
            not upon the path taken by the object in traveling from one
            position to the other. One well-known example is potential
            energy due to <strong>gravity</strong>; others include
            magnetic and electric fields.</p>
            <p>An <strong>artificial potential field</strong> (APF)
            algorithm uses the artificial potential field to understand
            the motion of a robot in a certain space. For instance, if
            we consider a space,</p>
            <ul>
            <li>divided into a grid of cells</li>
            <li>with <strong>obstacles</strong> and a <strong>goal
            node</strong>.</li>
            </ul>
            <p><img src="img/path/apf_2d.webp" width="200"></p>
            <p><br></p>
            <p>The algorithm assigns an <strong>artificial potential
            field</strong> to every point in the grid using “potential
            field functions” so that,</p>
            <ul>
            <li>the robot simulates from the highest potential to the
            lowest potential</li>
            <li>the goal node has the <strong>lowest
            potential</strong></li>
            <li>starting node will have the <strong>maximum
            potential</strong>.</li>
            </ul>
            <p>Hence, we can say that the autonomous vehicle → moves
            from lowest to the highest potential.</p>
            <p><a
            href="https://cerv.aut.ac.nz/wp-content/uploads/2021/12/A-Survey-of-Path-Planning-Algorithms-for-Autonomous-Vehicles.pdf">One
            view of the potential field functions</a> looks like,</p>
            <p><img src="img/path/apf_functions.png" width="300"></p>
            <p><br></p>
            <p>In a 3D space, we can think of the problem as <a
            href="https://medium.com/@rymshasiddiqui/path-planning-using-potential-field-algorithm-a30ad12bdb08">finding
            a path</a> through a set of obstacles (producing repulsive
            fields) while heading towards the goal (exhibiting
            attractive fields).</p>
            <p><img src="img/path/apf_3d.webp" width="300"></p>
            <p><br></p>
            <p><a
            href="https://medium.com/@rymshasiddiqui/path-planning-using-potential-field-algorithm-a30ad12bdb08">Read
            more</a> about APF, the algorithms and implementation issues
            for more details.</p>
            </section>
            <section id="a-and-d-search" class="level3"
            data-number="11.4.2">
            <h3 data-number="11.4.2"><span
            class="header-section-number">11.4.2</span> A* and D*
            Search</h3>
            <p>Graph search algorithms generally represent a map based
            on the grid method, which is to decompose the map into
            interconnected and non-coincident grids. Search for an
            optimal path from the starting grid to the target grid
            therefore can avoid collisions.</p>
            <p>The <a
            href="https://www.datacamp.com/tutorial/a-star-algorithm"><strong>A*
            algorithm</strong></a> is an informed search algorithm,
            meaning it leverages a <strong>heuristic function to guide
            its search towards the goal</strong>. This heuristic
            function estimates the cost of reaching the goal from a
            given node, allowing the algorithm to <strong>prioritize
            promising paths</strong> and <strong>avoid exploring
            unnecessary ones</strong>.</p>
            <p>The A∗ algorithm combines the Dijkstra algorithm with the
            Best First Search algorithm so as to obtain the optimal path
            through, establishing an open list and a closed list, where
            the grid points for selecting are placed in the open list
            and the selected path grid is placed in the closed list:</p>
            <ul>
            <li>the starting grid number of the autonomous vehicles is
            placed in the open list</li>
            <li>then we put the adjacent grids which it may pass through
            into the open list</li>
            <li>the evaluation function <span
            class="math inline"><em>f</em>(<em>n</em>)</span> of the
            adjacent grid of the starting point in the open list is
            calculated</li>
            <li>the starting point is moved into the closed list so as
            to set the grid point with the smallest value as the new
            starting point</li>
            <li>the loop continues until the target point raster is
            placed in the open list</li>
            <li>finally, the points in the closed list are connected in
            order to get the optimal path.</li>
            </ul>
            <p><br></p>
            <p>The valuation function can be expressed as,</p>
            <p><span
            class="math display"><em>f</em>(<em>n</em>) = <em>g</em>(<em>n</em>) + <em>h</em>(<em>n</em>)</span></p>
            <p>where,</p>
            <table>
            <colgroup>
            <col style="width: 57%" />
            <col style="width: 42%" />
            </colgroup>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>g</em>(<em>n</em></span>)</td>
            <td style="text-align: left;">actual cost of starting point
            → current point</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>h</em>(<em>n</em></span>)</td>
            <td style="text-align: left;"><strong>heuristic
            function</strong>; estimated cost of current point → target
            point</td>
            </tr>
            <tr>
            <td></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>The cost function,</p>
            <ul>
            <li>is usually expressed by using Euclidean distance</li>
            <li>a change of cost function can effectively
            <strong>improve the performance</strong>.</li>
            </ul>
            <p>Here is a visual example of how A* works:</p>
            <p><img src="img/path/a_star.avif" width="300"></p>
            <p><br></p>
            <p>And here is a <a
            href="https://www.datacamp.com/tutorial/a-star-algorithm">pseudocode</a>
            for it</p>
            <pre><code>function A_Star(start, goal):
    // Initialize open and closed lists
    openList = [start]          // Nodes to be evaluated
    closedList = []            // Nodes already evaluated
    
    // Initialize node properties
    start.g = 0                // Cost from start to start is 0
    start.h = heuristic(start, goal)  // Estimate to goal
    start.f = start.g + start.h       // Total estimated cost
    start.parent = null              // For path reconstruction
    while openList is not empty:
        // Get node with lowest f value - implement using a priority queue
       // for faster retrieval of the best node
        current = node in openList with lowest f value
        
        // Check if we&#39;ve reached the goal
        if current = goal:
            return reconstruct_path(current)
            
        // Move current node from open to closed list
        remove current from openList
        add current to closedList
        
        // Check all neighboring nodes
        for each neighbor of current:
            if neighbor in closedList:
                continue  // Skip already evaluated nodes
                
            // Calculate tentative g score
            tentative_g = current.g + distance(current, neighbor)
            
            if neighbor not in openList:
                add neighbor to openList
            else if tentative_g &gt;= neighbor.g:
                continue  // This path is not better
                
            // This path is the best so far
            neighbor.parent = current
            neighbor.g = tentative_g
            neighbor.h = heuristic(neighbor, goal)
            neighbor.f = neighbor.g + neighbor.h
    
    return failure  // No path exists
function reconstruct_path(current):
    path = []
    while current is not null:
        add current to beginning of path
        current = current.parent</code></pre>
            <p><br></p>
            <p><strong>Limitations of A* Algorithm</strong></p>
            <table>
            <colgroup>
            <col style="width: 41%" />
            <col style="width: 58%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;"><strong>issue</strong></th>
            <th
            style="text-align: left;"><strong>description</strong></th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><strong>performance in complex
            environments</strong></td>
            <td style="text-align: left;">A* can struggle in large
            search spaces where the heuristic may provide inaccurate
            estimates, resulting in longer computation times</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>high memory
            usage</strong></td>
            <td style="text-align: left;">the priority queue of nodes
            can consume significant memory, posing challenges in
            resource-constrained systems</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>heuristic
            dependence</strong></td>
            <td style="text-align: left;">the algorithm’s effectiveness
            relies on the quality of the heuristic; a poor choice can
            lead to inefficient pathfinding</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>limited real-time
            responsiveness</strong></td>
            <td style="text-align: left;">a* may not respond quickly
            enough in highly dynamic environments where obstacles
            frequently change, as it requires reevaluating paths based
            on prior calculations</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><strong>D* Algorithm</strong></p>
            <p>The <a
            href="https://ieeexplore.ieee.org/document/351061"><strong>Dynamic
            A*</strong></a> algorithm is capable of planning paths in
            <strong>unknown, partially known and changing
            environments</strong> in an efficient, optimal and complete
            manner. It will continue using the original path after
            crossing the obstacle, which improves the efficiency of the
            secondary path planning.</p>
            <p>It is specifically designed for dynamic environments
            where obstacles may appear or disappear during navigation.
            It is particularly effective for robotic applications that
            require <strong>real-time adaptability</strong> as the robot
            receives new information about its surroundings. By updating
            paths efficiently without recalculating from scratch, D*
            ensures timely responses to environmental changes.</p>
            <p><a
            href="https://engineering.miko.ai/path-planning-algorithms-a-comparative-study-between-a-and-d-lite-01133b28b8b4">Read
            more</a> about D* and its variants.</p>
            </section>
            <section id="rapidly-exploring-random-tree-rrt-algorithm"
            class="level3" data-number="11.4.3">
            <h3 data-number="11.4.3"><span
            class="header-section-number">11.4.3</span>
            Rapidly-exploring Random Tree (RRT) Algorithm</h3>
            <p>The idea is to use the starting point as the root node
            and grow the tree randomly in the feasible space until it
            touches the end point (leaves). We then get a
            <strong>collision-free path</strong> from the starting point
            to the ending point.</p>
            <p>RRT uses <strong>random sampling</strong> and rapid
            expansion to explore the search space. It randomly generates
            sampling points and expands the tree according to specific
            rules, gradually approaching the target point. Through
            iteration and connection, a feasible path from the initial
            point to the target point is finally found.</p>
            <p><img src="img/path/rrt.1.webp" width="300"></p>
            <p><br></p>
            <p>The <a
            href="https://www.nature.com/articles/s41598-024-76299-9">algorithm</a>
            for RRT,</p>
            <pre><code>Input: q_start, q_goal, n

Output: T

1: T.init (q_start);
2: for i = 1 to n do
3: q_rand &lt;- Sample(i);
4: q_nearest &lt;- Nearest (q_rand, T);
5: q_new &lt;- Steer (q_nearest, q_rand);
6:
7: if CollisionFree()) then
8:    T.addVertex(q_new);
9:    T.addEdge(q_near, q_new, L);
10: return T;</code></pre>
            <p><br></p>
            <p>Here is an <a
            href="https://graham-clifford.com/rrt-algorithm/">illustration
            of RRT</a> trying to find a path to the red dot:</p>
            <p><img src="img/path/rrt_animate.gif" width="400"></p>
            <p>The <a
            href="https://www.nature.com/articles/s41598-024-76299-9"><strong>RRT*
            Algorithm</strong></a> is a <strong>progressive</strong>
            <strong>optimal</strong> path planning algorithm based on
            RRT. It improves existing paths through rewiring steps,
            shortens path segments through optimization steps, and takes
            node costs into account to generate high-quality and more
            optimized paths. Compared with RRT, RRT* has better path
            quality, because it can find a better path while ensuring
            search efficiency.</p>
            <p>RRT* <strong>does not choose the nearest node</strong> as
            the parent node of the new node, but in a <strong>certain
            range</strong> around the new node, chooses the node with
            the best path (the least cost) as the parent node of the new
            node.</p>
            <p><img src="img/path/rrt_star_example.webp" width="500"></p>
            <p><br></p>
            <p>the method is to <strong>draw a circle</strong> around
            <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>a</em><em>r</em><em>e</em><em>s</em><em>t</em></sub></span>
            and compare the distance between a point in the circle and
            <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>w</em></sub></span>.
            If the distance between <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>a</em><em>r</em><em>e</em><em>s</em><em>t</em></sub></span>
            and <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>w</em></sub></span>
            is less than the distance between <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>w</em></sub></span>
            and <span class="math inline"><em>q</em><sub>1</sub></span>
            or <span class="math inline"><em>q</em><sub>2</sub></span>,
            <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>a</em><em>r</em><em>e</em><em>s</em><em>t</em></sub></span>
            and <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>w</em></sub></span>
            are connected. Meanwhile, we need to compare the shortest
            distance between <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>a</em><em>r</em><em>e</em><em>s</em><em>t</em></sub></span>
            and <span class="math inline"><em>q</em><sub>2</sub></span>.
            If the distance between <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>w</em></sub></span>
            and <span class="math inline"><em>Q</em> − 2</span> is
            shorter, we update the parent of <span
            class="math inline"><em>q</em><sub>2</sub></span> to <span
            class="math inline"><em>q</em><sub><em>n</em><em>e</em><em>w</em></sub></span>.
            This step is called the <strong>reconnect</strong>.</p>
            <p>Compared with RRT, the RRT* algorithm has the advantage
            of re-selecting nearby nodes and reconnecting extended nodes
            of random trees, inheriting the Probabilistic completeness
            of the RRT algorithm and making the planned routes more
            optimized.</p>
            <p>There have been <a
            href="https://www.nature.com/articles/s41598-024-76299-9">various
            improvements to RRT*</a> over the years, <em>e.g.,</em>
            G-RRT<em>, GPF-RRT</em>, APF-RRT<em>, Improved A_RRT</em>,
            <em>etc.</em></p>
            <p>The following figures provides insights into how these
            algorithms compare in terms of operations/performance:</p>
            <p><img src="img/path/rrt_improvements.webp" width="500"></p>
            <p><br> <br></p>
            <p>Note that the graphical representation algorithms
            (<em>e.g.,</em> Voronoi diagrams) are used <strong>in
            conjunction with</strong> the trajectory estimation
            algoritms (<em>e.g.,</em> APF, RRT).</p>
            <p><br></p>
            <p><strong>References</strong></p>
            <ul>
            <li><a
            href="https://cerv.aut.ac.nz/wp-content/uploads/2021/12/A-Survey-of-Path-Planning-Algorithms-for-Autonomous-Vehicles.pdf">A
            Survey of Path Planning Algorithms for Autonomous
            Vehicles</a> by Ming et al. </li>
            <li><a
            href="https://ieeexplore.ieee.org/document/351061">Optimal
            and efficient path planning for partially-known
            environments</a> by Stentz et al.</li>
            <li><a
            href="https://medium.com/@rymshasiddiqui/path-planning-using-potential-field-algorithm-a30ad12bdb08">Path
            Planning Using Potential Field Algorithm</a></li>
            <li><a
            href="https://thesai.org/Downloads/Volume10No8/Paper_76-Artificial_Potential_Field_Algorithm_Implementation.pdf">Artificial
            Potential Field Algorithm Implementation for Quadrotor Path
            Planning</a> by Iswanto et al.</li>
            <li><a
            href="https://www.cs.cmu.edu/~motionplanning/lecture/AppH-astar-dstar_howie.pdf">Robotic
            Motion Planning: A* and D* Search</a> by Howie Choset,
            CMU</li>
            <li><a
            href="https://www.datacamp.com/tutorial/a-star-algorithm">The
            A* Algorithm: A Complete Guide</a></li>
            <li><a
            href="https://engineering.miko.ai/path-planning-algorithms-a-comparative-study-between-a-and-d-lite-01133b28b8b4">Path-Planning
            Algorithms: A Comparative Study between A* and
            D*Lite</a></li>
            <li><a
            href="https://www.cs.cmu.edu/~ggordon/likhachev-etal.anytime-dstar.pdf">Anytime
            Dynamic A*: An Anytime, Replanning Algorithm</a></li>
            <li><a
            href="https://www.nature.com/articles/s41598-024-76299-9">Efficient
            path planning for autonomous vehicles based on RRT* with
            variable probability strategy and artificial potential field
            approach</a></li>
            <li><a
            href="https://www.sciencedirect.com/science/article/pii/S0968090X15003447?via%3Dihub">Real-time
            motion planning methods for autonomous on-road driving:
            State-of-the-art and future research directions</a></li>
            <li><a
            href="https://intellias.com/path-planning-for-autonomous-vehicles-with-hyperloop-option/">Path
            Planning for Autonomous Vehicles</a></li>
            <li><a
            href="https://www.thinkautonomous.ai/blog/path-planning-for-self-driving-cars/">Path
            Planning for Self-Driving Cars</a></li>
            <li><a href="https://www.youtube.com/watch?v=ySN5Wnu88nE">A*
            Search Algorithm</a>:
            <iframe width="560" height="315" src="https://www.youtube.com/embed/ySN5Wnu88nE?si=9DGfMp4xQzOBgx71" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe><!--rel="stylesheet" href="./custom.sibin.css"--></li>
            </ul>
            </section>
            </section>
            </section>
            <section id="object-detection-and-avoidance" class="level1"
            data-number="12">
            <h1 data-number="12"><span
            class="header-section-number">12</span> Object Detection and
            Avoidance</h1>
            <p>An autonomous vehicle uses sensory input devices like
            cameras, radar and lasers to allow the car to perceive the
            world around it, creating a digital map. But
            <strong>how</strong> does it actually do the “perception”
            part? Perception involves not just identifying that an
            object exists, but also,</p>
            <table>
            <tbody>
            <tr>
            <td style="text-align: left;">object
            <strong>classification</strong></td>
            <td style="text-align: left;"><strong>what</strong> is
            it?</td>
            </tr>
            <tr>
            <td style="text-align: left;">object
            <strong>localization</strong></td>
            <td style="text-align: left;"><strong>where</strong> is
            it?</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>If we consider a camera (one of the most common types of
            sensors in autonomous vehicles these days), this translates
            to <strong>recognizing</strong> various objects
            (<em>e.g.,</em> cars, traffic lights, pedestrians,
            <em>etc.</em>) and generating <strong>bounding
            boxes</strong> for them, as seen below.</p>
            <p><img src="img/object/camera_bounding_boxes.gif" width="400"></p>
            <p><br></p>
            <p>There are multiple classes of object detection and
            localization methods,</p>
            <ol type="1">
            <li>Classical <a
            href="#computer-vision-methods"><strong>computer
            vision</strong> methods</a></li>
            <li><a
            href="#deep-learning-methods"><strong>deep-learning</strong>
            based methods</a></li>
            </ol>
            <section id="computer-vision-methods" class="level2"
            data-number="12.1">
            <h2 data-number="12.1"><span
            class="header-section-number">12.1</span> Computer Vision
            Methods</h2>
            <p>There is a large body of work in computer vision to
            identify objects. Some of the more common ones are:</p>
            <ol type="1">
            <li><a
            href="https://medium.com/analytics-vidhya/a-gentle-introduction-into-the-histogram-of-oriented-gradients-fdee9ed8f2aa">Histogram
            of Gradient Objects</a> (HOG), mainly used for face and
            image detection, convert the image (<span
            class="math inline"><em>w</em><em>i</em><em>d</em><em>t</em><em>h</em> × <em>h</em><em>e</em><em>i</em><em>g</em><em>h</em><em>t</em> × <em>c</em><em>h</em><em>a</em><em>n</em><em>n</em><em>e</em><em>l</em><em>s</em></span>)
            into a feature vector of length <span
            class="math inline"><em>n</em></span> chosen by the user. It
            then uses a <strong>histogram of gradients</strong> that are
            then used as “features” of an image.</li>
            </ol>
            <p><img src="img/object/hog.webp" width="500"></p>
            <p><br></p>
            <p>Gradients are important – to check for edges and corners
            in an image (through regions of intensity changes) – since
            they often pack much more information than flat regions.</p>
            <p><a
            href="https://lear.inrialpes.fr/people/triggs/pubs/Dalal-cvpr05.pdf">Read
            the original paper</a> by Dalal and Triggs for more
            information.</p>
            <ol start="2" type="1">
            <li><a
            href="https://medium.com/@deepanshut041/introduction-to-sift-scale-invariant-feature-transform-65d7f3a72d40">Scale
            Invariant Feature Transform</a> (SIFT) is a method for
            extracting <strong>distinctive invariant features</strong>
            from images that can be used to perform <strong>reliable
            matching</strong> between different views of an object or
            scene. SIFT finds <strong>keypoints</strong> in an image
            that do not change based on:</li>
            </ol>
            <ul>
            <li>scale</li>
            <li>rotation</li>
            <li>illumination.</li>
            </ul>
            <p><img src="img/object/sift.png" width="400"></p>
            <p><br></p>
            <p>Image recognition matches individual features to a
            <strong>database of features</strong> from known objects
            using a fast nearest-neighbor algorithm. SIFT can robustly
            identify objects among clutter and occlusion while achieving
            near real-time performance.</p>
            <p><a
            href="https://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf">Read
            the original paper</a> by Lowe for more details.</p>
            <ol start="3" type="1">
            <li><a
            href="https://www.mygreatlearning.com/blog/viola-jones-algorithm/">Viola-Jones
            Detector</a> is used to accurately identify and analyze
            human faces.</li>
            </ol>
            <p>Given an image (it mainly works with grayscale images),
            the algorithm looks at many smaller subregions and tries to
            find a face by looking for <strong>specific features in each
            subregion</strong>. It needs to check many different
            positions and scales because an image can contain many faces
            of various sizes. It uses Haar-like features to detect faces
            in this algorithm.</p>
            <blockquote>
            <p>In the 19th century a Hungarian mathematician, Alfred
            Haar gave the concepts of <a
            href="https://en.wikipedia.org/wiki/Haar_wavelet">Haar
            wavelets</a>, which are a sequence of rescaled
            “square-shaped” functions which together form a wavelet
            family or basis.</p>
            </blockquote>
            <p><img src="img/object/viola_jones.webp" width="400"></p>
            <p><br></p>
            <p><a
            href="https://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/viola-cvpr-01.pdf">Read
            the original paper</a> by Viola and Jones for more
            details.</p>
            </section>
            <section id="deep-learning-methods" class="level2"
            data-number="12.2">
            <h2 data-number="12.2"><span
            class="header-section-number">12.2</span> Deep-Learning
            Methods</h2>
            <p>Many modern techniques use <a
            href="https://medium.com/@kattarajesh2001/convolutional-neural-networks-in-depth-c2fb81ebc2b2">convolutional
            neural networks</a> (CNNs) for object detection.</p>
            <p><a
            href="https://en.wikipedia.org/wiki/Deep_learning">Deep
            learning</a> uses <strong>neural networks</strong> to
            perform tasks such as classification, regression and
            representation learning. The field takes inspiration from
            biological neuroscience and is centered around stacking
            artificial neurons into layers and “training” them to
            process data. The adjective “deep” refers to the use of
            multiple layers (ranging from three to several hundred or
            thousands) in the network.</p>
            <p><img src="img/object/deep_learning.png" width="400"></p>
            <p>But first, a brief detour of CNNs…</p>
            <section id="convolutional-neural-networks-cnns"
            class="level3" data-number="12.2.1">
            <h3 data-number="12.2.1"><span
            class="header-section-number">12.2.1</span> Convolutional
            Neural Networks (CNNs)</h3>
            <p><br></p>
            <p><strong>CNNs</strong> are a class of deep learning neural
            networks that learns “features” via a “filter” (or kernel)
            optimization. They perform <strong><a
            href="https://en.wikipedia.org/wiki/Convolution">convolution
            operations</a></strong> at runtime → and are used in object
            detection to <strong>classify</strong> images from the
            camera.</p>
            <p><img src="img/object/cnn_intro.png" width="300"></p>
            <p>In mathematics, a “convolution” is an operation on two
            functions, <span class="math inline"><em>f</em></span> and
            <span class="math inline"><em>g</em></span> to produce a
            third function, <span
            class="math inline"><em>f</em> * <em>g</em></span> → as the
            <strong>integral of the product</strong> of the two
            functions after one is <strong>reflected</strong> about the
            y-axis and shifted.</p>
            <p><img src="img/object/equations/pngs/equations-1.png" width="700"></p>
            <p>Here is an <a
            href="https://betterexplained.com/articles/intuitive-convolution/">intuitive
            explanation of convolutions</a> for more information.</p>
            <p>Some <a
            href="https://en.wikipedia.org/wiki/Convolution">visual
            examples</a> of convolutions:</p>
            <p><img src="img/object/convolution.1.gif">
            <img src="img/object/convolution.2.gif"></p>
            <p>Though we are really interested in
            <strong>discrete</strong> convolutions → For complex-valued
            functions <span class="math inline"><em>f</em></span> and
            <span class="math inline"><em>g</em></span>, defined on the
            set <span class="math inline">ℤ</span> of integers, the
            discrete convolution of <span
            class="math inline"><em>f</em></span> and <span
            class="math inline"><em>g</em></span> is given by:</p>
            <p><span class="math display">$$
            (f * g)[n]=\sum_{m=-\infty}^{\infty} f[m] g[n-m],
            $$</span></p>
            <p>At a high level, this can be shown as:</p>
            <p><img src="img/object/discrete_convolution.gif" width="400"></p>
            <p><br></p>
            <p>For discrete sequences, for instance in digital signal
            processing, convolution involves,</p>
            <ul>
            <li>flipping one sequence</li>
            <li>shifting it across another</li>
            <li>multiplying corresponding elements and</li>
            <li>summing up the results over the range of overlap.</li>
            </ul>
            <p>So, the main idea is that convolution <strong>blends two
            functions</strong> and <strong>creates a third</strong>
            function which represents → how one function modifies the
            other function.</p>
            <p>When applied to CNNs, this concept shows how
            <strong>kernels</strong> (that act as
            <strong>filters</strong>) alter or transform input data.
            Hence, a kernel (aka “convolution matrix” or “mask”) is a
            small matrix used for certain operations, <em>e.g.,</em></p>
            <table>
            <colgroup>
            <col style="width: 31%" />
            <col style="width: 44%" />
            <col style="width: 24%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">operation</th>
            <th>kernel/matrix</th>
            <th>result</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;">identity</td>
            <td><span class="math inline">$\left[\begin{array}{lll}0
            &amp; 0 &amp; 0 \newline 0 &amp; 1 &amp; 0 \newline 0 &amp;
            0 &amp; 0\end{array}\right]$</span></td>
            <td><img src="img/object/kernel.1.png"></td>
            </tr>
            <tr>
            <td style="text-align: left;">ridge/edge detection</td>
            <td><span class="math inline">$\left[\begin{array}{rrr} -1
            &amp; -1 &amp; -1 \newline -1 &amp; 8 &amp; -1 \newline -1
            &amp; -1 &amp; -1\end{array}\right]$</span></td>
            <td><img src="img/object/kernel.2.png"></td>
            </tr>
            <tr>
            <td style="text-align: left;">sharpen</td>
            <td><span class="math inline">$\left[\begin{array}{rrr}-1
            &amp; -1 &amp; -1 \newline -1 &amp; 8 &amp; -1 \newline -1
            &amp; -1 &amp; -1\end{array}\right]$</span></td>
            <td><img src="img/object/kernel.3.png"></td>
            </tr>
            <tr>
            <td style="text-align: left;">gaussian blur</td>
            <td><span
            class="math inline">$\frac{1}{256}\left[\begin{array}{ccccc}1
            &amp; 4 &amp; 6 &amp; 4 &amp; 1 \newline4 &amp; 16 &amp; 24
            &amp; 16 &amp; 4 \newline6 &amp; 24 &amp; 36 &amp; 24 &amp;
            6 \newline4 &amp; 16 &amp; 24 &amp; 16 &amp; 4 \newline1
            &amp; 4 &amp; 6 &amp; 4 &amp; 1
            \end{array}\right]$</span></td>
            <td><img src="img/object/kernel.4.png"></td>
            </tr>
            <tr>
            <td style="text-align: left;">unsharp masking</td>
            <td><span
            class="math inline">$\frac{-1}{256}\left[\begin{array}{ccccc}1
            &amp; 4 &amp; 6 &amp; 4 &amp; 1 \newline4 &amp; 16 &amp; 24
            &amp; 16 &amp; 4 \newline6 &amp; 24 &amp; -476 &amp; 24
            &amp; 6 \newline4 &amp; 16 &amp; 24 &amp; 16 &amp; 4
            \newline1 &amp; 4 &amp; 6 &amp; 4 &amp;
            1\end{array}\right]$</span></td>
            <td><img src="img/object/kernel.5.png"></td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Hence, in its simplest form, convolution is:</p>
            <blockquote>
            <p>the process of adding each element of the image to its
            local neighbors, weighted by the kernel.</p>
            </blockquote>
            <p>The values of a given pixel in the output image →
            calculated by <strong>multiplying each kernel value by the
            corresponding input image pixel values</strong> denoted by
            the following pseudocode:</p>
            <pre><code>for each image row in input image:
    for each pixel in image row:

        set accumulator to zero

        for each kernel row in kernel:
            for each element in kernel row:

                if element position  corresponding* to pixel position then
                    multiply element value  corresponding* to pixel value
                    add result to accumulator
                endif

         set output image pixel to accumulator</code></pre>
            <p>The <strong>general</strong> form of a matrix
            convolution:</p>
            <p><span class="math display">$$
            \left[\begin{array}{cccc}
            x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 n} \newline
            x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 n} \newline
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \newline
            x_{m 1} &amp; x_{m 2} &amp; \cdots &amp; x_{m n}
            \end{array}\right] *\left[\begin{array}{cccc}
            y_{11} &amp; y_{12} &amp; \cdots &amp; y_{1 n} \newline
            y_{21} &amp; y_{22} &amp; \cdots &amp; y_{2 n} \newline
            \vdots &amp; \vdots &amp; \ddots &amp; \vdots \newline
            y_{m 1} &amp; y_{m 2} &amp; \cdots &amp; y_{m n}
            \end{array}\right]=\sum_{i=0}^{m-1} \sum_{j=0}^{n-1}
            x_{(m-i)(n-j)} y_{(1+i)}
            $$</span></p>
            <p>When a specific kernel is applied to an image, it
            <strong>modifies or transforms the image</strong> in a way
            that highlights or emphasizes the feature that the kernel is
            specialized to detect. This transformation effectively
            creates a new representation of the original image, focusing
            on the specific feature encoded by the applied kernel.</p>
            <p>Kernels come in various shapes:</p>
            <p><img src="img/object/kernel.6.png" width="500"></p>
            <p><strong>CNNs and kernels</strong></p>
            <p>In CNNs, we <strong>do not hand code</strong> these
            kernels to extract features. The neural network itself
            <strong>learns these kernels</strong> or filters to extract
            different features. The choice of which features to extract
            is up to the model → whatever feature it wants to extract,
            the CNN will <strong>learn</strong> the kernel that extracts
            that particular feature.</p>
            <p>These “learned” kernels act as specialized filters that
            modify the input, highlighting specific patterns or
            structures, thereby enabling the network to learn and
            discern various features. This is essential for tasks like
            image recognition, object detection, <em>etc.</em></p>
            <p>Let’s see a simple example. Consider a small patch of an
            image of a car:</p>
            <p><img src="img/object/car.1.png" width="300"></p>
            <p><br></p>
            <p>This image consists of <strong>three color
            channels</strong> (R, G, B):</p>
            <p><img src="img/object/car.rgb.png" width="300"></p>
            <p><br></p>
            <p>But, for simplicity, let’s consider a
            <strong>grayscale</strong> image first:</p>
            <p><img src="img/object/car.2.png" width="300"></p>
            <p><br></p>
            <p>This is what one version of a covolution could look
            like:</p>
            <p><img src="img/object/car.grayscale.1.png" width="400"></p>
            <p><br></p>
            <p>If we run it forward, this is what the result looks
            like:</p>
            <p><img src="img/object/car.convolution.gif" width="400"></p>
            <p><br></p>
            <p>But…we deal with <strong>color</strong> images and
            <strong>three</strong> channels!</p>
            <p><img src="img/object/pencils.jpg" width="300"></p>
            <p><br></p>
            <p>So, we have to deal with,</p>
            <p><img src="img/object/car.rgb.png" width="300"></p>
            <p><br></p>
            <p>The solution is simple…apply the kernel to
            <strong>eah</strong> of the channels!</p>
            <p><img src="img/object/convolution.0.gif" width="400"></p>
            <p><br></p>
            <p>Then we need to <strong>combine</strong> them (usually
            <strong>summed up</strong>) to get a <strong>single output
            value</strong> for that position.</p>
            <p><img src="img/object/car.rgb_output.gif" width="500"></p>
            <p>The “<a
            href="https://www.turing.com/kb/necessity-of-bias-in-neural-networks#">bias</a>”
            helps in shifting the activation function and influences the
            feature maps’ outputs. This is a <strong>constant</strong>
            that is added to the product of features and weights. It is
            used to <strong>offset the result</strong>. It helps the
            models to shift the activation function towards the positive
            or negative side.</p>
            <p><strong>Additional Definitions</strong></p>
            <p>Let’s quickly discuss some additional definitions that
            are relevant to CNNs.</p>
            <ol type="1">
            <li><strong>convolutional layer</strong> consists of
            <strong>multiple convolutional neurons</strong> → each
            neuron has it own filter. Each kernel corresponds to a
            specific feature or pattern the layer aims to detect within
            the input data. These kernels,
            <ul>
            <li>slide spatially across the input data’s width and
            height</li>
            <li>independently across each input channel</li>
            <li>performing convolutions.</li>
            </ul></li>
            </ol>
            <p><img src="img/object/convolution.3.gif" width="500"></p>
            <p><br></p>
            <ol start="2" type="1">
            <li><strong>stride</strong> describes the
            <strong>step</strong> for the movement of a kernel. This is
            best described by the following diagrams:</li>
            </ol>
            <p><img src="img/object/stride.1.gif" width="300">
            <img src="img/object/stride.2.gif" width="300"></p>
            <p><br></p>
            <p>Effects of different stride value:</p>
            <ul>
            <li>larger stride value increases step size → a smaller
            output size spatially</li>
            <li>stride of <span class="math inline">1</span> → preserves
            spatial dimensions more accurately</li>
            </ul>
            <ol start="3" type="1">
            <li><strong>padding</strong> is the addition of
            <strong>extra border pixels</strong> around input data
            <em>before</em> applying the convolutions. It helps to
            preserve the original spatial dimensions of the input data
            and retains the integrity of border pixels.</li>
            </ol>
            <p><img src="img/object/padding.gif" width="500"></p>
            <p><br></p>
            <ol start="4" type="1">
            <li><strong>pooling</strong> involves
            <strong>downsampling</strong> to reduce the spatial
            dimensions of feature maps obtained via convolutions → while
            <strong>preserving essential information</strong>. This
            reduces computational loads on the system. Several “pooling
            functions” exist, <em>e.g.,</em> average of the rectangular
            neighborhood, L2 norm of the rectangular neighborhood and a
            weighted average based on the distance from the central
            pixel. The most popular one is <strong>max pooling</strong>
            → it extracts the <strong>maximum value</strong> from each
            local region within the input feature map,
            <strong>highlighting the most prominent
            features</strong>.</li>
            </ol>
            <p><img src="img/object/max_pool.gif" width="400"></p>
            <p><br></p>
            <p>Coming back to our car example from before, we define a
            CNN with <strong>multiple layers</strong> of neurons that
            could look like,</p>
            <p><img src="img/object/car_cnn_complete.png" width="700"></p>
            <p><br></p>
            <p>This begs the question → why multiple layers? Usually
            each layer has its own unique function,</p>
            <p><img src="img/object/cnn_layers.png" height="200"></p>
            <p>The <a
            href="https://en.wikipedia.org/wiki/Softmax_function">Softmax
            function</a> converts a vector of <span
            class="math inline"><em>K</em></span> real numbers into a
            <strong>probability distribution</strong> of <span
            class="math inline"><em>K</em></span> <strong>possible
            outcomes</strong>. The Softmax function is often used as the
            last activation function of a neural network to normalize
            the output of a network to a probability distribution over
            predicted output classes.</p>
            <p>The <a
            href="https://medium.com/swlh/fully-connected-vs-convolutional-neural-networks-813ca7bc6ee5">fully
            connected (FC)</a> layer connect <strong>every</strong>
            neuron in one layer to every neuron in the other layer. The
            purpose of this is to combine our features into more
            attributes to predict the classes even better. In fact,
            combining more attributes (e.g. edge detect, blur detect,
            emboss detect) help to predict better the images.</p>
            <p><strong>Read more details about CNNs</strong> → <a
            href="https://medium.com/@kattarajesh2001/convolutional-neural-networks-in-depth-c2fb81ebc2b2">1</a>,
            <a
            href="https://medium.com/@kattarajesh2001/convolutional-neural-network-from-scratch-0d7513d62923">2</a>.</p>
            </section>
            <section id="cnns-and-object-detection" class="level3"
            data-number="12.2.2">
            <h3 data-number="12.2.2"><span
            class="header-section-number">12.2.2</span> CNNs and Object
            Detection</h3>
            <p>Regular CNNs can be used for object detection in camera
            images. But there is one challenge → they’re
            <strong>limited</strong> to single objects taking up the
            enitre image. Clearly this is not always the case.</p>
            <p>So what happens if we have,</p>
            <ul>
            <li>smaller objects (along with backgrounds)</li>
            <li>multiple objects?</li>
            </ul>
            <p>the solution → <strong>sliding windows</strong>! We break
            the image down into smaller segments and send each through
            the CNN-based framework.</p>
            <p><img src="img/object/sliding_window.1.gif">
            <img src="img/object/sliding_window.2.gif"></p>
            <p><br></p>
            <p>As we slide the window over the image, we take the
            resulting image patch and run it through the convolutional
            neural network to see if it corresponds to any possible
            object.</p>
            <ol type="1">
            <li>a window of fixed size slides across the image, pixel by
            pixel</li>
            <li>at each position, a classifier determines if the window
            contains an object of interest</li>
            <li>the process repeats with windows of different sizes to
            detect objects at various scales.</li>
            </ol>
            <p>In the second figure above, if it’s just an image of the
            road or the sky, it would be a false prediction. If it’s an
            image of a car or a person, it would return as a true
            prediction.</p>
            <p>There are some issues to consider:</p>
            <ul>
            <li>what is the right <strong>window size</strong> → do we
            start from a small window and keep increasing?</li>
            <li>the bounding box generated by the algorithm <strong>may
            not be accurate</strong> → trade-off between accuracy (fine
            stride) and efficiency (coarse stride)</li>
            <li><strong>computationally expensive</strong> → due to the
            large number of windows</li>
            </ul>
            <p>Hence, multiple techniques have been developed over the
            years and we discuss some of them here.</p>
            </section>
            </section>
            <section id="object-detection-parameters" class="level2"
            data-number="12.3">
            <h2 data-number="12.3"><span
            class="header-section-number">12.3</span> Object Detection |
            Parameters</h2>
            <p>As mentioned earlier, there are two distinct problems to
            be solved:</p>
            <ul>
            <li><strong>localization</strong> → drawing boxes around the
            objects so that the computer understand
            <strong>where</strong> things are in a picture.</li>
            <li><strong>classification</strong> → <strong>labels each
            object</strong>. So, if there’s a car in the image, it
            doesn’t just know it is there; it knows what exactly is that
            <em>i.e.,</em> a car.</li>
            </ul>
            <p>There are some basic parameters we need to define for
            object localization:</p>
            <ol type="1">
            <li><strong>midpoint</strong> <span
            class="math inline">(<em>b</em><sub><em>x</em></sub>, <em>b</em><sub><em>y</em></sub>)</span>
            → the <strong>center</strong> of the bounding box</li>
            <li><strong>height</strong> and <strong>width</strong> <span
            class="math inline">(<em>b</em><sub><em>h</em></sub>, <em>b</em><sub><em>w</em></sub>)</span>
            → dimensions of the bounding box</li>
            </ol>
            <p><img src="img/object/obj.1.png"></p>
            <p><br></p>
            <p>We also need to define additional labels → in order for
            the machine to localize objects, <em>viz.</em>,</p>
            <ol start="3" type="1">
            <li><strong>object probability</strong> <span
            class="math inline">(<em>p</em><sub><em>c</em></sub>)</span>
            → A binary indicator (<code>1</code> or <code>0</code>)
            denoting whether an object of interest is present</li>
            <li><strong>class labels</strong> → indicates the
            <strong>category</strong> of the object (<em>e.g.,</em> car,
            pedestrian, motorcycle)</li>
            </ol>
            <p>Let’s consider the following example → we need to
            <em>localize</em> objects (a car in this case) in the
            following image:</p>
            <p><img src="img/object/obj.2.png" width="400"></p>
            <p><br></p>
            <p>Note that our goal is not only to identify that there is
            an object in the image but also to <strong>precisely locate
            it</strong> using a <strong>bounding box</strong>. The
            picture lists the <strong>output components</strong> for our
            system, <em>viz.,</em></p>
            <ul>
            <li><span
            class="math inline"><em>p</em><sub><em>c</em></sub></span> →
            is a particular component present in the image? In this
            example, we assign,</li>
            </ul>
            <table>
            <thead>
            <tr>
            <th>class probabilities</th>
            <th style="text-align: left;">object</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>p</em><em>c</em><sub>1</sub></span></td>
            <td style="text-align: left;">car</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>p</em><em>c</em><sub>2</sub></span></td>
            <td style="text-align: left;">pedestrian</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>p</em><em>c</em><sub>3</sub></span></td>
            <td style="text-align: left;">motorcycle</td>
            </tr>
            <tr>
            <td></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>In this example, <span
            class="math inline"><em>p</em><sub><em>c</em></sub> = 1</span>
            (since we find one object, the car) and <span
            class="math inline"><em>p</em><em>c</em><sub>1</sub> = 1</span>,
            <span
            class="math inline"><em>p</em><em>c</em><sub>2</sub> = 0</span>,
            <span
            class="math inline"><em>p</em><em>c</em><sub>3</sub> = 0</span>
            since only a car was found and no pedestrians or
            motorcycles.</p>
            <p>We interpret this as → if <span
            class="math inline"><em>p</em><sub><em>c</em></sub>​ = 1</span>,
            the model indicates the presence of an object and the
            bounding box parameters <span
            class="math inline">(<em>b</em><sub><em>x</em></sub>, <em>b</em><sub><em>y</em></sub>, <em>b</em><sub><em>h</em></sub>, <em>b</em><sub><em>w</em></sub>​)</span>
            <strong>precisely locate the object</strong>. The class
            probabilities <span
            class="math inline">(<em>p</em><em>c</em><sub>1</sub>, <em>p</em><em>c</em><sub>2</sub>, <em>p</em><em>c</em><sub>3</sub>)</span>
            convey the <strong>likelihood of the object belonging to
            each class</strong>.</p>
            <p><img src="img/object/obj.3.png" width="400">
            <img src="img/object/obj.4.png" width="400"></p>
            <p><br></p>
            <p>If <span
            class="math inline"><em>p</em><sub><em>c</em></sub> = 0</span>,
            no specified objects are present in the image and the
            remaining parameters become “<strong>don’t
            cares</strong>”.</p>
            <p>The <strong>loss function</strong> for object
            localization penalizes the difference between predicted
            output (<span class="math inline"><em>ŷ</em></span>) and
            ground truth label (<span
            class="math inline"><em>y</em></span>):</p>
            <table>
            <tbody>
            <tr>
            <td style="text-align: left;"><span
            class="math inline"><em>p</em><sub><em>c</em></sub> = 1</span>
            <br> (object present)</td>
            <td style="text-align: left;">loss considers
            <strong>all</strong> components</td>
            </tr>
            <tr>
            <td style="text-align: left;"><span
            class="math inline"><em>p</em><sub><em>c</em></sub> = 0</span>
            <br> (no object),</td>
            <td style="text-align: left;">loss focuses
            <strong>only</strong> on <span
            class="math inline"><em>p</em><sub><em>c</em></sub></span></td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Re-visiting the Object Localization vs. Object
            Detection issue</strong></p>
            <p>The key difference between object localization and object
            detection lies in handling multiple objects:</p>
            <ul>
            <li><strong>object localization</strong> typically handles a
            <strong>single object</strong> or a <strong>fixed
            number</strong> of objects in an image</li>
            <li><strong>object detection</strong> can identify and
            locate an <strong>arbitrary number of objects</strong>
            within an image.</li>
            </ul>
            <p>Object detection introduces a more <strong>dynamic
            approach</strong> to identifying and localizing objects →
            overcoming limitations of fixed-box specifications inherent
            in object localization.</p>
            <p>The <a
            href="https://medium.com/@kattarajesh2001/object-detection-part-1-introduction-to-object-detection-321f1fd56295">Sliding
            Windows</a> approach discussed earlier is one example of
            object detection.</p>
            <p><strong>[TO BE COMPLETED!]</strong></p>
            </section>
            <section id="objectobstacle-avoidance" class="level2"
            data-number="12.4">
            <h2 data-number="12.4"><span
            class="header-section-number">12.4</span> Object/obstacle
            Avoidance</h2>
            <p>So far, we have looked at methods to
            <strong>detect</strong> and <strong>identify</strong>
            objects in our path. The main goal for this process is to
            ensure that we <em>do not collide</em> with the objects in
            our path, either by <strong>stopping</strong> or taking an
            <strong>alternate path</strong> → <strong>object (or
            obstacle) avoidance</strong>.</p>
            <p>To <a
            href="https://en.wikipedia.org/wiki/Obstacle_avoidance#">define
            obstacle avoidance</a>,</p>
            <blockquote>
            <p>It is the capability of a robot or an autonomous
            system/machine to <strong>detect and circumvent
            obstacles</strong> in its path to reach a predefined
            destination.</p>
            </blockquote>
            <p>The most obvious way to implement obstacle avoidance is
            to just use the <strong>sensors</strong> and actively react
            to obstacles and recalculate new paths, as shown below:</p>
            <p><img src="img/object/avoidance/wiki_robot.gif" width="300"></p>
            <p><br></p>
            <p>They follow <strong>three simple steps</strong>:</p>
            <ul>
            <li>sense</li>
            <li>think</li>
            <li>act.</li>
            </ul>
            <p>They take in inputs of distances in objects and provide
            the robot with data about its surroundings enabling it to
            detect obstacles and calculate their distances. The robot
            then adjusts its trajectory to navigate around obstacles
            while trying to reach its destination. This can be carried
            out in real-time.</p>
            <p>While this can work in many simple instances (like the
            example in the figure), for more complicated situations,
            <em>e.g.,</em> an autonomous car in an urban environment –
            especially when the destination may be far away and it is
            not obvious that blindly trying to find a way around the
            obstacles using sensors may be beneficial.</p>
            <p>Contemporary obstacle detection methods span a variety of
            methods, <em>viz.,</em></p>
            <ul>
            <li>reactive strategies</li>
            <li>global planners</li>
            <li>machine-learning based methods</li>
            </ul>
            <p><strong>Note:</strong> that a lot of obstacle avoidance
            methods overlap with the <a href="#path-planning">path
            planning</a> methods we discussed earlier – <em>e.g.,</em>
            the <a href="#artificial-potential-field-apf">artifical
            potential field (APF)</a> algorithm, <a
            href="#a-and-d-search">A* and D* searches</a> and <a
            href="#rapidly-exploring-random-tree-rrt-algorithm">RRT</a>
            methods – since their main goal is to find paths through an
            obstacle field as well.</p>
            <p>For instance, consider the weighted A* example in the
            following figure:</p>
            <p><img src="img/object/avoidance/Weighted_A_star_with_eps_5.gif" width="300"></p>
            <p><br></p>
            <p>Now. let’s look at some examples of these classes of
            algorithms.</p>
            <section id="classicalgeometric-methods" class="level3"
            data-number="12.4.1">
            <h3 data-number="12.4.1"><span
            class="header-section-number">12.4.1</span>
            Classical/Geometric Methods</h3>
            <p>These include methods that track the geometry of the
            space using various “physics-based” concepts, <em>e.g.,</em>
            the <a href="#artificial-potential-field-apf">APF</a> method
            discussed earlier. Other well-known methods in this category
            include:</p>
            <p><strong><a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=88137">Vector-Field
            Histogram (VFH)</a></strong> → that builds a
            <strong>histogram of obstacle densities</strong> and chooses
            low-density paths.</p>
            <p>The algorithm:</p>
            <ul>
            <li>computes obstacle-free steering directions for a robot
            based on range sensor readings</li>
            </ul>
            <p><img src="img/object/avoidance/vfh.range_readings.png" width="300"></p>
            <p><br></p>
            <ul>
            <li>readings are used to compute <strong>polar density
            histograms</strong> → to identify obstacle location and
            proximity</li>
            </ul>
            <p><img src="img/object/avoidance/vfh.histogram_density.png" width="300"></p>
            <p><br></p>
            <ul>
            <li>based on specified parameters and thresholds, the
            histograms are converted to binary histograms → to indicate
            valid steering directions for the robot.</li>
            </ul>
            <p><img src="img/object/avoidance/vhf.histograms.png" width="300"></p>
            <p><br></p>
            <p>Read the <a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=88137">original
            paper</a> and <a
            href="https://web.eecs.utk.edu/~leparker/Courses/CS594-fall08/Lectures/Oct-21-Obstacle-Avoidance-I.pdf">more
            details</a>.</p>
            <p><br></p>
            <p><strong><a
            href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf">Dynamic
            Window Approach</a></strong> → this method <strong>samples
            the velocity space</strong> and selects safe trajectories
            based on dynamic constraints.</p>
            <p>This approach correctly, and in an elegant manner,
            incorporates the <strong>dynamics</strong> of the robot → by
            reducing the search space to the dynamic window, which
            consists of the <strong>velocities reachable within a short
            time interval</strong>.</p>
            <p>Within this dynamic window the approach only considers
            admissible velocities yielding a trajectory on which the
            robot can <strong>stop safely</strong>. An objective
            function considers the <strong>distance to the next obstacle
            approach</strong>.</p>
            <p>Read the <a
            href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf">original
            paper</a> for more details.</p>
            </section>
            <section id="model-predictive-control-mpc" class="level3"
            data-number="12.4.2">
            <h3 data-number="12.4.2"><span
            class="header-section-number">12.4.2</span> Model Predictive
            Control (MPC)</h3>
            <p>MPC is an advanced method of process control that is used
            to control a process while <strong>satisfying a set of
            constraints</strong>. It allows the <strong>current
            timeslot</strong> to be optimized, while keeping future
            timeslots in account. This is achieved by,</p>
            <ul>
            <li>optimizing a <strong>finite time-horizon</strong> → but
            only implementing the current timeslot and</li>
            <li>then <strong>optimizing again</strong>, repeatedly.</li>
            </ul>
            <p>MPC can <strong>predict future events</strong> → and
            react accordingly (as opposed to plain PID).</p>
            <p><img src="img/object/avoidance/MPC_scheme_basic.svg" width="400"></p>
            <p><br></p>
            <p>MPC is <a
            href="https://en.wikipedia.org/wiki/Model_predictive_control">based
            on</a> <strong>iterative, finite-horizon
            optimization</strong> of a plant model. At time, <span
            class="math inline"><em>t</em></span>,</p>
            <ul>
            <li>the current plant state is sampled</li>
            <li>a cost minimizing control strategy is computed (via a
            numerical minimization algorithm)</li>
            <li>for a relatively short time horizon in the future, <span
            class="math inline">[<em>t</em> + <em>T</em>]</span></li>
            </ul>
            <p>Specifically, an <strong>online or on-the-fly
            calculation</strong> is used to explore state trajectories
            that emanate from the current state and find (via the
            solution of <a
            href="https://en.wikipedia.org/wiki/Euler–Lagrange_equation">Euler–Lagrange
            equations</a>) → a cost-minimizing control strategy until
            time <span
            class="math inline">[<em>t</em> + <em>T</em>]</span>.</p>
            <ul>
            <li>only the <strong>first step of the control strategy is
            implemented</strong></li>
            <li>plant state is <strong>sampled again</strong></li>
            <li>calculations are <strong>repeated starting from the new
            current state</strong></li>
            <li>yielding a <strong>new control</strong> and <strong>new
            predicted state path</strong>.</li>
            </ul>
            <p>The prediction <strong>horizon keeps being shifted
            forward</strong> (MPC is also called <em>receding horizon
            control</em>).</p>
            <p><strong>Note:</strong> while MPC is not optimal, in
            practice it has shown very good results.</p>
            <p><strong>Applying MPC to obstacle avoidance</strong>
            depends on the fact that → MPC systems rely on
            <strong>dynamic models</strong> of the process,</p>
            <p><img src="img/object/avoidance/mpc.model.png" width="300"></p>
            <p><br></p>
            <p>Once a model has been established, we can set up a
            control loop as follows:</p>
            <p><img src="img/object/avoidance/mpc.controller.png" width="300"></p>
            <p><br></p>
            <p>Given a reference command, <span
            class="math inline"><strong>r</strong></span>, the
            controller generates high rate <strong>vehicle
            commands</strong>, <span
            class="math inline"><strong>u</strong></span> to close the
            loop with vehicle dynamics.</p>
            <p>This computes the <strong>predicted state
            trajectory</strong>, <span
            class="math inline"><strong>x</strong>(<em>t</em>)</span>.</p>
            <p>The feasibility of this output is checked against
            <strong>vehicle and environmental constraints</strong>, such
            as rollover and obstacle avoidance constraints.</p>
            <p>MPC often works with path planning algorithms (such as
            RRT) → <em>e.g.,</em> <a
            href="https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&amp;isAllowed=y">CL-RRT</a>,</p>
            <ul>
            <li>the CL-RRT algorithm grows a <strong>tree of feasible
            trajectories</strong> (using RRT) → originating from the
            current vehicle state</li>
            <li>attempts to reach a specified goal set</li>
            <li>at the end of the tree growing phase → <strong>best
            trajectory</strong> is chosen for execution</li>
            <li>cycle repeats.</li>
            </ul>
            <p>The quality of the results depends on the
            <strong>sampling strategies</strong>. Sampling the space in
            a purely random manner could result in large numbers of
            wasted samples due to the numerous constraints. Many methods
            have been proposed for this purpose.</p>
            <p>Some examples:</p>
            <table>
            <colgroup>
            <col style="width: 37%" />
            <col style="width: 37%" />
            <col style="width: 25%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">situation</th>
            <th style="text-align: left;">details</th>
            <th style="text-align: center;">image</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;">intersection</td>
            <td style="text-align: left;">vehicle trying to make a right
            turn</td>
            <td
            style="text-align: center;"><img src="img/object/avoidance/clrrt.left_turn.png" width="200"></td>
            </tr>
            <tr>
            <td style="text-align: left;">parking lot</td>
            <td style="text-align: left;">goal is center right edge</td>
            <td
            style="text-align: center;"><img src="img/object/avoidance/clrrt.parking_lot.png" width="200"></td>
            </tr>
            <tr>
            <td style="text-align: left;">u-turn</td>
            <td style="text-align: left;">facing (red) road blockage
            <br> white and blues samples are forward/back
            maneouvers</td>
            <td
            style="text-align: center;"><img src="img/object/avoidance/clrrt.uturn.png" width="200"></td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: center;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>Read the <a
            href="https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&amp;isAllowed=y">original
            CLRRT paper</a> for more details and references.</p>
            <p><br></p>
            </section>
            <section id="learning-based-methods" class="level3"
            data-number="12.4.3">
            <h3 data-number="12.4.3"><span
            class="header-section-number">12.4.3</span> Learning-Based
            Methods</h3>
            <p>With the advent of ML/AI techniques, an autonomous
            vehicle can trace a path to its destination using massive
            amounts of data. It can also <strong>adapt quickly</strong>
            to changing scenarios/environments. It can achieve this
            using many testing stages on large data sets of obstacles
            and environmental conditions.</p>
            <p>ML-based solutions can even be <strong><a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8202134">mapless</a></strong>.
            Traditional motion planners for mobile ground robots with
            range sensors (<em>e.g.,</em> LiDAR) mostly depend on the
            obstacle map of the navigation environment where both,</p>
            <ul>
            <li>the highly precise laser sensor and</li>
            <li>the obstacle map building work of the environment</li>
            </ul>
            <p>are indispensable.</p>
            <p>Using an asynchronous deep reinforcement learning method,
            a “mapless” motion planner can be trained end-to-end
            <strong>without any manually designed features and prior
            demonstrations</strong>!</p>
            <ol type="1">
            <li><strong><a
            href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Reinforcement
            Learning</a></strong> is a <strong>computational
            approach</strong> to <strong>learning from
            interaction</strong>. These methods are focused on
            <strong>goal-directed learning from
            interaction</strong>.</li>
            </ol>
            <blockquote>
            <p>Reinforcement learning problems involve learning what to
            do—how to map situations to actions – so as to
            <strong>maximize a numerical reward signal</strong>. In an
            essential way they are <strong>closed-loop</strong> problems
            because the learning system’s actions influence its later
            inputs. Moreover, the <strong>learner is not told which
            actions to take</strong>, as in many forms of machine
            learning, but instead must <strong>discover which actions
            yield the most reward by trying them out</strong>. In the
            most interesting and challenging cases, actions may affect
            not only the immediate reward but also the next situation
            and, through that, <strong>all subsequent
            rewards</strong>.</p>
            </blockquote>
            <p>Three most important aspects of RL:</p>
            <ol type="1">
            <li><strong>closed-loop</strong> in an essential way</li>
            <li><strong>no direct instructions</strong> as to what
            actions to take</li>
            <li>consequences of actions, including reward signals, play
            out over <strong>extended time periods</strong>.</li>
            </ol>
            <p>One of the challenges → the <strong>trade-off between
            exploration and exploitation</strong>. To obtain a lot of
            reward, a reinforcement learning agent must prefer actions
            that it has <strong>tried in the past and found to be
            effective</strong> in producing reward. But to discover such
            actions, it has to <strong>try actions that it has not
            selected before</strong>. The agent has to exploit what it
            already knows in order to obtain reward, but it also has to
            explore in order to make better action selections in the
            future.</p>
            <p>A key feature of RL is that it explicitly considers the
            <strong>whole problem</strong> of a goal-directed agent
            interacting with an uncertain environment → this is a
            perfect analogy for path finding/obstacle detection.</p>
            <p>RL is really an interdisciplinary area of machine
            learning and optimal control.</p>
            <p>Consider this <a
            href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">example</a>:</p>
            <blockquote>
            <p>a mobile robot decides whether it should enter a new room
            in search of more trash to collect or start trying to find
            its way back to its battery recharging station. It makes its
            decision based on the current charge level of its battery
            and how quickly and easily it has been able to find the
            recharger in the past.</p>
            </blockquote>
            <p>Here is a <a
            href="https://en.wikipedia.org/wiki/Reinforcement_learning">typical
            framing of the RL problem</a>:</p>
            <p><img src="img/object/avoidance/Reinforcement_learning_diagram.svg" width="300"></p>
            <p><br></p>
            <p>In the above example, an agent takes actions in an
            environment, which is interpreted into a reward and a state
            representation, which are fed back to the agent.</p>
            <p>The simplest model for RL uses a <a
            href="https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf">Markov
            Decision Process (MDP)</a> → <em>i.e.,</em> optimization
            models for modeling decision-making in situations where
            outcomes are random, <em>viz.,</em></p>
            <ul>
            <li>a set of <strong>environment</strong> and <strong>agent
            states</strong> (the state space), <span
            class="math inline"><em>S</em></span></li>
            <li>a set of <strong>actions</strong> (the action space),
            <span class="math inline"><em>A</em></span>, of the
            agent</li>
            <li><span
            class="math inline"><em>P</em><sub><em>a</em></sub>(<em>s</em>, <em>s</em><sup>′</sup>) = Pr (<em>S</em><sub><em>t</em> + 1</sub> = <em>s</em><sup>′</sup> ∣ <em>S</em><sub><em>t</em></sub> = <em>s</em>, <em>A</em><sub><em>t</em></sub> = <em>a</em>)</span>
            → the transition probability (at time <span
            class="math inline"><em>t</em></span>) from state <span
            class="math inline"><em>s</em></span> to <span
            class="math inline"><em>s</em><sup>′</sup></span> under
            action <span class="math inline"><em>a</em></span></li>
            <li><span
            class="math inline"><em>R</em><sub><em>a</em></sub>(<em>s</em>, <em>s</em><sup>′</sup>)</span>
            → the immediate reward after transition from <span
            class="math inline"><em>s</em></span> to <span
            class="math inline"><em>s</em><sup>′</sup></span> under
            action <span class="math inline"><em>a</em></span>.</li>
            </ul>
            <p>The purpose of RL → agent to learn an optimal (or
            near-optimal) policy that <strong>maximizes reward
            function</strong> or other (user-provided) reinforcement
            signal that accumulates from immediate rewards.</p>
            <p>A basic reinforcement learning agent interacts with its
            environment in <strong>discrete time steps</strong>. At each
            time step <span class="math inline"><em>t</em></span>,</p>
            <ul>
            <li>the agent receives the current state <span
            class="math inline"><em>S</em><sub><em>t</em></sub></span>
            and reward <span
            class="math inline"><em>R</em><sub><em>t</em></sub></span></li>
            <li>it chooses an action <span
            class="math inline"><em>A</em><sub><em>t</em></sub></span>
            from the set of available actions
            <ul>
            <li>subsequently sent to the environment</li>
            </ul></li>
            <li>environment moves to a new state <span
            class="math inline"><em>S</em><sub><em>t</em> + 1</sub></span></li>
            <li>the reward <span
            class="math inline"><em>R</em><sub><em>t</em> + 1</sub></span>
            associated with transition <span
            class="math inline">(<em>S</em><sub><em>t</em></sub>, <em>A</em><sub><em>t</em></sub>, <em>S</em><sub><em>t</em> + 1</sub>)</span>
            is determined</li>
            </ul>
            <p><br></p>
            <p>Hence, the main <strong>goal</strong> of RL is to
            <strong>learn a “policy”</strong>,</p>
            <p><span
            class="math display"><em>π</em> : 𝒮 × 𝒜 → [0, 1], <em>π</em>(<em>s</em>, <em>a</em>) = Pr (<em>A</em><sub><em>t</em></sub> = <em>a</em> ∣ <em>S</em><sub><em>t</em></sub> = <em>s</em>)</span></p>
            <p>that <strong>maximizes the expected cumulative
            reward</strong>.</p>
            <p>Read the book, <a
            href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">“Reinforcement
            Learning: An Introduction”</a> by Sutton and Barto for a
            more detailed introduction to RL.</p>
            <p>Here is a <a
            href="https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf">primer
            on MDP</a> by Wang.</p>
            <p><br></p>
            <p><strong>RL Applied to Obstacle Detection</strong></p>
            <p>RL lends itself very nicely to the process of obstacle
            avoidance → <em>i.e.,</em> finding a path through an area
            with multiple obstacles. It comes to defining the
            <strong>right reward function</strong>.</p>
            <p>The following image shows a high-level flow of using
            (asynchronous) RL → for a robot to find its way through
            unfamiliar terrain.</p>
            <p><img src="img/object/avoidance/rl.example.png" width="400"></p>
            <p><br></p>
            <p>Read the paper, <a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8202134">“Virtual-to-real
            Deep Reinforcement Learning: Continuous Control of Mobile
            Robots for Mapless Navigation</a> by Tai et al. for more
            details.</p>
            <p><br></p>
            <ol start="2" type="1">
            <li><strong><a
            href="https://arxiv.org/pdf/1604.07316">Imitation
            Learning</a></strong> attempts to learn policies
            <strong>from human demonstrations</strong>.</li>
            </ol>
            <p>For instance, a system <strong>automatically learns
            internal representations</strong> of the necessary
            processing steps (<em>e.g.,</em> detecting useful road
            features) using <strong>only human steering angles</strong>
            as training signal!</p>
            <p>Here is a high-level diagram of the process:</p>
            <p><img src="img/object/avoidance/imitation_learning.example.png" width="400"></p>
            <p><br></p>
            <p>The steps are:</p>
            <ul>
            <li>images are fed into a CNN</li>
            <li>computes a proposed steering command</li>
            <li>proposed command is compared to the desired command for
            that image</li>
            <li>weights of the CNN are adjusted → to bring the CNN
            output closer to desired output</li>
            <li>weight adjustment is accomplished using back
            propagation.</li>
            </ul>
            <p>Once trained, the network can generate steering from the
            video images of a <strong>single center camera</strong>!</p>
            <p>Read the paper, <a
            href="https://arxiv.org/pdf/1604.07316">End to End Learning
            for Self-Driving Cars</a> by Bojarski et al. for more
            details.</p>
            </section>
            <section id="trajectory-calculations" class="level3"
            data-number="12.4.4">
            <h3 data-number="12.4.4"><span
            class="header-section-number">12.4.4</span> Trajectory
            Calculations</h3>
            <p>Let’s look at one <a
            href="https://ieeexplore.ieee.org/document/8519525">example</a>
            where mathematical (“sigmoid functions”) are used to
            estimate a smooth trajectory for avoiding an obstacle → in
            this case a moving car that’s ahead of you in the same
            lane.</p>
            <details>
            <summary>
            Sigmoid functions
            </summary>
            <p>A <a
            href="https://en.wikipedia.org/wiki/Sigmoid_function">Sigmoid
            function</a> is an <strong>S-shaped mathematical
            function</strong> that <strong>maps any input</strong> value
            to an output between <code>0</code> and <code>1</code>.</p>
            <p><img src="img/object/avoidance/sigmoid.svg" width="300"></p>
            <p><br></p>
            <p>The most common sigmoid function is the logistic
            function:</p>
            <p><span class="math display">$$
            \sigma(x) = \frac{1}{1 + e^{-x}} = \frac{e^x}{1 + e^x} = 1 -
            \sigma(-x)
            $$</span></p>
            <p>Key properties of sigmoid functions:</p>
            <ul>
            <li>smooth, continuous curve</li>
            <li>utput range limited to <span
            class="math inline">(0, 1)</span></li>
            <li>approaches <code>0</code> as <span
            class="math inline"><em>x</em> → −∞</span></li>
            <li>approaches <code>1</code> as <span
            class="math inline"><em>x</em> → +∞</span></li>
            <li>has a derivative that is <strong>always
            positive</strong></li>
            <li>steepest slope occurs at <span
            class="math inline"><em>x</em> = 0</span></li>
            </ul>
            </details>
            <p><br></p>
            <p>This method avoids vehicles/obstacles by proposing a
            <strong>smooth local modified trajectory</strong> of a
            global path. They use a combination of,</p>
            <ul>
            <li>a parametrized sigmoid function and</li>
            <li>a rolling horizon (a time-dependant model is solved
            repeatedly).</li>
            </ul>
            <p>The main idea is to react to the obstacles but also to
            ensure a <strong>smooth</strong> response/trajectory. This
            is a <strong>local</strong> method that can work in
            conjunction with a global path planning/obstacle avoidane
            method. The reference trajectory is calculated
            simultaneously when the displacement is started.</p>
            <p>One of the main considerations → <strong>execution
            time</strong> since it is imperative that the solution be
            calculated and implemented in a reall short amount of
            time.</p>
            <p>Consider the following Sigmoid function:</p>
            <p><span class="math display">$$
            y(x) = \frac{1}{1 + e^{(-a(x-c))}}
            $$</span></p>
            <p>where,</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>term</th>
            <th style="text-align: left;">definition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><span
            class="math inline"><em>y</em>(<em>x</em>)</span></td>
            <td style="text-align: left;">lateral offset of the
            vehicle</td>
            </tr>
            <tr>
            <td><span class="math inline"><em>x</em></span></td>
            <td style="text-align: left;">position in longitudinal
            direction</td>
            </tr>
            <tr>
            <td>$B#</td>
            <td style="text-align: left;">the “way position”, <span
            class="math inline"><em>P</em>3</span> <br> to generate
            obstacle avoidance manoeuvre</td>
            </tr>
            <tr>
            <td><span class="math inline"><em>c</em></span></td>
            <td style="text-align: left;">modifies the shape of the
            function</td>
            </tr>
            <tr>
            <td><span class="math inline"><em>a</em></span></td>
            <td style="text-align: left;">slope of the sigmoid</td>
            </tr>
            <tr>
            <td></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>The following figure shows the shape of the sigmoid ad
            curvature for various values of <span
            class="math inline"><em>a</em></span>,</p>
            <p><img src="img/object/avoidance/sigmoid.1.png" width="500"></p>
            <p><br></p>
            <p>The idea us to get from <span
            class="math inline"><em>P</em>1 → <em>P</em>3</span>,
            <strong>via</strong> <span
            class="math inline"><em>P</em>2</span>.</p>
            <ul>
            <li>inputs → obstacle position and position of vehicle</li>
            <li>lateral offset → calculated based on these
            parameters</li>
            </ul>
            <p>Hence, at a high level, the process looks like:</p>
            <p><img src="img/object/avoidance/sigmoid.2.png" width="400"></p>
            <p><br></p>
            <ol type="1">
            <li>find a <strong>circular area</strong> around he obstacle
            → so that we can compute a <strong>safe</strong> region to
            avoid and</li>
            <li>use the sigmoid functions to compute a <strong>smooth
            trajectory</strong> based on the circular region → make the
            transitions smoother and safer.</li>
            </ol>
            <p><br></p>
            <table>
            <tbody>
            <tr>
            <td><span class="math inline"><em>S</em></span></td>
            <td style="text-align: left;">desired
            <strong>lateral</strong> safety distance</td>
            </tr>
            <tr>
            <td><span
            class="math inline"><em>S</em><sub><em>m</em></sub></span></td>
            <td style="text-align: left;"><strong>longitudinal</strong>
            safety distance</td>
            </tr>
            <tr>
            <td></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>A “<strong>horizon planning approach</strong>” is used to
            compute the path by,</p>
            <ul>
            <li>dividing the drivable space into convex regions</li>
            <li>trajectory of each region is computed as the vehicle
            moves forward.</li>
            </ul>
            <p><br></p>
            <p>The various, incremental steps for the process,</p>
            <ol type="1">
            <li>approaching another car/obstacle → too far away to be a
            problem</li>
            </ol>
            <p><img src="img/object/avoidance/sigmoid.3.png" width="400"></p>
            <p><br></p>
            <ol start="2" type="1">
            <li>object detected
            <ul>
            <li>safety circle calculated</li>
            <li>smooth trajectory calculated our car (red) moves to new
            trajectory</li>
            </ul></li>
            </ol>
            <p><img src="img/object/avoidance/sigmoid.4.png" width="400"></p>
            <p><br></p>
            <p><strong>Note:</strong> the obstacle (blue car) has moved
            forward. So our calculations should account for this.</p>
            <ol start="3" type="1">
            <li>move past obstacle
            <ul>
            <li>we can start to move back to original path/lane</li>
            <li>complete the trajectory</li>
            </ul></li>
            </ol>
            <p><img src="img/object/avoidance/sigmoid.5.png" width="400"></p>
            <p><br></p>
            <p>Read the full paper <a
            href="https://ieeexplore.ieee.org/document/8519525">Smooth
            Obstacle Avoidance Path Planning for Autonomous Vehicles</a>
            by Ben-Messaoud et al for all the details.</p>
            <p><br></p>
            <p><strong>References</strong></p>
            <ul>
            <li><a
            href="https://www.sciencedirect.com/science/article/pii/S2666827021000827?via%3Dihub">Autonomous
            Driving Architectures: Insights of Machine Learning and Deep
            Learning Algorithms</a></li>
            <li><a
            href="https://www.vecnarobotics.com/resources/obstacle-avoidance-in-amr-and-agv-robots-explained/">Obstacle
            Avoidance in AMR and AGV Robots Explained</a></li>
            <li><a
            href="https://ieeexplore.ieee.org/document/8519525">Smooth
            Obstacle Avoidance Path Planning for Autonomous
            Vehicles</a></li>
            <li><a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=88137">Vector
            Field Histogram – Fast Obstacle Avoidance for Mobile
            robots</a></li>
            <li><a
            href="https://www.ri.cmu.edu/pub_files/pub1/fox_dieter_1997_1/fox_dieter_1997_1.pdf">The
            Dynamic Window Approach to Collision Avoidance</a></li>
            <li><a
            href="https://dspace.mit.edu/bitstream/handle/1721.1/52527/Kuwata-2009-Real-Time%20Motion%20Pla.pdf?sequence=1&amp;isAllowed=y">Real-Time
            Motion Planning With Applications to Autonomous Urban
            Driving</a></li>
            <li><a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=8202134">Virtual-to-real
            Deep Reinforcement Learning: Continuous Control of Mobile
            Robots for Mapless Navigation</a></li>
            <li><a href="https://arxiv.org/pdf/1604.07316">End to End
            Learning for Self-Driving Cars</a></li>
            <li>Obstacle Avoidance Slides by Prof. Lynne Parker,
            University of Tennesse Konxville: [<a
            href="https://web.eecs.utk.edu/~leparker/Courses/CS594-fall08/Lectures/Oct-21-Obstacle-Avoidance-I.pdf">1</a>],
            [<a
            href="https://web.eecs.utk.edu/~leparker/Courses/CS594-fall08/Lectures/Oct-23-ObstAvoid-II+Architectures.pdf">2</a>]</li>
            <li><a
            href="https://www.mathworks.com/help/nav/ug/vector-field-histograms.html">Vector
            Field Histogram</a></li>
            <li><a
            href="https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf">Reinforcement
            Learning: An Introduction</a></li>
            <li><a
            href="https://math.uchicago.edu/~may/REU2022/REUPapers/Wang,Yuzhou.pdf">MARKOV
            CHAINS AND MARKOV DECISION
            PROCESSES</a><!--rel="stylesheet" href="./custom.sibin.css"--></li>
            </ul>
            </section>
            </section>
            </section>
            <section id="safety-and-standards" class="level1"
            data-number="13">
            <h1 data-number="13"><span
            class="header-section-number">13</span> Safety and
            Standards</h1>
            <p>The <a
            href="https://www.atlantis-press.com/journals/jase/125934832/view">development
            of self-driving vehicles</a> adds layers of (software)
            intelligence on top of regular vehicles. However, the amount
            of software needed to achieve “autonomy” will <em>far</em>
            exceed the current software deployments. This demands a
            proper structure for such systems. Moreover, there will be a
            shift from <strong>open, deterministic components</strong>
            to more opaque, more probabilistic software components. This
            will raise new challenges for system designers. It also
            makes the analysis of <strong>safety guarantees</strong>
            that much harder.</p>
            <p>Multiple standards have been proposed (both domestically
            in the US as well as internationally). Others are still in
            development. As the (hardware and software) functionality
            and complexities increase, the standards themselves will
            likely have to be updated. One point to highlight is that
            the <em>primary</em> motivation of autonomous car
            technologies is reducing the frequency of traffic
            collisions.</p>
            <p>In this chapter, let’s take a look at some of the
            <strong>functional safety</strong> requirements/standards
            for autonomous systems.</p>
            <section id="levels-of-automation" class="level2"
            data-number="13.1">
            <h2 data-number="13.1"><span
            class="header-section-number">13.1</span> Levels of
            Automation</h2>
            <p>The <a
            href="https://www.nhtsa.gov/vehicle-safety/automated-vehicles-safety#the-topic-safety-timeline">National
            Highway Traffic Safety Administration(NHTSA)</a> defines
            <strong>five levels of automation</strong> for vehicles,</p>
            <ul>
            <li><strong>level 0</strong>: <strong>momentary driver
            assistance</strong> → the system provides momentary driving
            assistance, like warnings and alerts or emergency safety
            interventions while driver remains fully engaged and
            attentive. The driver is fully responsible for driving the
            vehicle.</li>
            </ul>
            <p><img src="img/safety/nhtsa.level.0.png" width="300"></p>
            <p><br></p>
            <p>Examples: automatic emergency braking, forward collision
            warning, lane departure warning.</p>
            <ul>
            <li><strong>level 1: driver assistance</strong> → the system
            provides continuous assistance with either
            acceleration/braking <strong>or</strong> steering, while
            driver remains fully engaged and attentive. Driver is fully
            still responsible for driving the vehicle.</li>
            </ul>
            <p><img src="img/safety/nhtsa.level.1.png" width="300"></p>
            <p><br></p>
            <p>Examples: adaptive cruise control, lane keeping
            assistance.</p>
            <ul>
            <li><strong>level 2: additional assistance</strong> → the
            system provides <strong>continuous</strong> assistance with
            both acceleration/braking <strong>and</strong> steering,
            while driver remains fully engaged and attentive. The driver
            is still responsible for driving the vehicle.</li>
            </ul>
            <p><img src="img/safety/nhtsa.level.2.png" width="300"></p>
            <p><br></p>
            <p>Examples: highway pilot.</p>
            <ul>
            <li><strong>level 3: conditional automation</strong> →
            system actively performs driving tasks while <strong>driver
            remains available</strong> to take over. When engaged, the
            <strong>system handles all aspects of the driving
            task</strong> while you, as the driver, are available to
            take over driving if requested. If the system can no longer
            operate and prompts the driver, the driver must be available
            to resume all aspects of the driving task.</li>
            </ul>
            <p><img src="img/safety/nhtsa.level.3.png" width="300"></p>
            <p><br></p>
            <p>Examples: some of the modern electric cars,
            <em>e.g.,</em> Tesla.</p>
            <ul>
            <li><strong>level 4: high automation</strong> → the system
            is <strong>fully responsible</strong> for driving tasks
            <strong>within limited service areas</strong> while
            occupants act only as passengers and do not need to be
            engaged. A human driver is not needed to operate the
            vehicle.</li>
            </ul>
            <p><img src="img/safety/nhtsa.level.4.png" width="300"></p>
            <p><br></p>
            <p>Examples: maybe Waymo?</p>
            <ul>
            <li><strong>level 5: full automation</strong> → the system
            is <strong>fully responsible</strong> for driving tasks
            while occupants act only as passengers and <strong>do not
            need to be engaged</strong>. The system can operate the
            vehicle universally – under all conditions and on all
            roadways. A human driver is not needed to operate the
            vehicle.</li>
            </ul>
            <p>Examples: these technologies do not exist in today’s
            vehicles.</p>
            <p><img src="img/safety/nhtsa.level.5.png" width="300"></p>
            <p><br></p>
            <p>Here is a <a
            href="https://www.nhtsa.gov/sites/nhtsa.gov/files/2022-05/Level-of-Automation-052522-tag.pdf">concise
            chart</a> that summarizes all of the automation levels.</p>
            </section>
            <section id="sae-j3016-standard-for-functional-safety"
            class="level2" data-number="13.2">
            <h2 data-number="13.2"><span
            class="header-section-number">13.2</span> SAE J3016 Standard
            for Functional Safety</h2>
            <p>The most well known standard is the <a
            href="https://www.atlantis-press.com/journals/jase/125934832/view">SAE
            J3016 standard</a> that has helped define some of the
            <strong>functional architectures</strong> in use in modern
            autonomous systems.</p>
            <p>This notion of “functional architecture” is inspired by
            the <a href="https://www.iso.org/standard/68383.html">ISO
            26262</a> automotive standard that specifies the,</p>
            <ul>
            <li><strong>intended functionality</strong> and</li>
            <li><strong>interactions</strong></li>
            </ul>
            <p>necessary for a vehicle to remain <strong>safe</strong>.
            This matches the “functional views” in software
            architectures – code is clustered into groups and
            distributed to different teams in order to reason about
            them. It closely aligns with the <a
            href="https://dl.acm.org/doi/10.1145/1764810.1764814">V
            Waterfall</a> model of software development.</p>
            <p>First, some definitions:</p>
            <table>
            <colgroup>
            <col style="width: 43%" />
            <col style="width: 56%" />
            </colgroup>
            <thead>
            <tr>
            <th><strong>term</strong></th>
            <th>description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td><strong>OEM</strong></td>
            <td>original equipment manufacturers</td>
            </tr>
            <tr>
            <td><strong>ECU</strong></td>
            <td>electronic control units</td>
            </tr>
            <tr>
            <td><strong><a
            href="https://www.csselectronics.com/pages/can-bus-simple-intro-tutorial">CAN</a>,
            <a
            href="https://www.ni.com/en/shop/seamlessly-connect-to-third-party-devices-and-supervisory-system/flexray-automotive-communication-bus-overview.html?srsltid=AfmBOoqDL3Ir4GQ0bTWSjM2sK_NnTXp9N7MkCDX6o-wPPwi1YsDbqtz8">Flexray</a></strong></td>
            <td>communication buses/standards</td>
            </tr>
            <tr>
            <td><strong><a
            href="https://www.autosar.org">AUTOSAR</a></strong></td>
            <td>software technology platforms</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p><strong>Note:</strong> the SAE J3016 standard
            <strong>does not</strong> provide <em>strict</em>
            requirements; rather, its purpose is to be
            <em>descriptive</em> and <em>broad</em> about the evolution
            of such systems. The idea is to sketch an
            <strong>incremental evolution</strong> → from no automation
            (level <code>0</code>) to full automation (level
            <code>5</code>).</p>
            <p>This SAE standard defines some important terms (that have
            become somewhat standard in the industry): |
            <strong>term</strong> | <strong>description</strong> |
            <strong>examples</strong> | |———-|—————–|————–| |
            <strong>Dynamic Driving Task (DDT)</strong> | real-time
            operational and tactical functions required to operate a
            vehicle, excluding strategic functions such as trip
            scheduling or route planning. DDT includes actuator control
            and tactical planning such as generating and following a
            trajectory, keeping the vehicle within the lanes,
            maintaining distance from other vehicles, <em>etc.</em> |
            steering, braking, lane keeping, trajectory planning | |
            <strong>Driving automation system</strong> | hardware and
            software systems collectively capable of performing some
            parts or all of the DDT on a sustained basis. Driving
            automation systems are composed of design-specific
            functionality called features. Focus is on the interplay
            between software components to design systems capable of
            achieving full autonomy. | automated parking, lane keep
            assistance | | <strong>Operational Design Domains
            (ODD)</strong> | specific conditions under which a given
            driving automation system or feature is designed to
            function. Requirements vary based on the domain. Full
            autonomy requires operation in all weather and traffic
            conditions. | sunny city driving, winter mountain roads | |
            <strong>DDT fall-back</strong> | the response by the user or
            by an Automated Driving System (ADS) to either perform the
            DDT task or achieve a safety state after a DDT
            performance-relevant system failure or upon leaving the
            designated ODD. | user intervention, safety state activation
            | | <strong>DDT fall-back-ready user</strong> | the user of
            a vehicle equipped with an engaged ADS feature who is able
            to operate the vehicle and is receptive to ADS-issued
            requests to intervene and perform DDT tasks during a system
            failure or when requested by the automated vehicle. | driver
            ready to take control | | <strong>DDT feature</strong> | a
            design-specific functionality at a specific level of driving
            automation with a particular ODD. | lane assistance in sunny
            weather | ||</p>
            <p><br></p>
            <p>These features/terms can be mapped to the five levels of
            automation we described earlier:</p>
            <p><img src="img/safety/sae_j3016_levels.png" width="400"></p>
            <p><br></p>
            <p>Notice,</p>
            <ol type="1">
            <li><p>who <strong>monitors</strong> the road → in the case
            of no automation up to partial automation (levels 0-2), the
            environment is monitored by a human driver, while for higher
            degrees of automation (levels 3-5), the vehicle becomes
            responsible for environmental monitoring.</p></li>
            <li><p>who is the <strong>fall back</strong> in case of
            failures/problems → intelligent driving automation systems
            (levels 4-5) embed the responsibility for automation
            fall-back constrained or not by operational domains, while
            for low levels of automation (levels 0-3) a human driver is
            fully responsible.</p></li>
            </ol>
            <p><br> <br></p>
            <p>The SAE standard defines <strong>three classes of
            components</strong>:</p>
            <table>
            <colgroup>
            <col style="width: 36%" />
            <col style="width: 63%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">class</th>
            <th style="text-align: left;">definition</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td
            style="text-align: left;"><strong>operational</strong></td>
            <td style="text-align: left;">basic vehicle control</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>tactical</strong></td>
            <td style="text-align: left;">planning and execution for
            event or object avoidance and expedited route following</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>strategic</strong></td>
            <td style="text-align: left;">destination and general route
            planning</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p>Each of these classes has an <strong>incremental
            role</strong> in the design of autonomous systems. The
            various classes are connected as follows:</p>
            <p><img src="img/safety/ddt.9.png" width="400"></p>
            <p><br></p>
            <p>Recall that this is a close analogue to the sensing →
            control/planning → actuation pipeline from before,</p>
            <p><img src="img/sense_planning_actuation.png" width="400"></p>
            <p><br></p>
            <p>Note that each OEM may choose to implement each of the
            above components differently.</p>
            <p>As you may have noticed from all the material covered so
            far, the sensing → planning → actuation pipeline can be
            <strong>decomposed</strong> into smaller components. The
            following decomposition matches the SAE J3016 standard:</p>
            <p><img src="img/safety/decomposition.7.png" width="500"></p>
            <p><br></p>
            <p>The <strong>world model</strong> stored data
            (<em>e.g.,</em> maps) maintain knowledge about,</p>
            <ul>
            <li>images</li>
            <li>maps</li>
            <li>entities</li>
            <li>events</li>
            </ul>
            <p>and also <strong>relationships</strong> between
            them. </p>
            <p>World modeling stores and uses <strong>historical
            information</strong> (from past processing loops) and
            provides <strong>interfaces</strong> to query and filter its
            content for other components</p>
            <p><strong>Behavior generation</strong> → is the
            <strong>highest cognitive class</strong> of functions in the
            architecture. It develops a number of possible state
            sequences from the current state and the behavior reasoning
            module selects the best alternative.</p>
            <p>The data in this decomposed functional diagram, flows
            <strong>left → right</strong>.</p>
            <p><img src="img/safety/decomposition.8.png" width="500"></p>
            <p><br></p>
            <p>To map these back to the operational classes,</p>
            <ul>
            <li>vehicle control and actuators interface →
            <strong>operational</strong> class</li>
            <li>planning class → <strong>tactical</strong></li>
            <li>behavior generation → <strong>both</strong> strategic
            and planning class of functions.</li>
            </ul>
            <p>In addition, there are <strong>orthogonal
            classes</strong>, <em>viz.,</em></p>
            <table>
            <colgroup>
            <col style="width: 57%" />
            <col style="width: 42%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">orthogonal class</th>
            <th style="text-align: left;">description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><strong>data
            management</strong></td>
            <td style="text-align: left;">implement long term data
            storage and retrieval</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>system and safety
            management</strong></td>
            <td style="text-align: left;">represent DDT fall-back
            mechanisms or other safety concerns → they act in parallel
            of normal control loops</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><img src="img/safety/orthogonal.3.png" width="500"></p>
            <p><br></p>
            <p><strong>Note:</strong> there is one other orthogonal
            class of functions <em>viz.,</em> <strong>security</strong>.
            We will address that in the next chapter.</p>
            <p>As you can imagine, this (closely) matches our initial
            course/system design:</p>
            <p><img src="img/stack_architecture/stack_overview.png" width="300"></p>
            <p><br></p>
            <p>The functional decomposition diagram shown earlier
            matches the software components in <strong><a
            href="https://www.autosar.org">AUTOSAR</a></strong> → a
            software standardization architecture that is popular in
            industry. The interfaces between the various components can
            be implemented using AUTOSAR’s standardized interface
            definitions.</p>
            <p>The <em>actual</em> interaction between some of the
            critical components may look like:</p>
            <p><img src="img/safety/interactions.png" width="300"></p>
            <p><br></p>
            <p>Read the <a
            href="https://sibin.github.io/teaching/csci6907_88-gwu/secure_autonomous/fall_2022/other_docs/J3016_201609.pdf">actual
            standard</a> for a full understanding. Here is a <a
            href="https://www.atlantis-press.com/journals/jase/125934832/view">good
            explanation</a> and summary.</p>
            </section>
            <section id="ansiul-4600-standard" class="level2"
            data-number="13.3">
            <h2 data-number="13.3"><span
            class="header-section-number">13.3</span> ANSI/UL 4600
            Standard</h2>
            <p>The <a
            href="https://users.ece.cmu.edu/~koopman/ul4600/191213_UL4600_VotingVersion.pdf">ANSI/UL
            4600</a> standard for Safety for the Evaluation of
            Autonomous Products, provides an umbrella for
            <strong>coordinating software development practices</strong>
            and computer-based system safety standards to make sure
            nothing is left out when <strong>assuring
            safety</strong>.</p>
            <p>This was the first comprehensive standard for public road
            autonomous vehicle safety to cover both → urban and highway
            use cases.</p>
            <p>The <a
            href="https://users.ece.cmu.edu/~koopman/ul4600/L109_UL4600.pdf">key
            ideas</a> are:</p>
            <ul>
            <li>system-level safety case provides direction</li>
            <li>vehicle as well as infrastructure and lifecycle
            processes all matter</li>
            <li>safety metrics used for feedback loops</li>
            <li>third party component interface protects proprietary
            info</li>
            <li>4600 helps you know that you’ve done enough work on
            safety</li>
            </ul>
            <p>UL 4600 works towards generating → <strong><a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10109263">safety
            cases</a></strong>.</p>
            <p>The main difference between <code>4600</code> and other
            safety standards is that it is <strong>not
            prescriptive</strong> (<em>i.e.,</em> doesn’t say “do X”).
            Rather, it is <strong>goal-oriented</strong>, <em>i.e.,</em>
            answering questions such as:</p>
            <blockquote>
            <p>“what should a safety case address?”</p>
            </blockquote>
            <p>So the idea is to avoid saying, “using this
            engineering/software approach to solve problem X”.</p>
            <p>What are the standards for how to assess a safety
            case?</p>
            <ul>
            <li>minimum <strong>coverage</strong> requirement (what goes
            in the safety case?)</li>
            <li><strong>properties</strong> of a well-formed safety
            case</li>
            <li>objective <strong>assessment</strong> criteria</li>
            </ul>
            <p><br></p>
            <p>Consider the following example of a safety case using the
            notion of <strong>claims, arguments and
            evidence</strong>.</p>
            <table>
            <colgroup>
            <col style="width: 23%" />
            <col style="width: 38%" />
            <col style="width: 38%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">claim/sub-claim <br>property
            of system</th>
            <th style="text-align: left;">argument <br> why is it
            true?</th>
            <th style="text-align: left;">evidence <br> supports
            argument</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;">“system avoids
            pedestrians”</td>
            <td style="text-align: left;">“detects and maneuver to
            avoid”</td>
            <td style="text-align: left;">tests, simulations, formal
            analyses</td>
            </tr>
            <tr>
            <td style="text-align: left;">“detect pedestrians”
            (<strong>sub claim</strong>)</td>
            <td style="text-align: left;">-</td>
            <td style="text-align: left;">evidence</td>
            </tr>
            <tr>
            <td style="text-align: left;">“maneuvers around pedestrians
            (<strong>sub claim</strong>)</td>
            <td style="text-align: left;">–</td>
            <td style="text-align: left;">evidence</td>
            </tr>
            <tr>
            <td style="text-align: left;">“stops if can’t maneuver”
            (<strong>sub claim</strong>)</td>
            <td style="text-align: left;">–</td>
            <td style="text-align: left;">evidence</td>
            </tr>
            <tr>
            <td style="text-align: left;">…</td>
            <td style="text-align: left;">…</td>
            <td style="text-align: left;">…</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>We can represent this diagrammatically as follows:</p>
            <p><img src="img/safety/safety_cases.png" width="300"></p>
            <p><br></p>
            <p>Safety cases need to consider many things:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <tbody>
            <tr>
            <td
            style="text-align: left;"><strong>technology</strong></td>
            <td style="text-align: left;">hw/sw, machine learning,
            tools, …</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>lifecycle</strong></td>
            <td style="text-align: left;">deployment, operation,
            incidents, maintenance, …</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>infrastructure</strong></td>
            <td style="text-align: left;">vehicle, roads, data networks,
            cloud computing, …</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>road
            users</strong></td>
            <td style="text-align: left;">pedestrians, light mobility,
            emergency responders, …</td>
            </tr>
            <tr>
            <td
            style="text-align: left;"><strong>environment</strong></td>
            <td style="text-align: left;">operational design domain
            (odd) definition</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p><strong>Safety Performance Indicator (SPI)</strong></p>
            <p>The idea is to provide metrics on the
            <strong>validity</strong> of safety cases. It is
            context-dependent and can include many aspects,
            <em>e.g.,</em></p>
            <ul>
            <li><strong>acceptable violation rate</strong> of standoff
            to pedestrians</li>
            <li><strong>gap tolerance</strong> of up to X meters in lane
            markings</li>
            <li><strong>false negative rate</strong> for
            camera/LiDAR</li>
            <li><em>etc.</em></li>
            </ul>
            <p>Read the <a
            href="https://www.shopulstandards.com/ProductDetail.aspx?productid=UL4600">full
            standard</a> online (select “digital view” and sign up for
            reading it for free). Here is a <a
            href="https://users.ece.cmu.edu/~koopman/ul4600/191213_UL4600_VotingVersion.pdf">draft
            version</a>.</p>
            <p>Here is a video that provides a summary:</p>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/xOuQkf1yuW0?si=J9nt2v0Ob5dXqaJk" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
            </iframe>
            </section>
            <section id="other-standardsefforts" class="level2"
            data-number="13.4">
            <h2 data-number="13.4"><span
            class="header-section-number">13.4</span> Other
            Standards/Efforts</h2>
            <ol type="1">
            <li>IEEE has a <a
            href="https://standards.ieee.org/initiatives/autonomous-intelligence-systems/standards/">suite
            of standards</a> that focus on <strong>autonomous and
            intelligent systems</strong>.</li>
            <li>NIST has many efforts on <a
            href="https://www.nist.gov/programs-projects/autonomous-systems-assurance">autonomous
            systems assurance</a> to <strong>verify</strong> that
            autonomous systems function correctly in a wide range of
            environments. One such is the <a
            href="https://csrc.nist.gov/projects/automated-combinatorial-testing-for-software">Combinatorial
            Methods for Trust and Assurance</a>.</li>
            <li>Wikipedia has a good summary of various <a
            href="https://en.wikipedia.org/wiki/Regulation_of_self-driving_cars">safety/standardization
            efforts across the world</a>.</li>
            </ol>
            </section>
            <section id="self-driving-car-liability" class="level2"
            data-number="13.5">
            <h2 data-number="13.5"><span
            class="header-section-number">13.5</span> Self-driving Car
            Liability</h2>
            <p>One of the main issues that arises when designing
            autonomous vehicles, especially ones that operate in the
            real world, is:</p>
            <blockquote>
            <p>“who is <strong>liable</strong> for the actions of the
            car?”</p>
            </blockquote>
            <p>Is it,</p>
            <ul>
            <li>the car <strong>owner</strong>?</li>
            <li>the car <strong>manufacturer</strong>?</li>
            <li>the <strong>software/hardware developers</strong>?</li>
            <li>the <strong>city/state/federal
            governments</strong>?</li>
            <li>someone else?</li>
            </ul>
            <p>This is an active area of research and discussion
            especially in the legal community. There is a need for
            existing liability laws to evolve to reasonably identify the
            appropriate remedies for damage and injury. This become
            particularly problematic as higher levels of autonomy are
            implemented → what about levels <code>4</code> and
            <code>5</code>?</p>
            <p>Various countries (the UK, France, Germany, Japan,
            <em>etc.</em> ) have started to draft policies and laws to
            handle the issues of liability.</p>
            <p>In the US, there is a policy proposal that will rest the
            <a
            href="https://digitalcommons.law.scu.edu/cgi/viewcontent.cgi?article=2731&amp;context=lawreview">liability
            with the <strong>manufacturer</strong></a> of the
            vehicles.</p>
            <p>Some US-based efforts include,</p>
            <ol type="1">
            <li>NHTSA released a report, “<a
            href="https://www.thefreelibrary.com/_/print/PrintArticle.aspx?id=16112768">the
            Automated Highway System: an idea whose time has come</a>”
            that provide initial guidelines for a regulatory framework.
            Some important provisions:
            <ul>
            <li>states are responsible for determining liability rules
            for autonomous vehicles. States should consider how to
            allocate liability among owners, operators, passengers,
            manufacturers, and others when a crash occurs.</li>
            <li>determination of who or what is the
            “<strong>driver</strong>” in a given circumstance
            <strong>does not necessarily determine liability</strong>
            for crashes involving that HAV.</li>
            </ul></li>
            <li><a
            href="https://www.congress.gov/bill/115th-congress/house-bill/3388#:~:text=The%20bill%20preempts%20states%20from,standards%20identical%20to%20federal%20standards.">H.R.
            3388, Self Drive Act</a> passed by the House of
            Representatives in 2017. Some key ideas:
            <ul>
            <li>advance safety by <strong>prioritizing the protection of
            consumers</strong></li>
            <li>reaffirm role and responsibilities of federal and state
            governments</li>
            <li>update the Federal Motor Vehicle Safety Standards to
            account for advances in technology and the evolution of
            highly automated vehicles.</li>
            </ul></li>
            </ol>
            <p>Interestingly, H.R. 3388 is <strong>limiting the role of
            states</strong>!</p>
            <p>There are also efforts to gauge the role of ML/AI in
            autonomous systems and tracking the liability when such
            agents are making choices. But since ML/AI is a fast moving
            field, developing new policies/laws is quite
            challenging.</p>
            <p>The <a
            href="https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR1702.pdf">NTSB
            investigation of the Tesla crash of 2016</a> makes for an
            interesting read.</p>
            <p><br></p>
            <p><strong>References</strong></p>
            <ul>
            <li><a href="https://www.iso.org/standard/68383.html">ISO
            26262 standard</a></li>
            <li><a
            href="https://www.sciencedirect.com/science/article/pii/S2666691X23000428">Standards
            relevant to automated driving system safety: A systematic
            assessment</a></li>
            <li><a
            href="https://www.atlantis-press.com/journals/jase/125934832/view">A
            Standard Driven Software Architecture for Fully Autonomous
            Vehicles</a> bu Serban <em>et al.</em></li>
            <li><a
            href="https://users.ece.cmu.edu/~koopman/ul4600/index.html">UL
            4600: Standard for Safety for the Evaluation of Autonomous
            Products</a> summary of resources page</li>
            <li><a
            href="https://users.ece.cmu.edu/~koopman/ul4600/L109_UL4600.pdf">Key
            Ideas: UL 4600 Safety Standard for Autonomous Vehicles</a>
            by Philip Koopman</li>
            <li><a
            href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=10109263">UL
            4600: What to Include in an Autonomous Vehicle Safety
            Case</a> by Koopman</li>
            <li><a
            href="https://users.ece.cmu.edu/~koopman/ul4600/191213_UL4600_VotingVersion.pdf">Proposed
            First Edition of the Standard for Safety for the Evaluation
            of Autonomous Products, UL 4600</a></li>
            <li><a
            href="https://www.shopulstandards.com/ProductDetail.aspx?productid=UL4600">UL
            4600 official standard site</a></li>
            <li><a
            href="https://www.nhtsa.gov/vehicle-safety/automated-vehicles-safety#the-topic-safety-timeline">NHTSA
            Automated Vehicular Safety</a></li>
            <li><a
            href="https://www.nhtsa.gov/sites/nhtsa.gov/files/2022-05/Level-of-Automation-052522-tag.pdf">NHTSA
            Levels of Automation sheet</a></li>
            <li><a
            href="https://standards.ieee.org/initiatives/autonomous-intelligence-systems/standards/">IEEE
            portfolio of AIS technology and impact standards and
            standards projects</a></li>
            <li><a
            href="https://www.nist.gov/programs-projects/autonomous-systems-assurance">NIST
            Autonomous Systems Assurance</a></li>
            <li><a
            href="https://csrc.nist.gov/projects/automated-combinatorial-testing-for-software">NIST
            Combinatorial Methods for Trust and Assurance</a></li>
            <li><a
            href="https://en.wikipedia.org/wiki/Self-driving_car_liability">Wikipedia
            Self-car Driving Liability</a></li>
            <li><a
            href="https://www.congress.gov/bill/115th-congress/house-bill/3388#:~:text=The%20bill%20preempts%20states%20from,standards%20identical%20to%20federal%20standards.">H.R.3388
            SELF DRIVE Act</a> – Safely Ensuring Lives Future Deployment
            and Research In Vehicle Evolution Act</li>
            <li><a
            href="https://digitalcommons.law.scu.edu/cgi/viewcontent.cgi?article=2731&amp;context=lawreview">The
            Coming Collision Between Autonomous Vehicles and the
            Liability System</a> by Marchant <em>et al.</em></li>
            <li><a
            href="https://www.thefreelibrary.com/_/print/PrintArticle.aspx?id=16112768">The
            Automated Highway System: an idea whose time has come</a> by
            Transport Research International</li>
            <li><a
            href="https://www.ntsb.gov/investigations/AccidentReports/Reports/HAR1702.pdf">NTSB
            Report on 2016 Tesla Crash</a> – “Collision Between a Car
            Operating With Automated Vehicle Control Systems and a
            Tractor-Semitrailer Truck Near Williston, Florida”, May 7,
            2016.<!--rel="stylesheet" href="./custom.sibin.css"--></li>
            </ul>
            </section>
            </section>
            <section id="security-for-autonomous-systems" class="level1"
            data-number="14">
            <h1 data-number="14"><span
            class="header-section-number">14</span> Security for
            Autonomous Systems</h1>
            <p>Security for autonomous systems significantly overlaps
            with similar issues for embedded, cyber-physical and
            <em>automotive</em> security. There is a lot of work on
            security for such systems – the added complexity is that use
            of ML/AI algorithms that can generate some <a
            href="https://spectrum.ieee.org/slight-street-sign-modifications-can-fool-machine-learning-algorithms">unique
            attack vectors</a>!</p>
            <p>In this chapter we will summarize many of the
            topics/issues that can lead to security and privacy problems
            in autonomous vehicles.</p>
            <p><strong>Note:</strong> that it is nearly impossible to
            summarize all of the work on security and privacy for
            autonomous systems so this chapter will touch upon some of
            the most relevant/interesting work in the area.</p>
            <p>The first issue is → how do you <strong>define</strong>
            or <strong>classify</strong> security issues? Is it…</p>
            <table>
            <tbody>
            <tr>
            <td>attacks</td>
            <td>?</td>
            </tr>
            <tr>
            <td>defenses</td>
            <td>?</td>
            </tr>
            <tr>
            <td>something else</td>
            <td>?</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <section id="attacks" class="level2" data-number="14.1">
            <h2 data-number="14.1"><span
            class="header-section-number">14.1</span> Attacks</h2>
            <p>One important thing to consider when discussing attacks
            in <em>any</em> system is the <strong><a
            href="https://arxiv.org/pdf/2412.15348">threat
            model</a></strong> <em>i.e.,</em> gain an understanding
            of,</p>
            <ol type="1">
            <li>the <strong>method</strong> of entry/attack</li>
            <li>the <strong>target</strong> of the attack</li>
            <li>the <strong>capabilities</strong> of the attacker →
            <em>e.g.,</em> how much computing power do they have?</li>
            </ol>
            <p>One way to reason about attacks on autonomous systems (or
            any system really) is,</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>passive</th>
            <th>active</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>stealing data</td>
            <td><font style="background-color: #FFEC8B;">causing
            physical harm</font></td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>In autonomous/cyber-physical systems, we are more
            concerned with the second category, <em>i.e.,</em> active,
            since we care for the integrity of the system and
            <strong>safety</strong> – of the users, the systems and the
            environment. Note that stealing data (proprietary
            information like system designs, software, user information,
            <em>etc.</em> ) is important but there is a large body of
            work on how to deal with such issues. We will focus more on
            the <strong>active</strong> attacks.</p>
            <p>There exist many <a
            href="https://dl.acm.org/doi/pdf/10.1145/3337791">taxonomies
            of attacks</a> for autonomous systems but let’s focus on the
            following:</p>
            <ol type="1">
            <li><a
            href="#sensor-based-attacks"><strong>sensor</strong>-based
            attacks</a></li>
            <li><a
            href="#actuation-based-attacks"><strong>actuation</strong>-based
            attacks</a></li>
            <li><a href="#computing-softwarehardware-attacks">compute
            <strong>software/hardware</strong> attacks</a></li>
            <li><a
            href="#miscellaneouscommunication-attacks"><strong>miscellaneous/communication</strong>
            attacks</a></li>
            </ol>
            <section id="sensor-based-attacks" class="level3"
            data-number="14.1.1">
            <h3 data-number="14.1.1"><span
            class="header-section-number">14.1.1</span> Sensor-based
            Attacks</h3>
            <p>By corrupting the <strong>inputs</strong> to an
            autonomous systems, adversaries can prevent the system from
            working correctly. Recall that <a
            href="#sensors-and-sensing">sensors</a> are the “eyes and
            ears” for an autonomous system → this is often the only way
            that the system can perceive the external world. If the
            sensor data is either corrupted or jammed, then the system
            cannot operate correctly.</p>
            <p>There is a large body of work on <a
            href="https://ieeexplore.ieee.org/document/9152711">physical
            attacks on sensor</a> aka by either jamming the signals or
            tampering with the physical circuitry on/around sensors.</p>
            <p>One recent, interesting attack that targets the cameras
            of autonomous vehicles, is using <a
            href="https://ieeexplore.ieee.org/abstract/document/9519394?casa_token=q0hmx8m-n2wAAAAA:WuBNY-P49Hx4Uibam2a_iaY0SG_j0yE6MPuJoplLwwKzT_KRH3l24sVNmeF921OwahC2u50U5A">acoustic
            manipulation</a> to <strong>control the output</strong> of a
            camera to become blurred. This can result in
            <strong>misclassifications</strong> that can have serious
            repercussions.</p>
            <p><img src="img/security/poltergeist.gif" width="400"></p>
            <p><br></p>
            <p>As we see from the image, the acoustic waves perturb the
            camera so that it doesn’t recognize the car in front and can
            lead to a crash!</p>
            <p>More recently, there is work that targets the sensor in
            more unique ways – <strong>without physical methods</strong>
            – <em>i.e.,</em> targeting the ML/vision algorithms by
            feeding it incorrect data or using ML to
            <strong>subtly</strong> change sensors values in
            software.</p>
            <p>For instance, imagine <a
            href="https://arxiv.org/pdf/1707.08945">placing small
            stickers/splotches of paint</a> on a stop sign so that the
            vision algorithm misclassifies it as a speed limit sign –
            leading to serious consequence!</p>
            <p>The entire attack process:</p>
            <p><img src="img/security/stop.4.png" width="400"></p>
            <p><br></p>
            <p>Other work <a
            href="https://arxiv.org/pdf/1806.02299">targets
            <em>specific</em> software components</a> such as YOLO and
            R-CNN. It is a similar idea to the previous paper where
            <strong>small perturbations are added</strong> to input
            images so that the ML algorithms (<em>e.g.,</em> YOLO) will
            either misclassify it or completely fail to recognize the
            image.</p>
            <p>In this case, they generate a seemingly random color
            patch that could look like this:</p>
            <p><img src="img/security/dpatch.1.png" width="200"></p>
            <p><br></p>
            <p>When we see the results side-by-side (without/with
            patch), we see that the object detector (YOLO in this case)
            can recognize the bicycle and generate a good bounding box
            around it in the first case and completely missed the object
            in the second case.</p>
            <p><img src="img/security/dpatch.2.png" width="300">
            <img src="img/security/dpatch.3.png" width="300"></p>
            <p>One interesting thing to note → the size of the patch
            relative to the actual image. This can be really small and
            hence is easily missed by a casual observer.</p>
            <p><br></p>
            <p>More recent work injects <a
            href="https://sibin.github.io/papers/2024_ArXiv_Requiem_KyoKim.pdf">small
            changes to sensor data</a> so that the autonomous vehicle is
            led astray. The idea is to use ML algorithms to compute
            minor perturbations that can be added to the sensor values
            (in software) before they’re fed into the EKF algorithm.
            This results in,</p>
            <ul>
            <li>EKF <strong>incorrectly assessing the current
            state</strong> of the system</li>
            <li>the autonomous system moving away from its expected
            path</li>
            </ul>
            <p>The following figures show the effect of the attach on
            two missions – climbing straight up and hovering in a
            circle.</p>
            <p><img src="img/security/requiem.1.png" width="200">
            <img src="img/security/requiem.2.png" width="200"></p>
            <p><br></p>
            <p><strong>Note:</strong> an important objective is to
            ensure that the <strong>anomaly detector does not
            detect</strong> the attack. To acheive this, the “spoofed”
            data should be <strong>indistinguishable</strong> from the
            real sensor data. If we look at the distributions of the
            two,</p>
            <p><img src="img/security/requiem.3.png" width="300"></p>
            <p><br></p>
            <p>The main method used in the paper is machine learning
            (specifically <a
            href="https://en.wikipedia.org/wiki/Generative_adversarial_network">GANs</a>)
            to generate the spoofed data.</p>
            </section>
            <section id="actuation-based-attacks" class="level3"
            data-number="14.1.2">
            <h3 data-number="14.1.2"><span
            class="header-section-number">14.1.2</span> Actuation-based
            Attacks</h3>
            <p>Another attack <em>vector</em> is to attack the <a
            href="#actuation"><strong>actuation</strong></a> side –
            recall that the eventual “control” of the vehicle is carried
            out by actuation commands that are sent out, often via PWM
            signals.</p>
            <p>There is quite a bit of work in the space of <a
            href="https://arxiv.org/pdf/1708.01834">actuation-based
            attacks</a> in literature. It is one of the more <a
            href="https://www.sciencedirect.com/science/article/abs/pii/S0925231217316351">active
            areas of research in control systems security</a>.</p>
            <p>One interesting attack, <a
            href="https://sibin.github.io/papers/2019_rtas_scheduleak_cy.pdf">ScheduLeak</a>,
            does something different:</p>
            <ul>
            <li><strong>overrides the actuation command</strong> (rather
            the PWM register)</li>
            <li>at a <strong>precise point in time</strong></li>
            </ul>
            <p><img src="img/security/scheduleak.1.png" width="400"></p>
            <p><br></p>
            <p>This allows the attacker to either cause the system to go
            into disarray or <strong>precisely</strong> take control of
            it.</p>
            <p>As the figure shows, overriding the PWM value at the
            <strong>right moment in time</strong> matters → if it is
            done too early or in a random fashion, then the attack may
            not be successful.</p>
            <p>The paper demonstrates multiple attack that can be
            launched using ScheduLeak. Here is one of them (taking
            control of an autonomous rover).</p>
            <p><img src="img/security/scheduleak.2.png" width="300"></p>
            <p><br></p>
            <p>Here is a video that demonstrates the attack:</p>
            <iframe width="560" height="315" src="https://www.youtube.com/embed/g12Zk1MboyE?si=ilx4ubbMnsJzjPQC" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen>
            </iframe>
            <p><br></p>
            <p>Check out <a href="https://scheduleak.github.io">more
            attacks using ScheduLeak</a>.</p>
            <p><strong>Note:</strong> this attack relies on the fact
            that the underlying software/tasks follow a <strong>periodic
            real-time</strong> computation model.</p>
            <p><br></p>
            <p>One other famous attack (that pretty much changed the
            landscape of CPS security) → <a
            href="https://spectrum.ieee.org/the-real-story-of-stuxnet"><strong>Stuxnet</strong></a>.</p>
            <p>An Iranian power plant (supposedly enriching nuclear
            fuel) <strong>exploded</strong> without any warning. It is
            considered one of the first examples of → a <strong>cyber
            attack resulting in physical damage</strong>.</p>
            <p>The power plant was using a Siemens SCADA controller:</p>
            <p><img src="img/security/stuxnet.JPG" width="300"></p>
            <p><br></p>
            <p>Attackers were able to,</p>
            <ul>
            <li>intrude into the <strong>air-gapped</strong>
            system!</li>
            <li>observe the state for a while</li>
            <li>send data back</li>
            <li>craft a <strong>specific malware payload</strong></li>
            </ul>
            <p>The malware did the following:</p>
            <ol type="1">
            <li>change the <strong>operational frequency</strong> →
            sometimes <strong>high</strong>, other times
            <strong>low</strong>
            <ul>
            <li>induces <strong>wear and tear</strong></li>
            </ul></li>
            <li><strong>intercept</strong> logs and engineering data →
            replace with seemingly benign(yet, fake) data
            <ul>
            <li>operators don’t notice anything untoward in the
            logs</li>
            </ul></li>
            </ol>
            <p><img src="img/security/mission_impossible.gif" width="400"></p>
            <p><br></p>
            <p>Very Mission Impossible-esque.</p>
            <p>They first discovered a problem when the <strong>entire
            plant blew up</strong>!</p>
            <p>The entire process, summarized:</p>
            <p><img src="img/security/stuxnet.2.webp" width="500"></p>
            <p><br></p>
            <p>How does this relate to autonomous systems? Well similar
            attacks can be launched against critical components, </p>
            <ul>
            <li>engines</li>
            <li>brakes</li>
            <li>sensors/actuators</li>
            </ul>
            <p>The (cyber-induced) wear and tear can be hard to
            track/detect and often is only noticed
            <strong>after</strong> the system has failed!</p>
            </section>
            <section id="computing-softwarehardware-attacks"
            class="level3" data-number="14.1.3">
            <h3 data-number="14.1.3"><span
            class="header-section-number">14.1.3</span> Computing
            Software/Hardware Attacks</h3>
            <p>There is a large body of work on security/attacks against
            computing hardware and software – both, for general purpose
            systems as well as embedded/<a
            href="https://dl.acm.org/doi/full/10.1145/3649499">cyber-physical</a>/<a
            href="https://www.usenix.org/conference/usenix-security-11/comprehensive-experimental-analyses-automotive-attack-surfaces">automotive
            systems</a>.</p>
            <p>A brief summary of the various computing components that
            can be attacked:</p>
            <table>
            <colgroup>
            <col style="width: 50%" />
            <col style="width: 50%" />
            </colgroup>
            <thead>
            <tr>
            <th>software</th>
            <th>hardware</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>operating system</td>
            <td>microprocessors</td>
            </tr>
            <tr>
            <td>entire software stack <br> e.g., EKF, planning
            algorithms, vision algorithms, sensor fusion</td>
            <td>hidden backdoors in chips/hardware units</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>In fact, these maps to almost all of the elements of the
            autonomous system that we have discussed so far:</p>
            <p><img src="img/stack_architecture/stack_overview.png" width="300"></p>
            <p><br></p>
            <p>A lof of the <strong>entry methods</strong> for the
            software-based attack involves <a
            href="https://dl.acm.org/doi/abs/10.1145/3329786"><strong>malware
            injection</strong></a>.</p>
            </section>
            <section id="miscellaneouscommunication-attacks"
            class="level3" data-number="14.1.4">
            <h3 data-number="14.1.4"><span
            class="header-section-number">14.1.4</span>
            Miscellaneous/Communication Attacks</h3>
            <p>Autonomous vehicles are also prone to
            <strong>remote</strong> attacks – <em>i.e.,</em> via
            communication protocols (CAN, cellular, V2X, <em>etc.</em>
            ).</p>
            <p>Some influential work in the area of <a
            href="https://www.usenix.org/conference/usenix-security-11/comprehensive-experimental-analyses-automotive-attack-surfaces">automotive
            security</a> is directly applicable here as well.</p>
            <p>A <a
            href="https://www.wired.com/2015/07/hackers-remotely-kill-jeep-highway/">real
            world demonstration</a> of this was demonstrated a few years
            ago → hackers took control of a Jeep driving on a highway
            (with the knowledge of the journalist driving the car). They
            showed that a (remote) hacker can take control of the brakes
            and steering, leaving the drivers and passengers
            helpless!</p>
            <p><br></p>
            <p><img src="img/security/tesla.webp" width="300"></p>
            <p><br></p>
            <p>Multiple efforts have shown Tesla cars (and their
            surrounding infrastructure) to be vulnerable to
            cyberattacks: <a
            href="https://www.npr.org/sections/alltechconsidered/2015/08/06/429907506/tesla-model-s-can-be-hacked-and-fixed-which-is-the-real-news/">1</a>,
            <a
            href="https://www.livescience.com/technology/electric-vehicles/white-hat-hackers-carjacked-a-tesla-using-cheap-legal-hardware-exposing-major-security-flaws-in-the-vehicle">2</a>,
            <a
            href="https://www.darkreading.com/vulnerabilities-threats/tesla-gear-hacked-multiple-times-pwn2own-contests">3</a>.</p>
            <p><br></p>
            <p>There also exists a lot of work on attacks that use the
            <a
            href="https://sibin.github.io/papers/2020_T-IV_V2X_Security_Survey_Monowar.pdf"><strong>“vehicle
            to everything” (V2X)</strong></a> communication
            protocols.</p>
            <p>Read more about <a
            href="https://www.sciencedirect.com/science/article/pii/S1570870520306788">attacks
            on unmanned aerial vehicles (UAVs)</a>.</p>
            </section>
            </section>
            <section id="defenses" class="level2" data-number="14.2">
            <h2 data-number="14.2"><span
            class="header-section-number">14.2</span> Defenses</h2>
            <p>Again, this is a vast area of research and many of the
            existing defensive techniques in cybersecurity (especially
            those related to embedded/cyber-physical/automotive systems)
            applies to autonomous vehicles as well.</p>
            <p>Defensive techniques can be classified into:</p>
            <table>
            <thead>
            <tr>
            <th>passive</th>
            <th>active</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>only detect/raise alarms</td>
            <td>take action</td>
            </tr>
            <tr>
            <td><em>e.g.</em>, intrusion detection</td>
            <td><em>e.g.</em>, eject attacker</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>We can classify the active methods into the following
            categories:</p>
            <table>
            <thead>
            <tr>
            <th>reactive</th>
            <th>proactive</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td>take action <em>on</em> detection</td>
            <td>action <em>without</em> detection</td>
            </tr>
            <tr>
            <td><em>e.g.</em>, eject attacker/sanitize system</td>
            <td><em>e.g.</em>, restart-based methods</td>
            </tr>
            <tr>
            <td></td>
            <td></td>
            </tr>
            </tbody>
            </table>
            <p>Let’s look at a <strong>few</strong>, relevant, defensive
            strategies since it will be quite difficult to go over
            everything.</p>
            <section id="detectingreacting-to-possible-attacks"
            class="level3" data-number="14.2.1">
            <h3 data-number="14.2.1"><span
            class="header-section-number">14.2.1</span>
            Detecting/Reacting to (possible) Attacks</h3>
            <p>Some attacks are hard to detect, <em>e.g.,</em> the <a
            href="https://sibin.github.io/papers/2024_ArXiv_Requiem_KyoKim.pdf">Requiem</a>
            attack that spoofs sensors values. It has been shown that
            the attack does such a good job that the intrusion detection
            modules on the devices are unable to detect it!</p>
            <p>In such scenarios, <strong>additional resources</strong>
            are required <em>e.g.,</em></p>
            <ul>
            <li><strong>redundant</strong> sensors → that measure the
            same quantities (<em>e.g.,</em> position), ideally separated
            from the communication pathways that the original/attacked
            sensors used</li>
            <li><strong>external resources</strong>, <em>e.g.,</em>
            other drones in the flock → thy can observe each other and
            try to identify irregular behavior</li>
            </ul>
            <p><br></p>
            <p>There is quite a bit of work to detect intrusions into
            such systems but we have to deal with certain challenges
            first,</p>
            <blockquote>
            <p>embedded systems may not have enough compute power or
            memory to add security mechanisms on top.</p>
            </blockquote>
            <p><img src="img/security/jessop_truth.gif" width="300"></p>
            <p><br></p>
            <p>Hence, <strong>careful design</strong> is required so
            that,</p>
            <ul>
            <li>all the <strong>applications continue to run</strong> →
            they aren’t blocked/diminished by ny security/pribacy
            mechanisms</li>
            <li>the security components work correctly.</li>
            </ul>
            <p><a
            href="https://sibin.github.io/papers/2013_RTAS_SecureCore-ManKiYoon.pdf">SecureCore</a>
            detects intrusions by checking for the
            <strong>effects</strong> of malware – <em>i.e.,</em> did the
            extra code that’s running use up additional <a
            href="https://sibin.github.io/papers/2013_RTAS_SecureCore-ManKiYoon.pdf">CPU
            cycles</a>, <a
            href="https://sibin.github.io/papers/2015_DAC_MemoryHeatMapSecureCore_ManKiYoon.pdf">memory</a>
            or <a
            href="https://sibin.github.io/papers/2017_IoTDI_SecureCoreSyscall_ManKiYoon.pdf">system
            calls</a>?</p>
            <p>So, any of these could be symptoms of malware:</p>
            <p><img src="img/security/securecore.1.png" width="400"></p>
            <p><br></p>
            <p>SecureCore uses one or more cores (a “<strong>secure
            core</strong>”) to <strong>observe</strong> the main cores
            and if it detects any signs of intrusions, switches control
            to the secure core. The secure core can then actuate the
            system in a safe manner of initiate a <strong>graceful
            shutdown</strong> that ensure the system doesn’t come to
            harm.</p>
            <p><img src="img/security/securecore.2.png" width="400"></p>
            <p><br></p>
            <p>This method,</p>
            <ul>
            <li>is able to <strong>quickly</strong> detect intrusions
            and</li>
            <li>keep the system <strong>safe</strong>.</li>
            </ul>
            <p><strong>Note:</strong> it doesn’t take any additional
            actions – <em>e.g.,</em> to remove the attacker – that is
            something left up to the system designer who can develop a
            policy to work with the SecureCore architecture.</p>
            <p><br></p>
            <p>Another method, <a
            href="https://sibin.github.io/papers/2023_ISORC_SCATE_Monowar.pdf">SCATE</a>,
            <strong>checks outgoing actuation commands</strong> → to
            verify that they don’t put the system into an unsafe state.
            If they do, then the actuation command is blocked.</p>
            <p><img src="img/security/scate.1.png" width="400"></p>
            <p><br></p>
            <p>The checks are carried out by switching into a trusted
            execution environment (<em>e.g.,</em> <a
            href="https://www.trustonic.com/technical-articles/what-is-trustzone/">ARM
            TrustZone</a> or <a
            href="https://eprint.iacr.org/2016/086.pdf">Intel
            SGX</a>).</p>
            <p>The problems with such an approach are,</p>
            <ul>
            <li>checking <strong>every</strong> actuation commands may
            be too onerous</li>
            <li>the delays could add up and cause the system to miss
            critical ting deadlines
            <ul>
            <li>(switching to TEE → checking actuation command →
            switching back to main normal world)</li>
            </ul></li>
            </ul>
            <p>Hence, SCATE only checks a <strong>subset</strong> of the
            actuation commands. To prevent an adversary from tracking
            which commands are checked, SCATE uses, <strong>game
            theory</strong></p>
            <ul>
            <li>to <strong>randomly</strong> pick a subset of commands
            to be checked</li>
            <li>make it <strong>seem</strong> like all commands are
            checked.</li>
            </ul>
            <p><img src="img/security/scate.2.png" width="400"></p>
            <p><br></p>
            <p>Now,</p>
            <blockquote>
            <p>what if the attacker is really smart and evades
            detection?</p>
            </blockquote>
            <p>Or, worse, is able to disable the detection mechanisms?
            So, we need a way to ensure that the system remains safe
            and</p>
            <ul>
            <li>we <strong>don’t rely on an external signal</strong>
            (intrusion detection)</li>
            <li>not check to see if something has been modified, like
            actuation commands in SCATE.</li>
            </ul>
            <p>We need to be4 <strong>proactive</strong> and not
            <em>reactive</em>.</p>
            <p><a
            href="https://sibin.github.io/papers/2018_iccps_restart_fardin.pdf">ReSecure</a>
            aims to avoid the problem of <strong>detecting</strong> an
            attack while still <strong>keeping the system safe</strong>.
            It does this by,</p>
            <ul>
            <li><strong>active reboots</strong> of the system</li>
            <li>reloading the software from a <strong>trusted
            state</strong>.</li>
            </ul>
            <p><img src="img/security/resecure.1.png" width="400"></p>
            <p><br></p>
            <p><strong>Note:</strong> the reboot intervals cannot be
            deterministic (<em>e.g.,</em> periodic) since an adversary
            can figure this out and then launch an attack that is
            limited to the time interval between reboots. Similarly, the
            reboot cannot be related to the detection of an intrusion →
            since it might be too late to prevent the negative effects
            of the attacks.</p>
            <p>ReSecure avoids this problem by,</p>
            <ul>
            <li>observing the state of the system and initiating a
            reboot when the system gets <strong>close to an unsafe
            state</strong> → <em>e.g.,</em> below a certain threshold
            for a drone</li>
            <li>calculating a <strong>minimum</strong> time, <span
            class="math inline"><em>T</em></span>, to the next reboot →
            the value of <span class="math inline"><em>T</em></span>
            depends on how close the system is to becoming “unsafe”. If
            it is close, then <span
            class="math inline"><em>T</em></span> will be small, if not
            <span class="math inline"><em>T</em></span> can be
            longer.</li>
            </ul>
            <p><img src="img/security/resecure.2.png" width="500"></p>
            <p><br></p>
            <table>
            <colgroup>
            <col style="width: 38%" />
            <col style="width: 61%" />
            </colgroup>
            <thead>
            <tr>
            <th style="text-align: left;">color</th>
            <th style="text-align: left;">meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
            <td style="text-align: left;"><strong>white</strong></td>
            <td style="text-align: left;">mission controller is in
            charge and platform is not compromised</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>yellow</strong></td>
            <td style="text-align: left;">system is undergoing a
            restart</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>green</strong></td>
            <td style="text-align: left;">a “secure execution interval”
            (SEI) is running</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>range</strong></td>
            <td style="text-align: left;">adversary is in charge</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>blue</strong></td>
            <td style="text-align: left;"><strong>RoT</strong> accepts
            new restart time</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>gray</strong></td>
            <td style="text-align: left;">RoT <strong>does not</strong>
            accept new restart time</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>red
            arrow</strong></td>
            <td style="text-align: left;">RoT triggers a restart</td>
            </tr>
            <tr>
            <td style="text-align: left;"><strong>blue
            arrow</strong></td>
            <td style="text-align: left;">SEI ends, the next restart
            time is scheduled in RoT and the mission controller
            starts</td>
            </tr>
            <tr>
            <td style="text-align: left;"></td>
            <td style="text-align: left;"></td>
            </tr>
            </tbody>
            </table>
            <p><br></p>
            <p>But, what if the attacker blocks the system from
            rebooting?</p>
            <p>To prevent this, ReSecure uses a <strong>hardware root of
            trust</strong> (RoT), <em>i.e.,</em> a set of hardware
            components (timer, interrupt handler, reset handler) that
            are <strong>separate</strong> from the main system and
            <strong>cannot be reprogrammed</strong> once the system has
            started up.</p>
            <p><img src="img/security/resecure.3.png" width="400"></p>
            </section>
            <section id="software-based-security-solutions"
            class="level3" data-number="14.2.2">
            <h3 data-number="14.2.2"><span
            class="header-section-number">14.2.2</span> Software-Based
            Security Solutions</h3>
            <p>While traditional software-security methods,
            <em>e.g.,</em> <strong>isolation</strong> (VMs, containers,
            <em>etc.</em> ) and <strong>security through
            obscurity</strong> can definitely improve the security of
            such systems, recall that we may have
            <strong>resource</strong> and <strong>timing
            constraints</strong>. So we need specific solutions to
            specific problems (<em>e.g.,</em> ScheduLeak).</p>
            <p>One of the main problems with such systems, especially
            ones that had <strong>real-time constraints</strong> is that
            they’re <strong>predictable</strong> → by design. For
            instance, a periodic real-time task keeps (infinitely)
            repeating across hyperperiods.</p>
            <p><img src="img/security/epsilon.1.png" width="400"></p>
            <p><br></p>
            <p>ScheduLeak (and other attacks) take advantage of this to
            <strong>predict</strong> when critical tasks will run in the
            future.</p>
            <p>Hence, we need to <strong>obfuscate</strong> the
            execution patterns so that,</p>
            <ul>
            <li>attackers <strong>cannot determine</strong> when tasks
            will execute</li>
            <li>the real-time constraints (<strong>deadlines</strong>)
            are still met.</li>
            </ul>
            <p><img src="img/security/epsilon.2.png" width="400"></p>
            <p><br></p>
            <p>Hence, we developed the notion of
            <strong>indistinguishability</strong> and an <a
            href="https://sibin.github.io/papers/2021_CCS_ScheduleIndistinguishability_CY.pdf"><strong><span
            class="math inline"><em>ϵ</em> − <em>s</em><em>c</em><em>h</em><em>e</em><em>d</em><em>u</em><em>l</em><em>e</em><em>r</em></span></strong></a>
            that can achieve both of these goals:</p>
            <p><img src="img/security/epsilon.3.png" width="500"></p>
            <p><br></p>
            <p>The <span
            class="math inline"><em>ϵ</em> − <em>s</em><em>c</em><em>h</em><em>e</em><em>d</em><em>u</em><em>l</em><em>e</em><em>r</em></span>
            borrows ideas from <a
            href="https://digitalprivacy.ieee.org/publications/topics/what-is-differential-privacy#:~:text=At%20its%20roots%2C%20differential%20privacy,a%20result%20of%20providing%20data"><strong>Differential
            Privacy</strong></a> to introduce <strong>noise</strong>
            into the execution patterns of the system.</p>
            <p>We see the effectiveness of this method when applied to a
            rover:</p>
            <p><img src="img/security/epsilon.4.png" width="400"></p>
            <p><br></p>
            <p>The rover is able to,</p>
            <ul>
            <li>follow the path faithfully</li>
            <li>not miss many deadlines.</li>
            </ul>
            </section>
            <section id="miscellaneous-methods" class="level3"
            data-number="14.2.3">
            <h3 data-number="14.2.3"><span
            class="header-section-number">14.2.3</span> Miscellaneous
            Methods</h3>
            <p>We can also improve the security of autonomous systems by
            <strong>reducing the number of communication
            endpoints</strong>. The idea being that if an adversary
            cannot easily get on to the device, then the chances of a
            successful attack are less.</p>
            <p>But the problem is that there are a lot of reasons for
            having multiple communication endpoints,</p>
            <ul>
            <li>other vehicles/traffic signs/infrastructure [v2x]</li>
            <li>network updates (os/software stack/security)</li>
            <li>entertainment systems</li>
            </ul>
            <p>So, there is a tension between → security and
            functionality.</p>
            <p><br></p>
            <p>Adding stronger/additional
            <strong>encryption/authentication</strong> could also deter
            would-be attackers. But this requires the use of a
            <strong>public-key infrastructure</strong> (PKI) and also
            increases,</p>
            <ul>
            <li>computational and memory overheads</li>
            <li>power consumption</li>
            </ul>
            <p>both of which will likely increase the loads on the
            (limited) embedded system. So it needs to be carried out
            carefully.</p>
            <p><br></p>
            <p>There is also work in <a
            href="https://sibin.github.io/papers/2020_T-IV_V2X_Security_Survey_Monowar.pdf">V2X
            security</a>.</p>
            </section>
            <section id="holistic-view" class="level3"
            data-number="14.2.4">
            <h3 data-number="14.2.4"><span
            class="header-section-number">14.2.4</span> Holistic
            View</h3>
            <p>Security should be treated as a <strong>first-class
            principle</strong> → it should be considered during the
            <strong>entire</strong> design, implementation and testing
            processes. Retrofitting it after the fact will most surely
            fail or, at the very least, lead to serious problems.</p>
            <p>One should consider the burdens of security integration
            in autonomous systems.</p>
            <p>This <a
            href="https://sibin.github.io/papers/2024_ACM_CSUR_RTSecuritySoK_MonowarHasan.pdf">SoK
            paper</a> classifies the various security methods in
            real-time systems (most also relevant to autonomous systems)
            and compares the solutions to each other by proposing a new
            metric, “<strong>attacker’s burden</strong>”.</p>
            <p><br></p>
            <p><strong>References</strong></p>
            <p>[TBD]</p>
            </section>
            </section>
            </section>
            <section id="footnotes"
            class="footnotes footnotes-end-of-document"
            role="doc-endnotes">
            <hr />
            <ol>
            <li id="fn1"><p>TBD<a href="#fnref1" class="footnote-back"
            role="doc-backlink">↩︎</a></p></li>
            <li
            id="fn2"><p>https://dl.acm.org/doi/10.5555/244522.244548<a
            href="#fnref2" class="footnote-back"
            role="doc-backlink">↩︎</a></p></li>
            <li
            id="fn3"><p>https://www.cecs.uci.edu/~papers/compendium94-03/papers/2002/date02/pdffiles/05a_1.pdf
            <!--link rel="stylesheet" href="./custom.sibin.css"--><a
            href="#fnref3" class="footnote-back"
            role="doc-backlink">↩︎</a></p></li>
            </ol>
            </section>
            </div>
    </div>
  </div>
  <!--script src="https://vjs.zencdn.net/5.4.4/video.js"></script-->

</body>
</html>
